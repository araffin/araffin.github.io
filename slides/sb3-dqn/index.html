<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>Stable-Baselines3</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/white.css" id="theme">
		<!-- Add DLR logo -->
		<link rel="stylesheet" href="css/dlr.css">
		<!-- Grid system: http://flexboxgrid.com/ -->
		<link rel="stylesheet" href="css/flexboxgrid.min.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css" id="highlight-theme">
		<!-- <link rel="stylesheet" href="plugin/highlight/monokai-sublime.css" id="highlight-theme"> -->
		<!-- <link rel="stylesheet" href="plugin/highlight/atom-one-dark.css" id="highlight-theme"> -->
	</head>
	<body>
		<!--
		Timer: https://www.timer-tab.com/
		Playstation 1: https://www.youtube.com/watch?v=Qg0BvV0HE4s
		Marmotte: https://www.youtube.com/watch?v=iweOSPbU73Y
		# Edith Piaf: https://www.youtube.com/watch?v=Q3Kvu6Kgp88
		-->
		<div class="reveal">
			<div class="slides">
				<header>
					www.dlr.de &middot; Antonin RAFFIN &middot; Stable-Baselines3 &middot; Formation RL &middot; 09.09.2021
				</header>
				<section data-background-image="images/bg_front.jpg">
					<div class="row bottom-xs">
						<div class="row middle-xs">
							<div class="col-xs-7">
								<div class="col-xs-12">
									<h3 id='main-title'>Stable-Baselines3 (SB3)</h3>
								</div>
							</div>
							<div class="col-xs-5">
								<a target="_blank" href="https://github.com/DLR-RM/stable-baselines3">
									<img src="images/sb_logo.png" alt="baymax">
								</a>
							</div>
						</div>
						<div class="col-xs-6 xsmall-text">
							Antonin RAFFIN (@araffin2) <br>
							<span class="italic">German Aerospace Center (DLR)</span><br>
							<a href="https://araffin.github.io/">https://araffin.github.io/</a>
						</div>
					</div>

				</section>
				<section>
					<h3>Outline</h3>
					<ol>
						<li>
							Stable-Baselines3 Overview
						</li>
						<li>
							Questions?
						</li>
						<li>
							DQN in Stable-Baselines3
						</li>
					</ol>
				</section>

				<section>
					<h4>Hands-on Session with SB3</h4>
					<p>Full video:
						<a href="https://rl-vs.github.io/rlvs2021/tips-and-tricks.html">
							https://rl-vs.github.io/rlvs2021/tips-and-tricks.html
						</a>
					</p>
				</section>

				<section>
					<h3>Stable-Baselines3 Overview</h3>
				</section>
				<section>
					<h4>History of the project</h4>
				</section>

				<section>
					<div class="row">
						<div class="col-xs-12">
							<h4>To fork or not to fork? (2018)</h4>
						</div>
						<div class="col-xs-6">
							<img src="images/sb3/baselines_fail.png" alt="build failed">
						</div>
						<div class="col-xs-6">
							<img src="images/sb3/hard_to_read.png" alt="hard-to-read">
						</div>
						<div class="col-xs-6">
							<img src="images/sb3/reddit_baselines.png" alt="reddit 1">
						</div>
						<div class="col-xs-6">
							<img src="images/sb3/reddit_2.png" alt="reddit 2">
						</div>
					</div>
					<aside class="notes">
						working on SRL (cf. Racing Car) evaluation?
						OpenAI Baselines working but breaking code + not easy to tweak
					</aside>
				</section>
				<section>
					<h3>Stable-Baselines?</h3>
					<div class="row">
						<div class="col-xs-6">
							<pre><code class="python medium-text" data-trim>
								from stable_baselines import A2C

								model = A2C("MlpPolicy", "CartPole-v1")
								model.learn(50000)
								model.save("a2c_cartpole")
							</code></pre>
						</div>
						<div class="col-xs-6">
							<img src="images/sb3/pr_openai.png" alt="PR OpenAI">
						</div>
					</div>
				</section>
				<section>
					<h3>Stable-Baselines (2018-2020)</h3>
					<p class="medium-text">Ashley Hill, Maximilian Ernestus, Adam Gleave, Anssi Kanervisto</p>
					<p class="medium-text">
						<a href="https://github.com/hill-a/stable-baselines">https://github.com/hill-a/stable-baselines</a>
					</p>
					<ul class="medium-text">
						<li class="fragment">5 maintainers</li>
						<li class="fragment">60+ contributors</li>
						<li class="fragment">1000+ issues / pull requests</li>
						<li class="fragment">300+ citations</li>
					</ul>
				</section>

				<section>
					<h3>Stable-Baselines3 (2020-...)</h3>
					<p class="medium-text">
						<a href="https://github.com/DLR-RM/stable-baselines3">https://github.com/DLR-RM/stable-baselines3</a>
					</p>
					<ul>
						<li class="fragment">cleaner codebase but same API</li>
						<li class="fragment">performance checked</li>
						<li class="fragment">code coverage: 95%</li>
						<li class="fragment">active community</li>
					</ul>
					<aside class="notes">
						TF2 coming, discussion with community -> PyTorch
						refer to blog post (how performance were checked):
						https://araffin.github.io/post/sb3/
						+ documentation
					</aside>
				</section>

				<section>
					<h4>Active Community</h4>
					<div class="row">
						<div class="col-xs-6">
							<p>Stable-Baselines (SB2)</p>
							<div class="row">
								<div class="col-xs-12">
									<img src="images/sb3/sb2_stars.png" alt="git stars">
								</div>
							</div>
							<div class="row">
								<div class="col-xs-12">
									<img src="https://pepy.tech/badge/stable-baselines/month" alt="pypi">
								</div>
							</div>
						</div>
						<div class="col-xs-6">
							<p>Stable-Baselines3 (SB3)</p>
							<div class="row">
								<div class="col-xs-12">
									<img src="images/sb3/sb3_stars.png" alt="github active">
								</div>
							</div>
							<div class="row">
								<div class="col-xs-12">
									<img src="https://pepy.tech/badge/stable-baselines3/month" alt="pypi">
								</div>
							</div>
							<div class="row">
								<div class="col-xs-12">
									<img src="images/sb3/contributors.png" alt="contributtors" width="60%">
								</div>
							</div>
						</div>
					</div>
					<aside class="notes">
						Active community is one of the big strength of the lib:
						improve the quality/ add features depending on the needs
						+ if a bug is present, quickly spotted
					</aside>
				</section>

				<section>
					<h3>Design Principles</h3>
					<ul>
						<li class="fragment">reliable implementations of RL algorithms</li>
						<li class="fragment">user-friendly</li>
						<li class="fragment">focus on model-free, single-agent RL</li>
						<li class="fragment">favour readability and simplicity over modularity</li>
					</ul>
					<aside class="notes">
						Despite being simple to use,
						still require knowledge about RL (we had to put a warning)
						small core to ensure quality and maintainability
					</aside>
				</section>

				<section>
					<h3>Features</h3>
					<ul class="medium-text">
						<li class="fragment">algorithms: A2C, DDPG, DQN, HER, PPO, SAC and TD3</li>
						<li class="fragment">clean and simple interface</li>
						<li class="fragment">fully documented</li>
						<li class="fragment">comprehensive (tensorboard logging, callbacks, ...)</li>
						<li class="fragment">training framework included (RL Zoo)</li>
						<li class="fragment">SB3 Contrib: QR-DQN, TQC, ...</li>
					</ul>
				</section>

				<section>
					<h3>Getting Started</h3>
					<pre class="fragment"><code class="python" data-trim data-line-numbers="1-5|6-7|8-9|10-11|12-15">
						import gym
						from stable_baselines3 import SAC
						# Train an agent using Soft Actor-Critic on Pendulum-v0
						env = gym.make("Pendulum-v0")
						model = SAC("MlpPolicy", env, verbose=1)
						# Train the model
						model.learn(total_timesteps=20000)
						# Save the model
						model.save("sac_pendulum")
						# Load the trained model
						model = SAC.load("sac_pendulum")
						# Start a new episode
						obs = env.reset()
						# What action to take in state `obs`?
						action, _ = model.predict(obs, deterministic=True)
					</code></pre>
					<aside class="notes">
						Quick overview of what is possible,
						more in detail in the notebook
					</aside>
				</section>

				<section>
					<h3>SB3 Training loop</h3>
					<img src="images/sb3/sb3_loop.png" alt="training loop" width="70%">
					<aside class="notes">
						Quick look of what is happening internally
					</aside>
				</section>

				<section>
					<h4>Training framework: RL Zoo</h4>

					<p class="medium-text">
						<a href="https://github.com/DLR-RM/rl-baselines3-zoo">
							https://github.com/DLR-RM/rl-baselines3-zoo
						</a>
					</p>

					<div class="row medium-text r-stack">
						<div class="col-xs-12 fragment">
							<ul>
								<li>training, loading, plotting, hyperparameter optimization</li>
								<li>100+ trained models + tuned hyperparameters</li>
							</ul>
							<pre style="width:100%"><code class="bash" data-trim>
								# Train an A2C agent on Atari breakout using tuned hyperparameters,
								# evaluate the agent every 10k steps and save a checkpoint every 50k steps
								python train.py --algo a2c --env BreakoutNoFrameskip-v4 \
								    --eval-freq 10000 --save-freq 50000
								# Plot the learning curve
								python scripts/all_plots.py -a a2c -e BreakoutNoFrameskip-v4 -f logs/
							</code></pre>
						</div>
					</div>

					<aside class="notes">
						- show the zoo and show how it relates to the first part<br>
						- does a lot for you (to prevent error and help reproducibiltiy)<br>
						- only use it when you are confident with SB3<br>
						All 3 previous examples combine
						SB3 + SB3 contrib + RL Zoo

					</aside>
				</section>

				<section>
					<h4>Recap</h4>
					<ul>
						<li class="fragment">reliable RL implementations</li>
						<li class="fragment">user-friendly</li>
						<li class="fragment">training framework (RL Zoo)</li>
					</ul>
				</section>

				<section>
					<h3>Questions?</h3>
				</section>

				<section>
					<h3>DQN in SB3</h3>
				</section>

				<section>
					<h3>Reminder</h3>
					<div class="row">
						<div class="col-xs-12">
							<img src="images/dqn/dqn.png" alt="DQN" height="80%">
						</div>
					</div>
				</section>

				<section>
					<section>
						<h3>Anatomy</h3>
						<div class="row">
							<div class="col-xs-12">
								<img src="images/dqn/dqn_sb3.png" alt="DQN" height="80%">
							</div>
						</div>
					</section>
					<section>
						<h3>SB3 Training loop</h3>
						<img src="images/sb3/sb3_loop.png" alt="training loop" width="70%">
					</section>
				</section>

				<section>
					<section>
						<h3>Q-network</h3>
						<div class="row">
							<div class="col-xs-12">
								<img src="images/dqn/q_net.png" alt="DQN" height="80%">
							</div>
						</div>
					</section>
					<section>
						<h3>Q-network (Code)</h3>
						<div class="row">
							<div class="col-xs-12 medium-text">
								<p>dqn/policies.py</p>
							</div>
							<div class="col-xs-12 medium-text">
								<pre><code class="python" data-trim data-line-numbers="1-2|3-4|6-7|9-13-|17-22|">
									def _build(self, lr_schedule: Schedule) -> None:
									    self.q_net = self.make_q_net()
									    self.q_net_target = self.make_q_net()
									    self.q_net_target.load_state_dict(self.q_net.state_dict())

									    # Setup optimizer with initial learning rate
									    self.optimizer = self.optimizer_class(self.parameters(), lr=lr_schedule(1))

									class QNetwork:

									# ...
									    action_dim = self.action_space.n  # number of actions
									    self.q_net = create_mlp(features_dim, action_dim, net_arch, activation_fn)

									# ...

									def _predict(self, observation: th.Tensor) -> th.Tensor:
									    q_values = self.forward(observation)
									    # Greedy action
									    action = q_values.argmax(dim=1)
									    return action

								</code></pre>
							</div>
						</div>
					</section>
					<section>
						<h3>Network architecture</h3>
						<div class="row">
							<div class="col-xs-12">
								<img src="images/sb3/net_arch.png" alt="Network Architecture">
							</div>
						</div>
					</section>

				</section>

				<section>
					<section>
						<h3>Replay Buffer</h3>
						<div class="row">
							<div class="col-xs-12">
								<img src="images/dqn/replay_buffer.png" alt="Replay" height="90%">
							</div>
						</div>
					</section>
					<section>
						<h3>Replay Buffer Sampling</h3>
						<div class="row">
							<div class="col-xs-12">
								<img src="images/dqn/replay_buffer_sampling.png" alt="Replay sampling" height="60%">
							</div>
						</div>
					</section>
					<section>
						<h3>Replay Buffer (Code)</h3>
						<div class="row">
							<div class="col-xs-12 medium-text">
								<p>common/buffers.py</p>
							</div>
							<div class="col-xs-12 medium-text">
								<pre><code class="python" data-trim data-line-numbers="1-7|9-23|25-28|30-33">
									class ReplayBuffer(BaseBuffer):
									    # ...
									        self.observations = np.zeros((self.buffer_size,) + self.obs_shape)
									        self.next_observations = np.zeros((self.buffer_size,) + self.obs_shape)
									        self.actions = np.zeros((self.buffer_size, self.action_dim))
									        self.rewards = np.zeros((self.buffer_size,))
									        self.dones = np.zeros((self.buffer_size,))

									    def add(
									        self,
									        obs: np.ndarray,
									        next_obs: np.ndarray,
									        action: np.ndarray,
									        reward: np.ndarray,
									        done: np.ndarray,
									        infos: List[Dict[str, Any]],
									    ) -> None:
									        # Copy to avoid modification by reference
									        self.observations[self.pos] = np.array(obs)
									        self.next_observations[self.pos] = np.array(next_obs)
									        self.actions[self.pos] = np.array(action)
									        self.rewards[self.pos] = np.array(reward)
									        self.dones[self.pos] = np.array(done)

									        self.pos += 1
									        if self.pos == self.buffer_size:
									            self.full = True
									            self.pos = 0

									    def sample(self, batch_size: int) -> ReplayBufferSamples:
									        upper_bound = self.buffer_size if self.full else self.pos
									        batch_inds = np.random.randint(0, upper_bound, size=batch_size)
									        return self._get_samples(batch_inds)
								</code></pre>
							</div>
						</div>
					</section>
				</section>

				<section>
					<section>
						<h3>Collecting Experience</h3>
						<div class="row">
							<div class="col-xs-12">
								<img src="images/sb3/sb3_loop.png" alt="training loop" width="70%">
							</div>
						</div>
					</section>
					<section>
						<h3>Collecting Experience (Code)</h3>
						<div class="row">
							<div class="col-xs-12 medium-text">
								<p>common/off_policy_algorithm.py</p>
							</div>
							<div class="col-xs-12 medium-text">
								<pre><code class="python" data-trim data-line-numbers="1-11|13-16|17-18|20-21|23-24|26-28|16-28|30-35|37-48">
									def collect_rollouts(
									    self,
									    env: VecEnv,
									    train_freq: TrainFreq,
									    replay_buffer: ReplayBuffer,
									    learning_starts: int = 0,
									    log_interval: Optional[int] = None,
									):
									    """
									    Collect experiences and store them into a ``ReplayBuffer``.
									    """

									    while should_collect_more_steps(train_freq, num_collected_episodes):
									        done = False

									        while not done: # Loop until the end of an episode
									            # Select action randomly or according to policy
									            action = self._sample_action(learning_starts)

									            # Perform action
									            new_obs, reward, done, infos = env.step(action)

									            # Store data in replay buffer
									            self._store_transition(action, new_obs, reward, done, infos)

									            # For DQN, check if the target network should be updated
									            # and update the exploration schedule
									            self._on_step()

									        if done:
									            self._episode_num += 1

									            # Log training infos
									            if log_interval is not None and self._episode_num % log_interval == 0:
									                self._dump_logs()

									    def predict(
									        self,
									        observation: np.ndarray,
									        deterministic: bool = False,
									    ) -> Tuple[np.ndarray, Optional[np.ndarray]]:

									        # Epsilon-greedy exploration
									        if not deterministic and np.random.rand() < self.exploration_rate:
									            action = np.array(self.action_space.sample())
									        else:
									            action, _state = self.q_net.predict(observation)
									        return action, _state
								</code></pre>
							</div>
						</div>
					</section>
				</section>

				<section>
					<section>
						<h3>Training the network</h3>
						<div class="row">
							<div class="col-xs-12">
								<img src="images/dqn/dqn_equa.png" alt="DQN equation" width="90%">
							</div>
						</div>
					</section>
					<section>
						<h3>Training the network (Code)</h3>
						<div class="row">
							<div class="col-xs-12 medium-text">
								<p>common/dqn.py</p>
							</div>
							<div class="col-xs-12 medium-text">
								<pre><code class="python" data-trim data-line-numbers="1-4|6-12|14-21|23-26">
									def train(self, gradient_steps: int, batch_size: int = 100) -> None:
									    for _ in range(gradient_steps):
									        # Sample replay buffer
									        replay_data = self.replay_buffer.sample(batch_size)

									        with th.no_grad():
									            # Compute the next Q-values using the target network
									            next_q_values = self.q_net_target(replay_data.next_observations)
									            # Follow greedy policy: use the one with the highest value
									            next_q_values, _ = next_q_values.max(dim=1)
									            # 1-step TD target
									            target_q_values = replay_data.rewards + (1 - replay_data.dones) * self.gamma * next_q_values

									        # Get current Q-values estimates
									        current_q_values = self.q_net(replay_data.observations)

									        # Retrieve the q-values for the actions from the replay buffer
									        current_q_values = th.gather(current_q_values, dim=1, index=replay_data.actions.long())

									        # Compute Huber loss (less sensitive to outliers)
									        loss = F.smooth_l1_loss(current_q_values, target_q_values)

									        # Optimize the Q-network
									        self.policy.optimizer.zero_grad()
									        loss.backward()
									        self.policy.optimizer.step()
								</code></pre>
							</div>
						</div>
					</section>
				</section>


				<section>
					<h3>Bonus: VecEnv</h3>
					<a href="https://excalidraw.com/#json=4623900759556096,tK4MDgInlTvtj0HimIHuRA">https://excalidraw.com/</a>
				</section>

				<section>
					<h3>Recap</h3>
					<div class="row">
						<div class="col-xs-12 medium-text">
							<ul>
								<li class="fragment">Q-Network and target network (dqn/policies.py)</li>
								<li class="fragment">Replay buffer (common/buffers.py)</li>
								<li class="fragment">Collecting experience (common/off_policy_algorithm.py)</li>
								<li class="fragment">Training the network (dqn/dqn.py)</li>
							</ul>
						</div>
					</div>
				</section>

				<section>
					<h3>Backup Slides</h3>
				</section>

				<section>
					<h3>SB3 vs other libs</h3>
					<img src="images/sb3/rl_libs.png" alt="RL libs" style="max-width:100%;">
					<aside class="notes">
						Main differences: community driven,
						extensive doc, benchmark
					</aside>
				</section>

				<section data-markdown>
					<textarea data-template>
						### Installation

						```
						pip install stable-baselines3[extra]
						```
						<small>[Documentation](https://stable-baselines3.readthedocs.io/)</small>
					</textarea>
				</section>

				<section>
					<h3>SB3 policy</h3>
					<div class="row">
						<div class="col-xs-6">
							<img src="images/sb3/net_arch.png" alt="Network Architecture">
							<p class="medium-text">Network Architecture</p>
						</div>
						<div class="col-xs-6">
							<img src="images/sb3/SB3Policy.png" alt="SB3 policy">
						</div>
					</div>
				</section>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				// Display the page number of the current slide
				slideNumber: true,

				// Add the current slide number to the URL hash so that reloading the
				// page/copying the URL will return you to the same slide
				hash: true,

				// Push each slide change to the browser history. Implies `hash: true`
				// history: false,

				// math: {
				// 	mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
				// 	config: 'TeX-AMS_HTML-full'  // See http://docs.mathjax.org/en/latest/config-files.html
				// 	// pass other options into `MathJax.Hub.Config()`
				// 	// TeX: { Macros: macros }
				// },

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes, RevealMath]
			});
		</script>
	</body>
</html>
