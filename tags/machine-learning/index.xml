<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | Antonin Raffin | Homepage</title>
    <link>//localhost:1313/tags/machine-learning/</link>
      <atom:link href="//localhost:1313/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2018 - 2024</copyright><lastBuildDate>Thu, 11 May 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Machine Learning</title>
      <link>//localhost:1313/tags/machine-learning/</link>
    </image>
    
    <item>
      <title>SBX: Stable Baselines Jax</title>
      <link>//localhost:1313/project/sbx/</link>
      <pubDate>Thu, 11 May 2023 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/project/sbx/</guid>
      <description>&lt;p&gt;Proof of concept version of Stable-Baselines3 in Jax.&lt;/p&gt;
&lt;p&gt;Github repository: 
&lt;a href=&#34;https://github.com/araffin/sbx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/araffin/sbx&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Implemented algorithms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/1801.01290&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Soft Actor-Critic (SAC)&lt;/a&gt; and 
&lt;a href=&#34;https://arxiv.org/abs/2110.01548&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SAC-N&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/2005.04269&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Truncated Quantile Critics (TQC)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://openreview.net/forum?id=xCVJMsPv3RT&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dropout Q-Functions for Doubly Efficient Reinforcement Learning (DroQ)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/1707.06347&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Proximal Policy Optimization (PPO)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/1312.5602&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Q Network (DQN)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/1802.09477&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Twin Delayed DDPG (TD3)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/1509.02971&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Deterministic Policy Gradient (DDPG)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://openreview.net/forum?id=PczQtTsTIX&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Batch Normalization in Deep Reinforcement Learning (CrossQ)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Datasaurust</title>
      <link>//localhost:1313/project/datasaurust/</link>
      <pubDate>Sat, 11 Mar 2023 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/project/datasaurust/</guid>
      <description>&lt;p&gt;Blazingly fast implementation of the 
&lt;a href=&#34;https://www.research.autodesk.com/publications/same-stats-different-graphs/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Datasaurus&lt;/a&gt; paper (500x faster than the original): &amp;ldquo;Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing&amp;rdquo; by Justin Matejka and George Fitzmaurice.&lt;/p&gt;
&lt;p&gt;Github repository: 
&lt;a href=&#34;https://github.com/araffin/datasaurust&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/araffin/datasaurust&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stable Baselines3</title>
      <link>//localhost:1313/project/stable-baselines3/</link>
      <pubDate>Mon, 11 May 2020 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/project/stable-baselines3/</guid>
      <description>&lt;p&gt;Stable Baselines3 is a set of improved implementations of reinforcement learning algorithms in PyTorch. It is the next major version of Stable Baselines.&lt;/p&gt;
&lt;p&gt;Github repository: 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/DLR-RM/stable-baselines3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Documentation: 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://stable-baselines3.readthedocs.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RL Baselines3 Zoo (collection of pre-trained agents): 
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/DLR-RM/rl-baselines3-zoo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RL Baselines3 Zoo also offers a simple interface to train, evaluate agents and do hyperparameter tuning.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning to Drive Smoothly in Minutes</title>
      <link>//localhost:1313/post/learning-to-drive/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/post/learning-to-drive/</guid>
      <description>&lt;p&gt;Read the full article on 
&lt;a href=&#34;https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning to Drive Smoothly in Minutes</title>
      <link>//localhost:1313/project/learning-to-drive/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/project/learning-to-drive/</guid>
      <description>&lt;p&gt;Learning to drive smoothly in minutes, using a reinforcement learning algorithm &amp;ndash; Soft Actor-Critic (SAC) &amp;ndash; and a Variational AutoEncoder (VAE) in the Donkey Car simulator.&lt;/p&gt;
&lt;p&gt;Github repository: 
&lt;a href=&#34;https://github.com/araffin/learning-to-drive-in-5-minutes&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/araffin/learning-to-drive-in-5-minutes&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Blog post on 
&lt;a href=&#34;https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RL Baselines Zoo</title>
      <link>//localhost:1313/project/rl-baselines-zoo/</link>
      <pubDate>Fri, 23 Nov 2018 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/project/rl-baselines-zoo/</guid>
      <description>&lt;p&gt;A collection of trained Reinforcement Learning (RL) agents, with tuned hyperparameters, using 
&lt;a href=&#34;https://github.com/hill-a/stable-baselines&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable Baselines&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Github repository: 
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/DLR-RM/rl-baselines3-zoo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Goals of this repository:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Provide a simple interface to train and enjoy RL agents&lt;/li&gt;
&lt;li&gt;Benchmark the different Reinforcement Learning algorithms&lt;/li&gt;
&lt;li&gt;Provide tuned hyperparameters for each environment and RL algorithm&lt;/li&gt;
&lt;li&gt;Have fun with the trained agents!&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>S-RL Toolbox</title>
      <link>//localhost:1313/project/srl-toolbox/</link>
      <pubDate>Wed, 10 Oct 2018 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/project/srl-toolbox/</guid>
      <description>&lt;p&gt;S-RL Toolbox: Reinforcement Learning (RL) and State Representation Learning (SRL) Toolbox for Robotics.&lt;/p&gt;
&lt;p&gt;Github repository: 
&lt;a href=&#34;https://github.com/araffin/robotics-rl-srl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/araffin/robotics-rl-srl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Documentation: 
&lt;a href=&#34;https://s-rl-toolbox.readthedocs.io&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://s-rl-toolbox.readthedocs.io&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Paper: 
&lt;a href=&#34;https://arxiv.org/abs/1809.09369&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/1809.09369&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;main-features&#34;&gt;Main Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;10 RL algorithms (
&lt;a href=&#34;https://github.com/hill-a/stable-baselines&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable Baselines&lt;/a&gt; included)&lt;/li&gt;
&lt;li&gt;logging / plotting / visdom integration / replay trained agent&lt;/li&gt;
&lt;li&gt;hyperparameter search (hyperband, hyperopt)&lt;/li&gt;
&lt;li&gt;integration with State Representation Learning (SRL) methods (for feature extraction)&lt;/li&gt;
&lt;li&gt;visualisation tools (explore latent space, display action proba, live plot in the state space, &amp;hellip;)&lt;/li&gt;
&lt;li&gt;robotics environments to compare SRL methods&lt;/li&gt;
&lt;li&gt;easy install using anaconda env or Docker images (CPU/GPU)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Stable Baselines</title>
      <link>//localhost:1313/project/stable-baselines/</link>
      <pubDate>Thu, 27 Sep 2018 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/project/stable-baselines/</guid>
      <description>&lt;p&gt;Stable Baselines is a set of improved implementations of Reinforcement Learning (RL) algorithms based on OpenAI Baselines.&lt;/p&gt;
&lt;p&gt;Github repository: 
&lt;a href=&#34;https://github.com/hill-a/stable-baselines&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/hill-a/stable-baselines&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can read a detailed presentation of Stable Baselines in the 
&lt;a href=&#34;https://towardsdatascience.com/stable-baselines-a-fork-of-openai-baselines-reinforcement-learning-made-easy-df87c4b2fc82&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium article&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Autonomous Racing Robot With an Arduino, a Raspberry Pi and a Pi Camera</title>
      <link>//localhost:1313/post/racing-robot/</link>
      <pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/post/racing-robot/</guid>
      <description>&lt;p&gt;Read the full article on 
&lt;a href=&#34;https://becominghuman.ai/autonomous-racing-robot-with-an-arduino-a-raspberry-pi-and-a-pi-camera-3e72819e1e63&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Racing Robot</title>
      <link>//localhost:1313/project/racing-robot/</link>
      <pubDate>Fri, 27 Oct 2017 00:00:00 +0000</pubDate>
      <guid>//localhost:1313/project/racing-robot/</guid>
      <description>&lt;p&gt;Autonomous toy racing car. CAMaleon team at the Toulouse Robot Race 2017. Humbavision team at IronCar. Medium article: 
&lt;a href=&#34;https://medium.com/@araffin/autonomous-racing-robot-with-an-arduino-a-raspberry-pi-and-a-pi-camera-3e72819e1e63&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://medium.com/@araffin/autonomous-racing-robot-with-an-arduino-a-raspberry-pi-and-a-pi-camera-3e72819e1e63&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Video of the car: 
&lt;a href=&#34;https://www.youtube.com/watch?v=xhI71ZdSh6k&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.youtube.com/watch?v=xhI71ZdSh6k&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
