<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Antonin Raffin | Homepage</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2018 - 2024</copyright><lastBuildDate>Mon, 15 May 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Automatic Hyperparameter Tuning - A Visual Guide (Part 1)</title>
      <link>/post/hyperparam-tuning/</link>
      <pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate>
      <guid>/post/hyperparam-tuning/</guid>
      <description>&lt;p&gt;When you&amp;rsquo;re building a machine learning model, you want to find the best hyperparameters to make it shine. But who has the luxury of trying out every possible combination?&lt;/p&gt;
&lt;p&gt;The good news is that automatic hyperparameter tuning can save the day. By trying out a bunch of configurations and stopping the least promising ones early, you can find the perfect hyperparameters without breaking a sweat.&lt;/p&gt;
&lt;p&gt;The trick is to allocate your &amp;ldquo;budget&amp;rdquo; (aka time and resources) wisely. You want to try out as many combinations as possible, but you don&amp;rsquo;t have an infinite amount of time.&lt;/p&gt;
&lt;p&gt;By pruning the bad trials early and focusing on the promising ones, you can find the best hyperparameters quickly and efficiently. And the best part? You can focus on more important things&amp;hellip; like drinking coffee or taking a nap.&lt;/p&gt;
&lt;p&gt;As a personal and concrete example, I used this technique on a 
&lt;a href=&#34;https://arxiv.org/abs/2209.07171&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;real elastic quadruped&lt;/a&gt; to optimize the parameters of a controller directly on the real robot (it can also be good 
&lt;a href=&#34;https://arxiv.org/abs/2310.05808&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;baseline&lt;/a&gt; for locomotion).&lt;/p&gt;
&lt;p&gt;In this blog post, I&amp;rsquo;ll explore some of the techniques for automatic hyperparameter tuning, using reinforcement learning as a concrete example.
I&amp;rsquo;ll discuss the challenges of hyperparameter optimization, and introduce different samplers and schedulers for exploring the hyperparameter space.
In part two (WIP), I&amp;rsquo;ll show how to use the 
&lt;a href=&#34;https://github.com/optuna/optuna&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optuna library&lt;/a&gt; to put these techniques into practice.&lt;/p&gt;
&lt;p&gt;If you prefer to learn with video, I recently gave this tutorial at ICRA 2022.
The 
&lt;a href=&#34;https://araffin.github.io/tools-for-robotic-rl-icra2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;, notebooks and videos are online:&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/AidFTOdGNFQ?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;div style=&#34;margin-top: 50px&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;hyperparameter-optimization-the-n-vs-bn-tradeoff&#34;&gt;Hyperparameter Optimization: The &amp;ldquo;n vs B/n&amp;rdquo; tradeoff&lt;/h2&gt;
&lt;p&gt;When you do hyperparameter tuning, you want to try a bunch of configurations &amp;ldquo;n&amp;rdquo; on a given problem.
Depending on how each trial goes, you may decide to continue or stop it early.&lt;/p&gt;
&lt;p&gt;The tradeoff you have is that you want to try as many configurations (aka sets of hyperparameters) as possible, but you don&amp;rsquo;t have an infinite budget (B).
So you have to allocate the budget you give to each configuration wisely (B/n, budget per configuration).&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./img/successive_halving_comment.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;As shown in the figure above, one way to achieve this goal is to start by giving all trials the same budget.
After some time, say 25% of the total budget, you decide to prune the least promising trials and allocate more resources to the most promising ones.&lt;/p&gt;
&lt;p&gt;You can repeat this process several times (here at 50% and 75% of the maximum budget) until you reach the budget limit.&lt;/p&gt;
&lt;p&gt;The two main components of hyperparameter tuning deal with this tradeoff:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the sampler (or search algorithm) decides which configuration to try&lt;/li&gt;
&lt;li&gt;the pruner (or scheduler) decides how to allocate the computational budget and when to stop a trial&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;samplers&#34;&gt;Samplers&lt;/h2&gt;
&lt;p&gt;So how do you sample configurations, how do you choose which set of parameters to try?&lt;/p&gt;
&lt;h3 id=&#34;the-performance-landscape&#34;&gt;The Performance Landscape&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s take a simple 2D example to illustrate the high-level idea.&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./img/perf_landscape.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;In this example, we want to obtain high returns (red area).
The performance depends on two parameters that we can tune.&lt;/p&gt;
&lt;p&gt;Of course, if we knew the performance landscape in advance, we wouldn&amp;rsquo;t need any tuning, we could directly choose the optimal parameters for our task.&lt;/p&gt;
&lt;p&gt;In this particular example, you can notice that one parameter must be tuned precisely (parameter one), while the second one can be chosen more loosely (it doesn&amp;rsquo;t impact performance much). Again, you don&amp;rsquo;t know this in advance.&lt;/p&gt;
&lt;h3 id=&#34;grid-search&#34;&gt;Grid Search&lt;/h3&gt;
&lt;p&gt;A common and inefficient way to sample hyperparameters is to discretize the search space and try all configurations: this is called grid search.&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./img/grid_search_comb.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;Grid search is simple but should be avoided.
As shown in the image above, you have to be very careful when discretizing the space:
if you are unlucky, you might completely miss the optimal parameters (the high return region in red is not part of the sampled parameters).&lt;/p&gt;
&lt;p&gt;You can have a finer discretization, but then the number of configurations will grow rapidly.
Grid search also scales very poorly with dimensions: the number of configurations you have to try grows exponentially!&lt;/p&gt;
&lt;p&gt;Finally, you may have noticed that grid search wastes resources: it allocates the same budget to important and unimportant parameters.&lt;/p&gt;
&lt;p&gt;A better but still simpler alternative to grid search is 
&lt;a href=&#34;https://www.jmlr.org/papers/v13/bergstra12a.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;random search&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;random-search&#34;&gt;Random Search&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://papers.nips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Random search&lt;/a&gt; samples the search space uniformly.&lt;/p&gt;
&lt;p&gt;It may seem counterintuitive at first that random search is better than grid search, but hopefully the diagram below will be of some help:&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./img/grid_vs_rs.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;By sampling uniformly, random search no longer depends on the discretization, making it a better starting point.
This is especially true once you have more dimensions.&lt;/p&gt;
&lt;p&gt;Of course, random search is pretty naive, so can we do better?&lt;/p&gt;
&lt;h3 id=&#34;bayesian-optimization&#34;&gt;Bayesian Optimization&lt;/h3&gt;
&lt;p&gt;One of the main ideas of 
&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-05318-5_1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Optimization&lt;/a&gt; (BO) is to learn a surrogate model that estimates, with some uncertainty, the performance of a configuration (before trying it).
In the figure below, this is the solid black line.&lt;/p&gt;
&lt;p&gt;It tries to approximate the real (unknown) objective function (dotted line).
The surrogate model comes with some uncertainty (blue area), which allows you to choose which configuration to try next.&lt;/p&gt;
&lt;p&gt;&lt;object style=&#34;margin: auto; display: block;&#34; width=&#34;60%&#34; type=&#34;image/svg+xml&#34; data=&#34;./img/bayesian_optim.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;A BO algorithm works in three steps. First, you have a current estimate of the objective function, which comes from your previous observations (configurations that have been tried).
Around these observations, the uncertainty of the surrogate model will be small.&lt;/p&gt;
&lt;p&gt;To select the next configuration to sample, BO relies on an acquisition function. This function takes into account the value of the surrogate model and the uncertainty.&lt;/p&gt;
&lt;p&gt;Here the acquisition function samples the most optimistic set of parameters given the current model (maximum of surrogate model value + uncertainty): you want to sample the point that might give you the best performance.&lt;/p&gt;
&lt;p&gt;Once you have tried this configuration, the surrogate model and acquisition function are updated with the new observation (the uncertainty around this new observation decreases), and a new iteration begins.&lt;/p&gt;
&lt;p&gt;In this example, you can see that the sampler quickly converges to a value that is close to the optimum.&lt;/p&gt;
&lt;p&gt;Gaussian Process (GP) and 
&lt;a href=&#34;https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tree of Parzen Estimators&lt;/a&gt; (TPE) algorithms both use this technique to optimize hyperparameters.&lt;/p&gt;
&lt;h3 id=&#34;other-black-box-optimization-bbo-algorithms&#34;&gt;Other Black Box Optimization (BBO) Algorithms&lt;/h3&gt;
&lt;p&gt;I won&amp;rsquo;t cover them in detail but you should also know about two additional classes of black box optimization (BBO) algorithms: 
&lt;a href=&#34;https://blog.otoro.net/2017/10/29/visual-evolution-strategies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evolution Strategies&lt;/a&gt; (ES, CMA-ES) and 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Particle_swarm_optimization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Particle Swarm Optimization&lt;/a&gt; (PSO).
Both of those approaches optimize a population of solutions that evolves over time.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Now that you&amp;rsquo;re familiar with the different samplers for automatic hyperparameter tuning, it&amp;rsquo;s time to dive into another critical aspect: pruners.
These techniques work hand in hand with the search algorithms to further improve the efficiency of the optimization process.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;schedulers--pruners&#34;&gt;Schedulers / Pruners&lt;/h2&gt;
&lt;p&gt;The job of the pruner is to identify and discard poorly performing hyperparameter configurations, eliminating them from further consideration.
This ensures that your resources are focused on the most promising candidates, saving valuable time and computating power.&lt;/p&gt;
&lt;p&gt;Deciding when to prune a trial can be tricky.
If you don&amp;rsquo;t allocate enough resources to a trial, you won&amp;rsquo;t be able to judge whether it&amp;rsquo;s a good trial or not.&lt;/p&gt;
&lt;p&gt;If you prune too aggressively, you will favor the candidates that perform well early (and then plateau) to the detriment of those that perform better with more budget.&lt;/p&gt;
&lt;h3 id=&#34;median-pruner&#34;&gt;Median Pruner&lt;/h3&gt;
&lt;p&gt;A simple but effective scheduler is the 
&lt;a href=&#34;https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.MedianPruner.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;median pruner&lt;/a&gt;, used in 
&lt;a href=&#34;https://research.google/pubs/pub46180/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Vizier&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The idea is to prune if the intermediate result of the trial is worse than the median of the intermediate results of previous trials at the same step.
In other words, at a given time, you look at the current candidate.
If it performs worse than half of the candidates at the same time, you stop it, otherwise you let it continue.&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./img/median_pruner.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;To avoid biasing the optimization toward candidates that perform well early in training, you can play with a &amp;ldquo;warmup&amp;rdquo; parameter that prevents any trial from being pruned until a minimum budget is reached.&lt;/p&gt;
&lt;h3 id=&#34;successive-halving&#34;&gt;Successive Halving&lt;/h3&gt;
&lt;p&gt;Successive halving is a slightly more advanced algorithm.
You start with many configurations and give them all a minimum budget.&lt;/p&gt;
&lt;p&gt;Then, at some intermediate step, you reduce the number of candidates and keep only the most promising ones.&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./img/successive_halving_comment.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;One limitation with this algorithm is that it has three hyperparameters (to be tuned :p!): the minimum budget, the initial number of trials and the reduction factor (what percentage of trials are discarded at each intermediate step).&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s where the 
&lt;a href=&#34;https://arxiv.org/abs/1603.06560&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hyperband&lt;/a&gt; algorithm comes in (I highly recommend reading the paper). Hyperband does a grid search on the successive halving parameters (in parallel) and thus tries different tradeoffs (remember the &amp;ldquo;n&amp;rdquo; vs. &amp;ldquo;n/B&amp;rdquo; tradeoff ;)?).&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I introduced the challenges and basic components of automatic hyperparameter tuning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the trade-off between the number of trials and the resources allocated per trial&lt;/li&gt;
&lt;li&gt;the different samplers that choose which set of parameters to try&lt;/li&gt;
&lt;li&gt;the various schedulers that decide how to allocate resources and when to stop a trial&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second part (WIP) will be about applying hyperparameter tuning in practice with the 
&lt;a href=&#34;https://github.com/optuna/optuna&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optuna&lt;/a&gt; library, using reinforcement learning as an example
(if you are impatient, the video and the colab notebook are already 
&lt;a href=&#34;https://araffin.github.io/tools-for-robotic-rl-icra2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;online&lt;/a&gt;).&lt;/p&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;All the graphics were made using 
&lt;a href=&#34;https://excalidraw.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;excalidraw&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;did-you-find-this-post-helpful-consider-sharing-it-&#34;&gt;Did you find this post helpful? Consider sharing it ðŸ™Œ&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Rliable: Better Evaluation for Reinforcement Learning - A Visual Explanation</title>
      <link>/post/rliable/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>/post/rliable/</guid>
      <description>&lt;p&gt;It is critical for Reinforcement Learning (RL) practitioners to properly evaluate and compare results.
Reporting results with poor comparison leads to a progress mirage and may underestimate the stochasticity of the results. To this end, 
&lt;a href=&#34;https://arxiv.org/abs/2108.13264&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep RL at the Edge of the Statistical Precipice&lt;/a&gt; (Neurips Oral) provides recommendations for a more rigorous evaluation of DeepRL algorithms. The paper comes with an open-source library named 
&lt;a href=&#34;https://github.com/google-research/rliable&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rliable&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This blog post is meant to be a visual explanation of the tools used by the 
&lt;a href=&#34;https://agarwl.github.io/rliable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rliable&lt;/a&gt; library to better evaluate and compare RL algorithms.
We will go through the different recommendations of the authors and give a visual explanation for each of them.&lt;/p&gt;
&lt;h2 id=&#34;score-normalization&#34;&gt;Score Normalization&lt;/h2&gt;
&lt;p&gt;To have more datapoints that just 10 random seeds, rliable recommends aggregating all N runs across all M tasks (e.g., aggregating all Atari games results) so we have a total of NxM runs from which we can sample from. To have comparable scores across tasks, we first need to normalize the scores of each run per task as follows:&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./score_norm.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;Note: the score may depend on what you want to compare. It is usually the final performance of the RL agent, after training.&lt;/p&gt;
&lt;h2 id=&#34;stratified-bootstrap-confidence-intervals&#34;&gt;Stratified Bootstrap Confidence Intervals&lt;/h2&gt;
&lt;p&gt;To account for uncertainty in aggregate performance, rliable uses stratified bootstrap confidence intervals.
This may sound complicated, but let&amp;rsquo;s go slowly through the meaning of each of those terms.&lt;/p&gt;
&lt;p&gt;First, bootstrap means sampling with replacement. For instance, if we sample four times with replacement 3 runs of indices [1, 2, 3] on a task A, we may get: [2, 2, 3, 1] the first time, [3, 1, 1, 1] the second time, &amp;hellip;&lt;/p&gt;
&lt;p&gt;Stratified bootstrap means that we first group our datapoints into buckets (or strata), and then sample with replacement each of those buckets according to their size:&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./stratified_bootstrap.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;In RL, the buckets are the different tasks or environments. With stratified bootstrap, all tasks are always represented in the sampled runs. This avoids computing the aggregate metrics only on a subset of all the environments:&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./bootstrap_rl.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;Each time we sample with replacement the runs, we compute the different metrics (for instance, mean score) for those sampled runs. To report uncertainty, rliable computes 
&lt;a href=&#34;https://acclab.github.io/bootstrap-confidence-intervals.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bootstrap confidence intervals&lt;/a&gt; (CIs) following the 
&lt;a href=&#34;https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;percentiles&amp;rsquo; method&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./bootstrap_ci.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;Note: there are other methods for computing CI with bootstrap, but percentiles was found by the authors to work well in practice.&lt;/p&gt;
&lt;h2 id=&#34;interquartile-mean-iqm&#34;&gt;Interquartile Mean (IQM)&lt;/h2&gt;
&lt;p&gt;To summarize benchmark performance, it is common to report mean/median performance of the runs.
However, mean is known to be sensible to outliers and median may not reflect enough the distribution of scores, so rliable suggests to use Interquartile Mean (IQM) instead:&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./iqm.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;h2 id=&#34;performance-profiles&#34;&gt;Performance Profiles&lt;/h2&gt;
&lt;p&gt;To report performance variability across tasks and runs, the authors proposes to use performance profiles.
It tells for a given target performance (for example, 60% of the reference performance) the proportion of runs that achieve it.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Performance Profile&#34; src=&#34;./perf_profile.jpg&#34;&gt;
Source: image from the authors of the rliable library&lt;/p&gt;
&lt;h2 id=&#34;probability-of-improvement&#34;&gt;Probability of Improvement&lt;/h2&gt;
&lt;p&gt;Finally, to test whether an algorithm X is probably better or not than an algorithm Y, rliable uses the U-statistic from a 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mannâ€“Whitney U test&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./proba_improvement.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;The probability of improvement is then average over the tasks.
A probability of improvement around 0.5 means that the two algorithms have similar performances.&lt;/p&gt;
&lt;h2 id=&#34;in-practice-using-the-rl-zoo&#34;&gt;In Practice: Using the RL Zoo&lt;/h2&gt;
&lt;p&gt;To allow more users to use rliable, we added basic support of it in the 
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo#plot-with-the-rliable-library&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL Baselines3 Zoo&lt;/a&gt;, a training framework for 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable-Baselines3&lt;/a&gt;. Fore more information, please follow the instructions in the 
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo#plot-with-the-rliable-library&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;README&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen the different tools used by rliable to better evaluate RL algorithms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;score normalization to aggregate scores across tasks&lt;/li&gt;
&lt;li&gt;stratified bootstrap to provide proper confidence intervals&lt;/li&gt;
&lt;li&gt;interquartile mean (IQM) to summarize benchmark performance&lt;/li&gt;
&lt;li&gt;performance profile for an overview of the results and their variability&lt;/li&gt;
&lt;li&gt;probability of improvement to compare two algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;I would like to thank 
&lt;a href=&#34;https://psc-g.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pablo Samuel Castro&lt;/a&gt; and 
&lt;a href=&#34;https://agarwl.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rishabh Agarwal&lt;/a&gt; for checking the correctness of the visuals.&lt;/p&gt;
&lt;p&gt;All the graphics were made using 
&lt;a href=&#34;https://excalidraw.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;excalidraw&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;did-you-find-this-post-helpful-consider-sharing-it-&#34;&gt;Did you find this post helpful? Consider sharing it ðŸ™Œ&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Stable-Baselines3: Reliable Reinforcement Learning Implementations</title>
      <link>/post/sb3/</link>
      <pubDate>Sun, 28 Feb 2021 00:00:00 +0000</pubDate>
      <guid>/post/sb3/</guid>
      <description>&lt;p&gt;After several months of beta, we are happy to announce the release of 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable-Baselines3 (SB3)&lt;/a&gt; v1.0, a set of reliable implementations of reinforcement learning (RL) algorithms in PyTorch =D! It is the next major version of 
&lt;a href=&#34;https://github.com/hill-a/stable-baselines&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable Baselines&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The implementations have been 
&lt;a href=&#34;https://arxiv.org/abs/2005.05719&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;benchmarked&lt;/a&gt; against reference codebases, and automated unit tests cover 95% of the code.&lt;/p&gt;
&lt;p&gt;In this blog post, we give you an overview of Stable-Baselines3: the motivation behind it, its design principles and features, how we ensure high-quality implementations and some concrete examples.&lt;/p&gt;
&lt;!-- The algorithms follow a consistent interface and are accompanied by extensive documentation, making it simple to train and compare different RL algorithms. --&gt;
&lt;h2 id=&#34;tldr&#34;&gt;TL;DR:&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable-Baselines3 (SB3)&lt;/a&gt; is a library providing &lt;em&gt;reliable&lt;/em&gt; implementations of reinforcement learning algorithms in PyTorch. It provides a &lt;em&gt;clean and simple interface&lt;/em&gt;, giving you access to off-the-shelf state-of-the-art model-free RL algorithms.&lt;/p&gt;
&lt;p&gt;The library is &lt;em&gt;
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fully documented&lt;/a&gt;&lt;/em&gt;, tested and its interface allows to train an RL agent in only few lines of code =):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gym
from stable_baselines3 import SAC
# Train an agent using Soft Actor-Critic on Pendulum-v0
env = gym.make(&amp;quot;Pendulum-v0&amp;quot;)
model = SAC(&amp;quot;MlpPolicy&amp;quot;, env, verbose=1)
# Train the model
model.learn(total_timesteps=20000)
# Save the model
model.save(&amp;quot;sac_pendulum&amp;quot;)
# Load the trained model
model = SAC.load(&amp;quot;sac_pendulum&amp;quot;)
# Start a new episode
obs = env.reset()
# What action to take in state `obs`?
action, _ = model.predict(obs, deterministic=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where defining and training a RL agent can be written in two lines of code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_baselines3 import PPO
# Train an agent using Proximal Policy Optimization on CartPole-v1
model = PPO(&amp;quot;MlpPolicy&amp;quot;, &amp;quot;CartPole-v1&amp;quot;).learn(total_timesteps=20000)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;
&lt;p&gt;GitHub repository: 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/DLR-RM/stable-baselines3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Paper: 
&lt;a href=&#34;http://jmlr.org/papers/v22/20-1364.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://jmlr.org/papers/v22/20-1364.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Documentation: 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://stable-baselines3.readthedocs.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RL Baselines3 Zoo: 
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/DLR-RM/rl-baselines3-zoo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Contrib: 
&lt;a href=&#34;https://github.com/Stable-Baselines-Team/stable-baselines3-contrib&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Stable-Baselines-Team/stable-baselines3-contrib&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RL Tutorial: 
&lt;a href=&#34;https://github.com/araffin/rl-tutorial-jnrr19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/araffin/rl-tutorial-jnrr19&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Deep reinforcement learning (RL) research has grown rapidly in recent years, yet results are often 
&lt;a href=&#34;https://arxiv.org/abs/1709.06560&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;difficult to reproduce&lt;/a&gt;.
A major challenge is that small implementation details can have a substantial effect on performance &amp;ndash; often greater than the 
&lt;a href=&#34;https://iclr.cc/virtual_2020/poster_r1etN1rtPB.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;difference between algorithms&lt;/a&gt;.
It is particularly important that implementations used as experimental &lt;em&gt;baselines&lt;/em&gt; are reliable; otherwise, novel algorithms compared to weak baselines lead to inflated estimates of performance improvements.&lt;/p&gt;
&lt;p&gt;To help with this problem, we present Stable-Baselines3 (SB3), an open-source framework implementing seven commonly used model-free deep RL algorithms, relying on the 
&lt;a href=&#34;https://github.com/openai/gym&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI Gym interface&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We take great care to adhere to software engineering best practices to achieve high-quality implementations that match prior results.&lt;/p&gt;
&lt;h2 id=&#34;history&#34;&gt;History&lt;/h2&gt;
&lt;p&gt;SB3 builds on our experience maintaining &lt;em&gt;
&lt;a href=&#34;https://github.com/hill-a/stable-baselines&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable Baselines&lt;/a&gt;&lt;/em&gt; (SB2), a fork of OpenAI Baselines built on TensorFlow 1.x.
If you haven&amp;rsquo;t heard of it, Stable-Baselines (SB2) is a trusted library and has already been used in &lt;em&gt;many 
&lt;a href=&#34;https://stable-baselines.readthedocs.io/en/master/misc/projects.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;projects&lt;/a&gt;&lt;/em&gt; and &lt;em&gt;
&lt;a href=&#34;https://scholar.google.fr/scholar?oi=bibs&amp;amp;hl=fr&amp;amp;cites=7029285800852969820&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;papers&lt;/a&gt;&lt;/em&gt; with already more than 300+ citations!&lt;/p&gt;
&lt;p&gt;Those two years of maintaining SB2 have been a rewarding exchange with our users, where tons of bugs where fixed and new features like callbacks where added to ease the use of the library.&lt;/p&gt;
&lt;p&gt;However, SB2 was still relying on OpenAI Baselines initial codebase and with the upcoming release of Tensorflow 2, more and more internal TF code was being deprecated.&lt;/p&gt;
&lt;p&gt;After discussing the matter with the community, we decided to go for a complete rewrite in PyTorch (cf issues 
&lt;a href=&#34;https://github.com/hill-a/stable-baselines/issues/366&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#366&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/hill-a/stable-baselines/issues/576&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#576&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/hill-a/stable-baselines/issues/733&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#733&lt;/a&gt;), codename: Stable-Baselines3&lt;sup&gt;1&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Stable-Baselines3 keeps the same easy-to-use API while improving a lot on the internal code, in particular by adding static type checking.&lt;/p&gt;
&lt;p&gt;Re-starting almost from scratch is long-term investment: it took 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quite some effort&lt;/a&gt; and time but we now have a smaller, cleaner and reliable core that is easier to maintain and extend =).&lt;/p&gt;
&lt;p&gt;There are already 
&lt;a href=&#34;https://github.com/search?p=1&amp;amp;q=reinforcement&amp;#43;learning&amp;#43;library&amp;amp;type=Repositories&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;many&lt;/a&gt; open source reinforcement learning libraries (almost one new every week), so why did we create a new one? In the next sections you will learn about the design principles and main features of the Stable-Baselines3 library that differenciate it from others.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; The very first name of the new version was &amp;ldquo;torchy-baselines&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;design-principles&#34;&gt;Design Principles&lt;/h2&gt;
&lt;p&gt;Our main goal is to provide a user-friendly and reliable RL library.
To keep SB3 simple to use and maintain, we focus on model-free, single-agent RL algorithms, and rely on external projects to extend the scope to 
&lt;a href=&#34;https://github.com/HumanCompatibleAI/imitation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;imitation&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/takuseno/d3rlpy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;offline&lt;/a&gt; learning.&lt;/p&gt;
&lt;p&gt;We prioritize maintaining &lt;em&gt;stable&lt;/em&gt; implementations over adding new features or algorithms, and avoid making breaking changes.
We provide a consistent, clean and fully documented API, inspired by the 
&lt;a href=&#34;https://scikit-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scikit-learn&lt;/a&gt; API.&lt;/p&gt;
&lt;p&gt;Our code is 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/developer.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;easily modifiable&lt;/a&gt; by users as we favour readability and simplicity over modularity, although we make use of object-oriented programming to reduce code duplication.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;p&gt;Stable-Baselines3 provides many features, ranging from a simple API to a complete 
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;experimental framework&lt;/a&gt; that allows advanced usage like automatic hyperparameters tuning.&lt;/p&gt;
&lt;h3 id=&#34;simple-api&#34;&gt;Simple API&lt;/h3&gt;
&lt;p&gt;Training agents in Stable-Baselines3 takes just a few lines of code, after which the agent can be queried for actions (see quick example below).
This allows you to easily use the baseline algorithms and components in your experiments (eg. 
&lt;a href=&#34;https://xbpeng.github.io/projects/Robotic_Imitation/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imitating Animals&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/hardmaru/slimevolleygym&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slime Volleyball&lt;/a&gt;, 
&lt;a href=&#34;https://adversarialpolicies.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adversarial Policies&lt;/a&gt;), as well as apply RL to novel tasks and environments, like 
&lt;a href=&#34;https://pwnagotchi.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;continual learning&lt;/a&gt; when attacking WiFi networks or 
&lt;a href=&#34;https://github.com/jaberkow/WaveRL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dampening bridge vibrations&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gym

from stable_baselines3 import A2C
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback

# Save a checkpoint every 1000 steps
checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=&amp;quot;./logs/&amp;quot;,
                                         name_prefix=&amp;quot;rl_model&amp;quot;)

# Evaluate the model periodically
# and auto-save the best model and evaluations
# Use a monitor wrapper to properly report episode stats
eval_env = Monitor(gym.make(&amp;quot;LunarLander-v2&amp;quot;))
# Use deterministic actions for evaluation
eval_callback = EvalCallback(eval_env, best_model_save_path=&amp;quot;./logs/&amp;quot;,
                             log_path=&amp;quot;./logs/&amp;quot;, eval_freq=2000,
                             deterministic=True, render=False)

# Train an agent using A2C on LunarLander-v2
model = A2C(&amp;quot;MlpPolicy&amp;quot;, &amp;quot;LunarLander-v2&amp;quot;, verbose=1)
model.learn(total_timesteps=20000, callback=[checkpoint_callback, eval_callback])

# Retrieve and reset the environment
env = model.get_env()
obs = env.reset()

# Query the agent (stochastic action here)
action, _ = model.predict(obs, deterministic=False)

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;documentation&#34;&gt;Documentation&lt;/h3&gt;
&lt;p&gt;SB3 comes with 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;extensive documentation&lt;/a&gt; of the code API.
We also include a user guide, covering both basic and more advanced usage with a collection of concrete examples.
Moreover, we have developed a 
&lt;a href=&#34;https://github.com/araffin/rl-tutorial-jnrr19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab notebook based RL tutorial&lt;/a&gt;, so you can demo the library directly in the browser.
Additionally, we include 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;common tips&lt;/a&gt; for running RL experiments and a 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/developer.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;developer guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We also pay close attention to 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues?q=is%3Aissue&amp;#43;is%3Aopen&amp;#43;label%3Aquestion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;questions&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues?q=is%3Aissue&amp;#43;is%3Aopen&amp;#43;label%3Adocumentation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;uncertainties&lt;/a&gt; from SB3 users, updating the documentation to address these.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Documentation&#34; src=&#34;./doc.png&#34;&gt;
&lt;em&gt;Stable-Baselines3 Documentation&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;high-quality-implementations&#34;&gt;High-Quality Implementations&lt;/h3&gt;
&lt;p&gt;Algorithms are verified against published results by comparing the agent learning curves (cf issues 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues/48&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#48&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues/49&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#48&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;As an example, to compare against TD3 and SAC original implementation, we integrated SB3 callbacks and made sure both SB3 and original implementations were using the same hyperparameters (the code diff for SAC and TD3 repos can be found 
&lt;a href=&#34;https://github.com/rail-berkeley/softlearning/compare/master...Artemis-Skade:master&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/sfujim/TD3/compare/master...araffin:master&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;there&lt;/a&gt;).&lt;/p&gt;
&lt;!-- https://github.com/Artemis-Skade/softlearning --&gt;
&lt;!-- https://github.com/araffin/TD3 --&gt;
&lt;p&gt;During this period, that&amp;rsquo;s how we realized some tricky details that made a big difference.
For example, PyTorch RMSProp is different from TensorFlow one (we include a 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/sb2_compat/rmsprop_tf_like.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;custom version&lt;/a&gt; inside our codebase), and the &lt;code&gt;epsilon&lt;/code&gt; value of the optimizer can make a 
&lt;a href=&#34;https://twitter.com/araffin2/status/1329382226421837825&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;big difference&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;A2C&#34; src=&#34;./a2c.png&#34;&gt;
&lt;em&gt;A and B are actually the same RL algorithm (A2C), sharing the exact same code, same hardware, same hyperparameters&amp;hellip; except the epsilon value to avoid division by zero in the optimizer (one is &lt;code&gt;eps=1e-5&lt;/code&gt;, the other &lt;code&gt;eps=1e-7&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Despite all those tricky details (and other 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues/105&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nasty bugs&lt;/a&gt;), at the end, we managed to match SB2 results and original implementations closely:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;A2C&#34; src=&#34;./a2c_comp.png&#34;&gt;
&lt;em&gt;Stable-Baselines (SB2) vs Stable-Baselines3 (SB3) A2C result on CartPole-v1&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Breakout&#34; src=&#34;./Result_Breakout-1.png&#34;&gt;
&lt;em&gt;Stable-Baselines (SB2) vs Stable-Baselines3 (SB3) results on BreakoutNoFrameskip-v4&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;HalfCheetah&#34; src=&#34;./Result_HalfCheetah-1.png&#34;&gt;
&lt;em&gt;Stable-Baselines3 (SB3) vs original implementations results on HalfCheetahBulletEnv-v0&lt;/em&gt;&lt;/p&gt;
&lt;!-- Moreover, all functions are typed (parameter and return types) and documented with a consistent style, and most functions are covered by unit tests. --&gt;
&lt;!-- Continuous integration checks that all changes pass unit tests and type check, as well as validating the code style and documentation. --&gt;
&lt;h3 id=&#34;comprehensive&#34;&gt;Comprehensive&lt;/h3&gt;
&lt;p&gt;Stable-Baselines3 contains the following state-of-the-art on- and off-policy algorithms, commonly used as experimental baselines: A2C, DDPG, DQN, HER, PPO, SAC and TD3.&lt;/p&gt;
&lt;p&gt;Moreover, SB3 provides various algorithm-independent features. We support logging to CSV files and 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/tensorboard.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TensorBoard&lt;/a&gt;. Users can log custom metrics and modify training via user-provided 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;callbacks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To speed up training, we support parallel (or &amp;ldquo;vectorized&amp;rdquo;) environments. To simplify training, we implement common environment wrappers, like preprocessing Atari observations to match the original DQN experiments.&lt;/p&gt;
&lt;video controls&gt;
 &lt;source src=&#34;./tb_video.mp4&#34; type=&#34;video/mp4&#34;&gt;
Your browser does not support the video tag.
&lt;/video&gt;
&lt;p&gt;&lt;em&gt;Tensorboard video integration&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;experimental-framework&#34;&gt;Experimental Framework&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL Baselines Zoo&lt;/a&gt; provides scripts to train and evaluate agents, tune hyperparameters, record videos, store experiment setup and visualize results.
We also include a collection of pre-trained reinforcement learning agents together with tuned hyperparameters for simple control tasks, 
&lt;a href=&#34;https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/gym/pybullet_envs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyBullet&lt;/a&gt; environments and Atari games, optimized using 
&lt;a href=&#34;https://github.com/pfnet/optuna&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optuna&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We follow best practices for training and evaluation, such as evaluating in a separate environment, using deterministic evaluation where required (SAC) and storing all hyperparameters necessary to replicate the experiment.&lt;/p&gt;
&lt;p&gt;Below, you can see basic usage of the RL zoo (training, loading, tuning hyperparameters), which has a simple command line:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Train an A2C agent on Atari breakout using tuned hyperparameters,
# evaluate the agent every 10k steps and save a checkpoint every 50k steps
python train.py --algo a2c --env BreakoutNoFrameskip-v4 \
    --eval-freq 10000 --save-freq 50000
# Plot the learning curve
python scripts/all_plots.py -a a2c -e BreakoutNoFrameskip-v4 -f logs/

# Load and evaluate a trained agent for 1000 steps
# optionally, you can also load a checkpoint using --load-checkpoint
python enjoy.py --algo sac --env Pendulum-v0 -n 1000

# Tune the hyperparameters of ppo on BipedalWalker-v3 with a budget of 50 trials
# using 2 parallel jobs, a TPE sampler and median pruner
python train.py --algo ppo --env BipedalWalker-v3 -optimize --n-trials 50 \
    --n-jobs 2 --sampler tpe --pruner median

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;stable-baselines3-contrib&#34;&gt;Stable-Baselines3 Contrib&lt;/h3&gt;
&lt;p&gt;We implement experimental features in a separate 
&lt;a href=&#34;https://github.com/Stable-Baselines-Team/stable-baselines3-contrib&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;contrib repository&lt;/a&gt;.
This allows SB3 to maintain a stable and compact core, while still providing the latest features, like 
&lt;a href=&#34;https://sb3-contrib.readthedocs.io/en/master/modules/qrdqn.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantile Regression DQN (QR-DQN)&lt;/a&gt; or 
&lt;a href=&#34;https://sb3-contrib.readthedocs.io/en/master/modules/tqc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Truncated Quantile Critics (TQC)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Implementations in contrib need not be tightly integrated with the main SB3 codebase, but we maintain the same stringent review requirements to ensure users can trust the contrib implementations.
Implementations from contrib that have stood the test of time may be integrated into the main repository.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sb3_contrib import QRDQN, TQC

# Train an agent using QR-DQN on Acrobot-v0
model = QRDQN(&amp;quot;MlpPolicy&amp;quot;, &amp;quot;Acrobot-v0&amp;quot;).learn(total_timesteps=20000)
# Train an agent using Truncated Quantile Critics on Pendulum-v0
model = TQC(&amp;quot;MlpPolicy&amp;quot;, &amp;quot;Pendulum-v0&amp;quot;).learn(total_timesteps=20000)
&lt;/code&gt;&lt;/pre&gt;
&lt;!-- ## What&#39;s new? --&gt;
&lt;h2 id=&#34;migration-from-stable-baselines-sb2&#34;&gt;Migration from Stable-Baselines (SB2)&lt;/h2&gt;
&lt;p&gt;If you are Stable-Baselines (SB2) user and would like to switch to SB3, we have a 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/migration.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;migration guide&lt;/a&gt; waiting for you ;)&lt;/p&gt;
&lt;p&gt;Most of the time, it only requires to change the import &lt;code&gt;from stable_baselines&lt;/code&gt; by &lt;code&gt;from stable_baselines3&lt;/code&gt; and rename some parameters.&lt;/p&gt;
&lt;p&gt;For instance, if your code was like that for Stable-Baselines:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_baselines import PPO2
from stable_baselines.common.cmd_util import make_atari_env

env = make_atari_env(&amp;quot;BreakoutNoFrameskip-v4&amp;quot;, num_env=8, seed=21)

model = PPO2(&amp;quot;MlpPolicy&amp;quot;, env, n_steps=128, nminibatches=4,
              noptepochs=4, ent_coef=0.01, verbose=1)

model.learn(int(1e5))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the corresponding SB3 code is:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_baselines3 import PPO
# cmd_util was renamed env_util for clarity
from stable_baselines3.common.env_util import make_atari_env

# num_env was renamed n_envs
env = make_atari_env(&amp;quot;BreakoutNoFrameskip-v4&amp;quot;, n_envs=8, seed=21)

# we use batch_size instead of nminibatches which
#Â was dependent on the number of environments
# batch_size = (n_steps * n_envs) // nminibatches = 256
# noptepochs was renamed n_epochs
model = PPO(&amp;quot;MlpPolicy&amp;quot;, env, n_steps=128, batch_size=256,
            n_epochs=4, ent_coef=0.01, verbose=1)

model.learn(int(1e5))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For a complete migration example, you can also compare the 
&lt;a href=&#34;https://github.com/araffin/rl-baselines-zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL Zoo of SB2&lt;/a&gt; with the 
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;one from SB3&lt;/a&gt;.&lt;/p&gt;
&lt;div style=&#34;margin-top:3em&#34;&gt;&lt;/div&gt;
&lt;hr/&gt;
&lt;div style=&#34;margin-top:3em&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s see now how we can now use the library in practice with some examples. We&amp;rsquo;re going to see how to easily customize the network architecture, train an agent to play Atari games and normalize observations when training on continuous control tasks like PyBullet environments.&lt;/p&gt;
&lt;p&gt;For each of them, you can try it online using 
&lt;a href=&#34;https://github.com/Stable-Baselines-Team/rl-colab-notebooks/tree/sb3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google colab notebook&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;custom-policy-network&#34;&gt;Custom Policy Network&lt;/h3&gt;
&lt;p&gt;To
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;customize a policy&lt;/a&gt; with SB3, all you need to do is choose a network architecture and pass a &lt;code&gt;policy_kwargs&lt;/code&gt; (&amp;ldquo;policy keyword arguments&amp;rdquo;) to the algorithm constructor.&lt;/p&gt;
&lt;p&gt;The following snippet shows how to customize the architecture and activation function for one on-policy (PPO) and one off-policy (SAC) algorithm:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch as th

from stable_baselines3 import PPO, SAC

# Custom actor (pi) and value function (vf) networks
# of two layers of size 32 each with Relu activation function
policy_kwargs = dict(activation_fn=th.nn.ReLU,
                     net_arch=dict(pi=[32, 32], vf=[32, 32]))
# Create the agent
model = PPO(&amp;quot;MlpPolicy&amp;quot;, &amp;quot;CartPole-v1&amp;quot;, policy_kwargs=policy_kwargs, verbose=1)

# Custom actor architecture with two layers of 64 units each
# Custom critic architecture with two layers of 400 and 300 units
policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[400, 300]))
# Create the agent
model = SAC(&amp;quot;MlpPolicy&amp;quot;, &amp;quot;Pendulum-v0&amp;quot;, policy_kwargs=policy_kwargs, verbose=1)
model.learn(5000)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;atari-games&#34;&gt;Atari Games&lt;/h3&gt;
&lt;p&gt;Training a RL agent on Atari games is straightforward thanks to &lt;code&gt;make_atari_env&lt;/code&gt; helper function and the 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecframestack&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VecFrameStack&lt;/a&gt; wrapper. It will do all the 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;preprocessing&lt;/a&gt; and multiprocessing for you.&lt;/p&gt;
&lt;p&gt;Colab link: 
&lt;a href=&#34;https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/atari_games.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Try it online&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack
from stable_baselines3 import A2C

# There already exists an environment generator
# that will make and wrap atari environments correctly.
# Here we are also multi-worker training (n_envs=4 =&amp;gt; 4 environments)
env = make_atari_env(&#39;PongNoFrameskip-v4&#39;, n_envs=4, seed=0)
# Frame-stacking with 4 frames
env = VecFrameStack(env, n_stack=4)

model = A2C(&#39;CnnPolicy&#39;, env, verbose=1)
model.learn(total_timesteps=25000)

obs = env.reset()
while True:
    # By default, deterministic=False, so we use the stochastic policy
    action, _states = model.predict(obs)
    obs, rewards, dones, info = env.step(action)
    env.render()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;pybullet-normalizing-input-features&#34;&gt;PyBullet: Normalizing Input Features&lt;/h3&gt;
&lt;p&gt;Normalizing input features may be essential to successful training of an RL agent (by default, images are scaled but not other types of input), for instance when training on 
&lt;a href=&#34;https://github.com/bulletphysics/bullet3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyBullet&lt;/a&gt; environments. For that, a wrapper exists and will compute a running average and standard deviation of input features (it can do the same for rewards).&lt;/p&gt;
&lt;p&gt;Colab link: 
&lt;a href=&#34;https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/pybullet.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Try it online&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import gym
import pybullet_envs

from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3 import PPO

env = DummyVecEnv([lambda: gym.make(&amp;quot;HalfCheetahBulletEnv-v0&amp;quot;)])
# Automatically normalize the input features and reward
env = VecNormalize(env, norm_obs=True, norm_reward=True,
                   clip_obs=10.)

model = PPO(&#39;MlpPolicy&#39;, env)
model.learn(total_timesteps=2000)

# Don&#39;t forget to save the VecNormalize statistics when saving the agent
log_dir = &amp;quot;/tmp/&amp;quot;
model.save(log_dir + &amp;quot;ppo_halfcheetah&amp;quot;)
stats_path = os.path.join(log_dir, &amp;quot;vec_normalize.pkl&amp;quot;)
env.save(stats_path)

# To demonstrate loading
del model, env

# Load the agent
model = PPO.load(log_dir + &amp;quot;ppo_halfcheetah&amp;quot;)

# Load the saved statistics
env = DummyVecEnv([lambda: gym.make(&amp;quot;HalfCheetahBulletEnv-v0&amp;quot;)])
env = VecNormalize.load(stats_path, env)
#  do not update them at test time
env.training = False
# reward normalization is not needed at test time
env.norm_reward = False

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;more-examples&#34;&gt;More Examples&lt;/h3&gt;
&lt;p&gt;You can find more examples and associated colab notebooks in the 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/examples.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;to-the-infinity-and-beyond&#34;&gt;To the Infinity and Beyond!&lt;/h2&gt;
&lt;p&gt;We presented Stable-Baselines3 v1.0, a set of reliable reinforcement learning implementations and the next major version of the Stable-Baselines.&lt;/p&gt;
&lt;p&gt;If you want to follow the updates of the library, we encourage you to star the repo on 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt; and click on &amp;ldquo;Watch -&amp;gt; Custom -&amp;gt; Releases&amp;rdquo; to be notified each time a new version is released ;) (you can also follow 
&lt;a href=&#34;https://twitter.com/ARGleave&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adam&lt;/a&gt; or 
&lt;a href=&#34;https://twitter.com/araffin2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Antonin&lt;/a&gt; on Twitter).
Coming soon, one of our long-time requested feature: 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/pull/243&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mixed observations&lt;/a&gt; (aka dict obs) support.&lt;/p&gt;
&lt;p&gt;In case you want to contribute, make sure to read the 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/blob/master/CONTRIBUTING.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;contributing guide&lt;/a&gt; first.&lt;/p&gt;
&lt;p&gt;Finally, if you make a cool project using Stable-Baselines3, please tell us when you want it to appear in 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/misc/projects.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;our project page&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;about-the-authors&#34;&gt;About the Authors&lt;/h2&gt;
&lt;p&gt;This blog post was co-written by Stable-Baselines3 maintainers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/araffin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Antonin Raffin&lt;/a&gt; (@araffin)&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hill-a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ashley Hill&lt;/a&gt; (@hill-a)&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/ernestum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maximilian Ernestus&lt;/a&gt; (@ernestum)&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/adamgleave&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adam Gleave&lt;/a&gt; (@AdamGleave)&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/Miffyli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anssi Kanervisto&lt;/a&gt; (@Miffyli).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citing-the-project&#34;&gt;Citing the Project&lt;/h2&gt;
&lt;p&gt;To cite Stable-Baselines3 in publications:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;did-you-find-this-post-helpful-consider-sharing-it-&#34;&gt;Did you find this post helpful? Consider sharing it ðŸ™Œ&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Learning to Drive Smoothly in Minutes</title>
      <link>/post/learning-to-drive/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/post/learning-to-drive/</guid>
      <description>&lt;p&gt;Read the full article on 
&lt;a href=&#34;https://towardsdatascience.com/learning-to-drive-smoothly-in-minutes-450a7cdb35f4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stable Baselines: a Fork of OpenAI Baselinesâ€Šâ€”â€ŠReinforcement Learning Made Easy</title>
      <link>/post/stable-baselines/</link>
      <pubDate>Mon, 20 Aug 2018 00:00:00 +0000</pubDate>
      <guid>/post/stable-baselines/</guid>
      <description>&lt;p&gt;Read the full article on 
&lt;a href=&#34;https://towardsdatascience.com/stable-baselines-a-fork-of-openai-baselines-reinforcement-learning-made-easy-df87c4b2fc82&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simple and Robust {Computerâ€Šâ€”â€ŠArduino} Serial Communication</title>
      <link>/post/arduino-serial/</link>
      <pubDate>Sat, 10 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/post/arduino-serial/</guid>
      <description>&lt;p&gt;Read the full article on 
&lt;a href=&#34;https://medium.com/@araffin/simple-and-robust-computer-arduino-serial-communication-f91b95596788&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Autonomous Racing Robot With an Arduino, a Raspberry Pi and a Pi Camera</title>
      <link>/post/racing-robot/</link>
      <pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate>
      <guid>/post/racing-robot/</guid>
      <description>&lt;p&gt;Read the full article on 
&lt;a href=&#34;https://becominghuman.ai/autonomous-racing-robot-with-an-arduino-a-raspberry-pi-and-a-pi-camera-3e72819e1e63&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
