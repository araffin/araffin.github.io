<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Antonin Raffin | Homepage</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© 2018 - 2025</copyright><lastBuildDate>Tue, 16 Sep 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>img/map[gravatar:%!s(bool=false) shape:circle]</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>RL102: From Tabular Q-Learning to Deep Q-Learning (DQN)</title>
      <link>/post/rl102/</link>
      <pubDate>Tue, 16 Sep 2025 00:00:00 +0000</pubDate>
      <guid>/post/rl102/</guid>
      <description>&lt;p&gt;This blog post is meant to be a practical introduction to (deep) reinforcement learning&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;, presenting the main concepts and providing intuitions to understand the more recent Deep RL algorithms.
For a more in-depth and theoretical introduction, I recommend reading the 
&lt;a href=&#34;http://incompleteideas.net/book/the-book-2nd.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL Bible&lt;/a&gt; by Sutton and Barto.&lt;/p&gt;
&lt;p&gt;The plan for this post is to start from tabular Q-learning and work our way up to Deep Q-learning (DQN).
In a following post, I will continue on to the Soft Actor-Critic (SAC) algorithm and its extensions.&lt;/p&gt;
&lt;p&gt;The associated code and notebooks for this tutorial can be found on GitHub: 
&lt;a href=&#34;https://github.com/araffin/rlss23-dqn-tutorial&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/araffin/rlss23-dqn-tutorial&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note: this post is a written version of the 
&lt;a href=&#34;https://araffin.github.io/talk/rlss-23/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tutorial&lt;/a&gt; I gave at the 
&lt;a href=&#34;https://rlsummerschool.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL Summer School 2023&lt;/a&gt;. It is also part of my PhD Thesis (to be published).&lt;/p&gt;
&lt;p&gt;Note: I assume you are familiar with basic RL concepts, if not please have a look at this 
&lt;a href=&#34;https://spinningup.openai.com/en/latest/spinningup/rl_intro.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL 101&lt;/a&gt; tutorial.
I provide a very short formal introduction to the RL terminology used in this post at the end of it.&lt;/p&gt;
&lt;h2 id=&#34;tabular-rl-a-discrete-world&#34;&gt;Tabular RL: A Discrete World&lt;/h2&gt;
&lt;p&gt;In tabular RL, states and actions are discrete, so in this setting it is possible to represent the world as a large table.
Each entry corresponds to a state and can be subdivided by the number of possible actions in that state.&lt;/p&gt;
&lt;img style=&#34;width:70%&#34; src=&#34;./img/q_table.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;
Illustration of a $Q$-Table representing the expected outcomes of actions for each state.
&lt;/p&gt;
&lt;h2 id=&#34;action-value-function-q-function&#34;&gt;Action-Value Function ($Q$-function)&lt;/h2&gt;
&lt;p&gt;One key element to solve the discounted 
&lt;a href=&#34;https://spinningup.openai.com/en/latest/spinningup/rl_intro.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL problem&lt;/a&gt; is the action-value function, or $Q$-function, noted $Q^{\pi}$ for a given policy $\pi$.
It is defined as the expected discounted return starting in state $s$, taking action $a$, and following policy $\pi$:&lt;/p&gt;
&lt;!--Q^{\pi}(s, a) = \mathop{\mathbb{E}}_{\tau \sim \pi}\left[{r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots \left| s_t = s, a_t = a\right.}\right].--&gt;
&lt;img style=&#34;height: 45px;&#34; src=&#34;./img/q_eq23.svg&#34;/&gt;
&lt;p&gt;In other words, the $Q$-function gives an estimate of &lt;em&gt;how good it is to take the action $a$ in state $s$ while following a policy $\pi(s)$&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The $Q$-function can be estimated recursively, also known as the Bellman equation:&lt;/p&gt;
&lt;!--Q^{\pi}(s, a) = \mathop{\mathbb{E}}_{s&#39;\sim P}\left[ r(s,a) + \gamma \mathop{\mathbb{E}}_{a&#39;\sim \pi}\left[ Q^{\pi}(s&#39;,a&#39;) \right] \right].--&gt;
&lt;img id=&#34;bellman-eq&#34; style=&#34;height: 55px;&#34; src=&#34;./img/q_eq24.svg&#34;/&gt;
&lt;p&gt;This rewrite allows to build an estimate of the $Q$-value without having to wait for terminal states.
It is the formula used in practice.&lt;/p&gt;
&lt;p&gt;By definition of the optimal policy, which selects the actions that &lt;strong&gt;max&lt;/strong&gt;imize the &lt;span style=&#34;color: #5F3DC4&#34;&gt;expected return&lt;/span&gt;, the following optimal $Q$-function Bellman equation is obtained:&lt;/p&gt;
&lt;!--Q^{\ast}(s,a) = \mathop{\mathbb{E}}_{s&#39;\sim P}\left[ r(s,a) + \gamma \boldsymbol{\max_{a&#39;}} \color{#5F3DC4}{Q^{\ast}(s&#39;,a&#39;)} \right].--&gt;
&lt;img style=&#34;height: 60px;&#34; src=&#34;./img/q_eq25.svg&#34;/&gt;
&lt;p&gt;The other way around, if we have the optimal action-value function $Q^\ast$, we can retrieve the action taken by the optimal policy $\pi^*$ using:&lt;/p&gt;
&lt;p&gt;\begin{align}
\pi^*(s) = \mathop{\text{argmax}}_{a \in A}{\ Q^{\ast}(s, a)}.
\end{align}&lt;/p&gt;
&lt;!--(not optimal most of the time)--&gt;
&lt;p&gt;Similarly, we can derive a greedy policy from the $Q$-function associated with policy $\pi$:&lt;/p&gt;
&lt;p&gt;\begin{align}
\pi(s) = \mathop{\text{argmax}}_{a \in A}\ Q^\pi(s, a).
\end{align}&lt;/p&gt;
&lt;p&gt;This policy is implicitly defined: we take the action that maximizes the $Q$-function.
In the tabular case, this action is found by enumerating all possible actions.&lt;/p&gt;
&lt;p&gt;For the rest of this blog post, I will drop the $\pi$ superscript from $Q^{\pi}$ so as not to overload the notation (more indices are coming), but unless otherwise noted, $Q$ will always be $Q^{\pi}$.&lt;/p&gt;
&lt;h2 id=&#34;q-learning&#34;&gt;$Q$-Learning&lt;/h2&gt;
&lt;p&gt;For discrete states and actions, the $Q$-learning algorithm can be used to estimate the $Q$-function of a policy, in this particular case represented as a lookup table ($Q$-table, shown 
&lt;a href=&#34;#tabular-rl-a-discrete-world&#34;&gt;above&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;an-iterative-algorithm&#34;&gt;An iterative algorithm&lt;/h3&gt;
&lt;p&gt;The idea is to start with an initial estimate for the first iteration ($n = 0$)&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; and slowly update the estimate over time according to the 
&lt;a href=&#34;#bellman-eq&#34;&gt;Bellman equations&lt;/a&gt;.
For each transition tuple $(s_t, a_t, r_t, s_{t+1})$, we compute the error between the &lt;span style=&#34;color: #1864AB&#34;&gt;estimation&lt;/span&gt;  and the &lt;span style=&#34;color: #A61E4D&#34;&gt;target&lt;/span&gt; value and update the estimate with a learning rate $\eta$:&lt;/p&gt;
&lt;!--$$
  Q^n(s_t, a_t) = Q^{n-1}(s_t, a_t) + \eta \cdot \bigl(\color{#A61E4D}{r_t + \gamma \cdot \max_{a&#39;} Q^{n-1}(s_{t+1}, a&#39;)} - \color{#1864AB}{Q^{n-1}(s_t, a_t)}\bigr).
$$--&gt;
&lt;img style=&#34;height: 50px;&#34; src=&#34;./img/q_learning.svg&#34;/&gt;
&lt;p&gt;Under the assumptions that all state-action pairs are visited an infinite number of times and that we use a learning rate $\eta \in ]0,1[$, the $Q$-learning algorithm converges to a fixed point (i.e., $Q^{n+1}(s, a) = Q^n(s, a)$): the optimal action-value function $Q^*(s, a)$.&lt;/p&gt;
&lt;!--&lt;img style=&#34;width:50%&#34; src=&#34;./img/tabular_limit_1.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;
Illustration of a $Q$-function approximated by a lookup table (tabular case)
&lt;/p&gt;
&lt;img style=&#34;width:50%&#34; src=&#34;./img/tabular_limit_2.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;
Illustration of a $Q$-function approximated using regression (FQI)
&lt;/p&gt;--&gt;
&lt;h3 id=&#34;limitations-of-q-learning&#34;&gt;Limitations of $Q$-learning&lt;/h3&gt;
&lt;p&gt;The main limitation of $Q$-learning and its $Q$-table is that it can only handle discrete states.
The size of the table grows with the number of states, which becomes intractable when this number is infinite (continuous states).&lt;/p&gt;
&lt;img width=&#34;50%&#34;  src=&#34;./img/tabular_limit_1.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;
Illustration of a $Q$-function approximated by a lookup table (tabular case).
&lt;/p&gt;
&lt;p&gt;Moreover, it does not provide any generalization (as shown in the picture above): knowing the $Q$-values for some states does not help to predict the $Q$-values of unseen states.&lt;/p&gt;
&lt;h2 id=&#34;function-approximation-and-fitted-q-iteration-fqi&#34;&gt;Function Approximation and Fitted Q-Iteration (FQI)&lt;/h2&gt;
&lt;p&gt;A straightforward extension to $Q$-learning is to estimate the $Q$-function using function approximation instead of a $Q$-table, as displayed in the figure below:&lt;/p&gt;
&lt;img style=&#34;width:100%&#34; src=&#34;./img/q_table_fqi.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;
Illustration of a $Q$-Table (left) and Fitted Q-Iteration (FQI) value estimator (right).
Compared to the $Q$-Table, which is limited to discrete states, the FQI value estimator approximates the $Q$-value for continuous state spaces.
&lt;/p&gt;
&lt;h3 id=&#34;a-regression-problem&#34;&gt;A Regression Problem&lt;/h3&gt;
&lt;p&gt;In other words, the $Q$-value estimation problem can be formulated as a regression problem ($\color{#1864AB}{f_{\textcolor{black}{\theta}}(X)} = \color{#A61E4D}{Y}$):&lt;/p&gt;
&lt;!--\begin{align}
\color[rgb]{0.09,0.39,0.67}{Q^n_{\theta}(s_t, a_t)} &amp;= 
\color[rgb]{0.65,0.12,0.30}{r_t + \gamma \cdot \max_{a&#39; \in \mathcal{A}}(Q^{n-1}_{\theta}(s_{t+1}, a&#39;))} \\
\mathcal{L}(\theta, X, Y) &amp;= 
\frac{1}{2} \bigl(\color[rgb]{0.65,0.12,0.30}{Y} - \color[rgb]{0.09,0.39,0.67}{f_{\theta}(X)}\bigr)^2
\end{align}--&gt;
&lt;img style=&#34;height: 110px;&#34; src=&#34;./img/fqi_eq29.svg&#34;/&gt;
&lt;p&gt;where $\color{#1864AB}{X = (s_t, a_t)}$ is the input, $\color{#A61E4D}{Y = r_t + \gamma \cdot \max_{a&amp;rsquo; \in \mathcal{A}} \ldots }$ is the target, $\theta$ are the parameters to be optimized&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;, and $\mathcal{L}$ is the loss function.&lt;/p&gt;
&lt;p&gt;This is similar to what the $Q$-learning algorithm does.&lt;/p&gt;
&lt;div class=&#34;d-flex align-items-baseline&#34;&gt;















&lt;figure class=&#34;img-responsive flex-wrap w-50&#34; &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/tabular_limit_1.svg&#34; &gt;


  &lt;img src=&#34;./img/tabular_limit_1.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;
















&lt;figure class=&#34;img-responsive flex-wrap w-50&#34; &gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;./img/tabular_limit_2.svg&#34; &gt;


  &lt;img src=&#34;./img/tabular_limit_2.svg&#34; alt=&#34;&#34;  &gt;
&lt;/a&gt;



&lt;/figure&gt;

&lt;/div&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;
Illustration of a $Q$-function approximated by a lookup table (left, tabular case) and using regression (right, FQI)
&lt;/p&gt;
&lt;p&gt;Since the target $\color{#A61E4D}{Y}$ used to update $Q_{\theta}$ depends on $Q_{\theta}$ itself, we need to iterate.&lt;/p&gt;
&lt;p&gt;Computing an iterative approximation of the $Q$-function using regression is the main idea behind the 
&lt;a href=&#34;https://www.jmlr.org/papers/volume6/ernst05a/ernst05a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fitted Q-Iteration&lt;/a&gt; algorithm (FQI), presented below.
This algorithm uses a fixed dataset $\mathcal{D}$ of $m$ transitions $(s_t, a_t, r_t, s_{t+1})$.&lt;/p&gt;
&lt;img style=&#34;width:100%&#34; src=&#34;./img/fqi_algo.svg&#34;/&gt;
&lt;p&gt;In Python code, this is how the FQI algorithm looks like:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;initial_targets = rewards
# Initial Q-value estimate
qf_input = np.concatenate((states, actions))
# qf_model can be any sklearn regression model
# for instance: qf_model = LinearRegression()
qf_model.fit(qf_input, initial_targets)
 
for _ in range(N_ITERATIONS):
    # Re-use Q-value model from previous iteration
    # to create the next targets
    next_q_values = get_max_q_values(qf_model, next_states)
    # Non-terminal states target
    targets[non_terminal_states] = rewards + gamma * next_q_values
    # Special case for terminal states
    targets[terminal_states] = rewards
    # Update Q-value estimate
    qf_model.fit(qf_input, targets)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;limitations-of-fqi&#34;&gt;Limitations of FQI&lt;/h3&gt;
&lt;p&gt;FQI is a step toward a more practical algorithm, but it still has some limitations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It requires a dataset of transitions $\mathcal{D}$ and does not provide an explicit way to collect new transitions&lt;/li&gt;
&lt;li&gt;A loop over actions is needed to obtain $\max_{a&amp;rsquo; \in \mathcal{A}}Q^{n-1}_\theta(\ldots)$, which is inefficient&lt;/li&gt;
&lt;li&gt;Because $Q_\theta^{n}$ depends on $Q_\theta^{n-1}$, this leads to instability (the regression target is constantly moving)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;deep-q-learning-dqn---extending-fqi&#34;&gt;Deep Q-Learning (DQN) - Extending FQI&lt;/h2&gt;
&lt;p&gt;The 
&lt;a href=&#34;https://arxiv.org/abs/1312.5602&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Q-Learning&lt;/a&gt; (DQN) algorithm introduces several components to overcome the limitations of FQI.&lt;/p&gt;
&lt;h3 id=&#34;experience-replay&#34;&gt;Experience Replay&lt;/h3&gt;
&lt;p&gt;First, instead of having a fixed dataset of transitions, DQN uses 
&lt;a href=&#34;https://apps.dtic.mil/sti/tr/pdf/ADA261434.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;experience replay&lt;/a&gt;, also called a replay buffer.&lt;/p&gt;
&lt;p&gt;The replay buffer, shown below, is a first in first out (FIFO) data structure of capacity $m$, the maximum number of transitions that can be stored.
When the buffer is full, old experience is removed.&lt;/p&gt;
&lt;p&gt;Experience replay provides a compromise between online learning, where transitions are discarded after use, and offline learning, where transitions are stored forever.&lt;/p&gt;
&lt;img style=&#34;width:100%&#34; src=&#34;./img/replay_buffer.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;
DQN replay buffer.
&lt;/p&gt;
&lt;p&gt;To train its $Q$-network, DQN creates mini-batches&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; of experience by sampling uniformly from the replay buffer, as illustrated below.&lt;/p&gt;
&lt;p&gt;This breaks the correlation between consecutive samples, allows the agent to learn from diverse experiences (not just the most recent ones), and allows more efficient use of data by reusing past transitions multiple times.&lt;/p&gt;
&lt;img style=&#34;width:60%&#34; src=&#34;./img/replay_buffer_sampling.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;
DQN replay buffer sampling.
&lt;/p&gt;
&lt;h3 id=&#34;epsilon-greedy-exploration&#34;&gt;$\epsilon$-greedy Exploration&lt;/h3&gt;
&lt;img style=&#34;width:60%&#34; src=&#34;./img/epsilon_greedy.svg&#34;/&gt;
&lt;p&gt;DQN collects samples using an $\epsilon$-greedy strategy: at each step, it chooses a random action with a probability $\epsilon$, or otherwise follows the greedy policy (take the action with the highest $Q$-value in that state).&lt;/p&gt;
&lt;p&gt;To balance exploration and exploitation, DQN starts with $\epsilon_\text{initial} = 1$ (random policy) and linearly decreases its value until it reaches its final value, usually $\epsilon_\text{final} = 0.01$.&lt;/p&gt;
&lt;h3 id=&#34;q-network-and-target-q-network&#34;&gt;$Q$-Network and Target $Q$-network&lt;/h3&gt;
&lt;img style=&#34;width:100%&#34; src=&#34;./img/target_net.svg&#34;/&gt;
&lt;p&gt;Like 
&lt;a href=&#34;https://link.springer.com/chapter/10.1007/11564096_32&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural FQI&lt;/a&gt; (NFQ), DQN uses a neural network to approximate the $Q$-function.
However, to avoid the loop over actions of FQI, it outputs all the $Q$-values for a given state.&lt;/p&gt;
&lt;p&gt;Finally, to stabilize learning, DQN uses an old copy of the $Q$-network $Q_{\theta_\text{targ}}$ to compute the regression target.
This second network, the target network, is updated every $k$ steps, so that it slowly follows the online $Q$-network.&lt;/p&gt;
&lt;h3 id=&#34;the-full-dqn-algorithm&#34;&gt;The Full DQN Algorithm&lt;/h3&gt;
&lt;img style=&#34;width:100%&#34; src=&#34;./img/dqn.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;
  Deep Q-Network (DQN) and its main components.
&lt;/p&gt;
&lt;img style=&#34;width:100%&#34; src=&#34;https://araffin.github.io/slides/dqn-tutorial/images/dqn/annotated_dqn.png&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;
  Annotated DQN algorithm from the &lt;a href=&#34;https://arxiv.org/abs/1312.5602&#34;&gt;DQN paper&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;Overall, the DQN algorithm is very similar to the FQI algorithm, the main difference being that DQN alternates between collecting new transitions and updating its network.&lt;/p&gt;
&lt;img style=&#34;width:100%&#34; src=&#34;./img/dqn_algo.svg&#34;/&gt;
&lt;h2 id=&#34;beyond-dqn-algorithms-for-continuous-action-spaces-ddpg-td3-sac-&#34;&gt;Beyond DQN: Algorithms for Continuous Action Spaces (DDPG, TD3, SAC, &amp;hellip;)&lt;/h2&gt;
&lt;p&gt;In this post, I started from the tabular case, where you can use the $Q$-learning algorithm to estimate the $Q$-value function, represented as a table.&lt;/p&gt;
&lt;p&gt;To extend the idea of $Q$-learning to continuous states, FQI replaces the $Q$-table with function approximation.
It then refines its estimate iteratively, solving a regression problem at each step.&lt;/p&gt;
&lt;p&gt;Finally, DQN introduces several components to overcome the limitations of FQI.
Notably it uses a neural network for faster inference and a replay buffer to re-use past data.&lt;/p&gt;
&lt;p&gt;The key components of DQN ($Q$-network, target network, replay buffer) are at the core of Deep RL algorithms for continuous control used on real robots, such as Soft Actor-Critic (SAC).
I will introduce these algorithms in a future blog post.&lt;/p&gt;
&lt;h2 id=&#34;appendix-rl101&#34;&gt;Appendix: RL101&lt;/h2&gt;
&lt;p&gt;Here is a very short formal introduction to the RL terminology used in this post.&lt;/p&gt;
&lt;p&gt;In reinforcement learning, an agent interacts with its environment, usually modeled as a Markov Decision Process&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; (MDP) $(\mathcal{S}, \mathcal{A}, P, r)$ where $\mathcal{S}$ is the state space, $\mathcal{A}$ the action space and $P(\mathbf{s}&amp;rsquo; \mid \mathbf{s}, \mathbf{a})$ the transition function.
At every step $t$, the agent performs an action $\mathbf{a}$ in state $\mathbf{s}$ following its policy $\pi : \mathcal{S} \mapsto \mathcal{A}$.
It then receives a feedback signal in the next state $\mathbf{s}&amp;rsquo;$: the reward $r(\mathbf{s}, \mathbf{a})$. The objective of the agent is to maximize the long-term reward.
More formally, the goal is to maximize the expectation of the sum of discounted reward, over the trajectories $\rho_\pi$ generated using its policy $\pi$:&lt;/p&gt;
&lt;!--J = \sum_t \mathbb{E}_{(\mathbf{s}_t,\mathbf{a}_t)\sim\rho_\pi}\left[ \gamma^t \, r(\mathbf{s}_t,\mathbf{a}_t) \right]--&gt;
&lt;img style=&#34;height: 60px;&#34; src=&#34;./img/rl_obj.svg&#34;/&gt;
&lt;p&gt;where $\gamma \in [0,1)$ is the discount factor and represents a trade-off between maximizing short-term and long-term rewards.
The agent-environment interactions are often broken down into sequences called &lt;em&gt;episodes&lt;/em&gt;, that end when the agent reaches a terminal state.&lt;/p&gt;
&lt;p&gt;In the example of learning to walk, if the goal is to achieve the fastest speed, an immediate reward can be the distance traveled between two timesteps.
The state would be the current information about the robot (joint positions, velocities, torques, linear acceleration, &amp;hellip;) and the action would be a desired motor position.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@article{raffin2025rl102,
  title   = &amp;quot;RL102: From Tabular Q-Learning to Deep Q-Learning (DQN)&amp;quot;,
  author  = &amp;quot;Raffin, Antonin&amp;quot;,
  journal = &amp;quot;araffin.github.io&amp;quot;,
  year    = &amp;quot;2025&amp;quot;,
  month   = &amp;quot;Sept&amp;quot;,
  url     = &amp;quot;https://araffin.github.io/post/rl102/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;I would like to thank Anssi and Alison for their feedback =).&lt;/p&gt;
&lt;p&gt;All the graphics were made using 
&lt;a href=&#34;https://excalidraw.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;excalidraw&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;did-you-find-this-post-helpful-consider-sharing-it-&#34;&gt;Did you find this post helpful? Consider sharing it ð&lt;/h3&gt;
&lt;h2 id=&#34;footnotes&#34;&gt;Footnotes&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;I will focus for now on value-based methods.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;The initial estimate is usually zero.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;To ease the transition to DQN, we consider only parametric estimators here (i.e., we exclude kNN 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Nonparametric_regression&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;for instance&lt;/a&gt;)&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Rather than doing updates using the entire dataset, it is more practical to perform gradient updates with subsets sampled from the dataset.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;The Markov property states that next state and reward depend only on the current state and action, not on the history of previous states and actions.&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Getting SAC to Work on a Massive Parallel Simulator: Tuning for Speed (Part II)</title>
      <link>/post/tune-sac-isaac-sim/</link>
      <pubDate>Tue, 01 Jul 2025 00:00:00 +0000</pubDate>
      <guid>/post/tune-sac-isaac-sim/</guid>
      <description>&lt;p&gt;This second post details how I tuned the Soft-Actor Critic (SAC) algorithm to learn as fast as PPO in the context of a massively parallel simulator (thousands of robots simulated in parallel).
If you read along, you will learn how to automatically tune SAC for speed (i.e., minimize wall clock time), how to find better action boundaries, and what I tried that didn&amp;rsquo;t work.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;../sac-massive-sim/&#34;&gt;Part I&lt;/a&gt; analyzes why SAC doesn&amp;rsquo;t work out of the box on Isaac Sim environments.&lt;/li&gt;
&lt;li&gt;Part II is about tuning SAC for speed and making it work as good as PPO.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;in-the-previous-episode&#34;&gt;In the Previous Episode&amp;hellip;&lt;/h2&gt;
&lt;p&gt;In the 
&lt;a href=&#34;../sac-massive-sim/&#34;&gt;first part&lt;/a&gt;, I stopped at the point where we could detect some signs of life from SAC (it was learning something).&lt;/p&gt;
&lt;p&gt;By limiting the action space limits to 3% of the original size, and quickly tuning SAC (bigger network, reduced initial exploration rate), I could get SAC to learn to solve the Unitree A1 task on a flat surface in a matter of minutes.&lt;/p&gt;
&lt;p&gt;However, SAC took more time to train than PPO (12 minutes vs. 6 minutes), and it did not reach PPO&amp;rsquo;s performance level.
Luckily, I still had several ideas for improving SAC&amp;rsquo;s training speed and performance&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;h2 id=&#34;defining-proper-action-bound---extracting-the-limits-with-ppo&#34;&gt;Defining Proper Action Bound - Extracting the Limits with PPO&lt;/h2&gt;
&lt;p&gt;First, I wanted to define the action space more precisely.
Correctly defining the boundaries of the action space is important for both the speed of convergence and the final performance.
A larger action space gives the agent more flexibility, which can lead to better performance, but slower learning.
Conversely, a smaller action space can accelerate learning, though it may result in suboptimal solutions.&lt;/p&gt;
&lt;p&gt;Thus, rather than simply restricting the action space to a small percentage of the original, I 
&lt;a href=&#34;https://gist.github.com/araffin/e069945a68aa0d51fcdff3f01e945c70&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recorded&lt;/a&gt; the actions taken by a trained PPO agent and took the 2.5th and 97.5th percentiles for the new limits.
In other words, the new action space contains 95% of the actions commanded by a trained PPO agent&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# np.percentile(actions, 2.5, axis=0)
low = np.array([-2.0, -0.4, -2.6, -1.3, -2.2, -1.9, -0.7, -0.4, -2.1, -2.4, -2.5, -1.7])
# np.percentile(actions, 97.5, axis=0)
high = np.array([1.1, 2.6, 0.7, 1.9, 1.3, 2.6, 3.4, 3.8, 3.4, 3.4, 1.9, 2.1])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;need-for-speed-or-how-i-learned-to-stop-worrying-about-sample-efficiency&#34;&gt;Need for Speed or: How I Learned to Stop Worrying About Sample Efficiency&lt;/h2&gt;
&lt;p&gt;The second aspect I wanted to improve was the hyperparameters of the SAC algorithm.
The default hyperparameters of the SAC algorithm are optimized for sample efficiency.
While this is ideal for learning directly on a single real robot, it is suboptimal for training thousands of robots in simulation.&lt;/p&gt;
&lt;p&gt;In 
&lt;a href=&#34;../sac-massive-sim/&#34;&gt;part one&lt;/a&gt;, I quickly tuned SAC by hand to get it up and running.
This was sufficient for obtaining initial results, but it would be very time-consuming to continue tuning manually in order to reach PPO&amp;rsquo;s performance level.
That&amp;rsquo;s why I turned to automatic hyperparameter optimization.&lt;/p&gt;
&lt;p&gt;If you are not familiar with automatic hyperparameter tuning, I wrote two blog posts about it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;../hyperparam-tuning/&#34;&gt;Automatic Hyperparameter Tuning - A Visual Guide (Part 1)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;../optuna/&#34;&gt;Automatic Hyperparameter Tuning - In Practice (Part 2)&lt;/a&gt; shows how to use the 
&lt;a href=&#34;https://github.com/optuna/optuna&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optuna library&lt;/a&gt; to put these techniques into practice&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;new-objective-learn-as-fast-as-possible&#34;&gt;New Objective: Learn as Fast as Possible&lt;/h3&gt;
&lt;p&gt;Since I&amp;rsquo;m using a massively parallel simulator, I no longer care about how many samples are needed to learn something, but rather, how quickly it can learn, regardless of the number of samples used.
In practice, this translates to an objective function that looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def objective(trial: optuna.Trial) -&amp;gt; float:
    &amp;quot;&amp;quot;&amp;quot;Optimize for best performance after 5 minutes of training.&amp;quot;&amp;quot;&amp;quot;
    # Sample hyperparameters
    hyperparams = sample_sac_params(trial)
    agent = sbx.SAC(env=env, **hyperparams)
    # Callback to exit the training loop after 5 minutes
    callback = TimeoutCallback(timeout=60 * 5)
    # Train with a max budget of 50_000_000 timesteps
    agent.learn(total_timesteps=int(5e7), callback=callback)
    # Log the number of interactions with the environments
    trial.set_user_attr(&amp;quot;num_timesteps&amp;quot;, agent.num_timesteps)
    # Evaluate the trained agent
    env.seed(args_cli.seed)
    mean_reward, std_reward = evaluate_policy(agent, env, n_eval_episodes=512)
    return mean_reward
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The agent is evaluated after five minutes of training, regardless of how many interactions with the environment were needed (the &lt;code&gt;TimeoutCallback&lt;/code&gt; forces the agent to exit the training loop).&lt;/p&gt;
&lt;h3 id=&#34;sac-hyperparameters-sampler&#34;&gt;SAC Hyperparameters Sampler&lt;/h3&gt;
&lt;p&gt;Similar to 
&lt;a href=&#34;../optuna/&#34;&gt;PPO&lt;/a&gt;, many hyperparameters can be tuned for SAC.
After some trial and error, I came up with the following sampling function (I&amp;rsquo;ve included comments that explain the meaning of each parameter):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def sample_sac_params(trial: optuna.Trial) -&amp;gt; dict[str, Any]:
    # Discount factor
    gamma = trial.suggest_float(&amp;quot;gamma&amp;quot;, 0.975, 0.995)
    learning_rate = trial.suggest_float(&amp;quot;learning_rate&amp;quot;, 1e-4, 0.002, log=True)
    # Initial exploration rate (entropy coefficient in the SAC loss)
    ent_coef_init = trial.suggest_float(&amp;quot;ent_coef_init&amp;quot;, 0.001, 0.02, log=True)
    # From 2^7=128 to 2^12 = 4096, the mini-batch size
    batch_size_pow = trial.suggest_int(&amp;quot;batch_size_pow&amp;quot;, 7, 12, log=True)
    # How big should should the actor and critic networks be
    # net_arch = trial.suggest_categorical(&amp;quot;net_arch&amp;quot;, [&amp;quot;default&amp;quot;, &amp;quot;simba&amp;quot;, &amp;quot;large&amp;quot;])
    # I&#39;m using integers to be able to use CMA-ES,
    # &amp;quot;default&amp;quot; is [256, 256], &amp;quot;large&amp;quot; is [512, 256, 128]
    net_arch_complexity = trial.suggest_int(&amp;quot;net_arch_complexity&amp;quot;, 0, 3)
    # From 1 to 8 (how often should we update the networks, every train_freq steps in the env)
    train_freq_pow = trial.suggest_int(&amp;quot;train_freq_pow&amp;quot;, 0, 3)
    # From 1 to 1024 (how many gradient steps by step in the environment)
    gradient_steps_pow = trial.suggest_int(&amp;quot;gradient_steps_pow&amp;quot;, 0, 10)
    # From 1 to 32 (the policy delay parameter, similar to TD3 update)
    policy_delay_pow = trial.suggest_int(&amp;quot;policy_delay_pow&amp;quot;, 0, 5)
    # Polyak coeff (soft update of the target network)
    tau = trial.suggest_float(&amp;quot;tau&amp;quot;, 0.001, 0.05, log=True)
    
    # Display true values
    trial.set_user_attr(&amp;quot;batch_size&amp;quot;, 2**batch_size_pow)
    trial.set_user_attr(&amp;quot;gradient_steps&amp;quot;, 2**gradient_steps_pow)
    trial.set_user_attr(&amp;quot;policy_delay&amp;quot;, 2**policy_delay_pow)
    trial.set_user_attr(&amp;quot;train_freq&amp;quot;, 2**train_freq_pow)
    # Note: to_hyperparams() does the convertions between sampled value and expected value
    # Ex: converts batch_size_pow to batch_size
    # This is useful when replaying trials
    return to_hyperparams({
        &amp;quot;train_freq_pow&amp;quot;: train_freq_pow,
        &amp;quot;gradient_steps_pow&amp;quot;: gradient_steps_pow,
        &amp;quot;batch_size_pow&amp;quot;: batch_size_pow,
        &amp;quot;tau&amp;quot;: tau,
        &amp;quot;gamma&amp;quot;: gamma,
        &amp;quot;learning_rate&amp;quot;: learning_rate,
        &amp;quot;policy_delay_pow&amp;quot;: policy_delay_pow,
        &amp;quot;ent_coef_init&amp;quot;: ent_coef_init,
        &amp;quot;net_arch_complexity&amp;quot;: net_arch_complexity,
    })
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;replay-ratio&#34;&gt;Replay Ratio&lt;/h3&gt;
&lt;p&gt;A metric that will be useful to understand the tuned hyperparameters is the replay ratio.
The replay ratio (also known as update-to-data ratio or UTD ratio) measures the number of gradient updates performed per environment interaction or experience collected.
This ratio represents how many times an agent updates its parameters relative to how much new experience it gathers.
For SAC, it is defined as &lt;code&gt;replay_ratio = gradient_steps / (num_envs * train_freq)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In a classic setting, the replay ratio is usually greater than one when optimizing for sample efficiency.
That means that SAC does at least one gradient step per interaction with the environment.
However, in the current setting, since collecting new data is cheap, the replay ratio tends to be lower than 1/4 (one gradient step for every four steps in the environment).&lt;/p&gt;
&lt;h3 id=&#34;optimization-result---tuned-hyperparameters&#34;&gt;Optimization Result - Tuned Hyperparameters&lt;/h3&gt;
&lt;p&gt;To optimize the hyperparameters, I used Optuna&amp;rsquo;s CMA-ES sampler for 100 trials&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; (taking about 10 hours with a population size of 10 individuals).
Afterward, I retrained the best trials to 
&lt;a href=&#34;https://arxiv.org/abs/2209.07171&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;filter out&lt;/a&gt; any lucky seeds, i.e., to find hyperparameters that work consistently across different runs.&lt;/p&gt;
&lt;p&gt;This is what the optimization history looks like. Many sets of hyperparameters were successful:&lt;/p&gt;
&lt;img style=&#34;max-width: 100%&#34; src=&#34;./img/optuna_sac.png&#34; alt=&#34;Hyperparameter optimization history&#34; /&gt;
&lt;p style=&#34;font-size: 12pt; text-align:center;&#34;&gt;Hyperparameter optimization history&lt;/p&gt;
&lt;p&gt;These are the tuned hyperparameters of SAC found by the CMA-ES sampler while optimizing for speed:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;batch_size: 512
buffer_size: 2_000_000
ent_coef: auto_0.009471776840423638
gamma: 0.983100250213744
gradient_steps: 32
learning_rate: 0.00044689099625712413
learning_starts: 2000
policy: MlpPolicy
policy_delay: 8
policy_kwargs:
  net_arch: [512, 256, 128]
  activation_fn: !!python/name:isaaclab_rl.sb3.elu &#39;&#39;
  optimizer_class: !!python/name:optax._src.alias.adamw &#39;&#39;
  layer_norm: true
tau: 0.0023055560568780655
train_freq: 1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compared to the default hyperparameters of SAC, there are some notable changes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The network architecture is much larger (&lt;code&gt;[512, 256, 128]&lt;/code&gt; vs. &lt;code&gt;[256, 256]&lt;/code&gt;), but similar to that used by PPO in Isaac Sim.&lt;/li&gt;
&lt;li&gt;The lower replay ratio (RR â 0.03 for 1024 environments, or three gradient steps every 100 steps in an environment) and higher policy delay (update the actor once every eight critic updates) make it faster, as less time is taken for gradient updates.&lt;/li&gt;
&lt;li&gt;The discount factor is lower than the default value of 0.99, which favors shorter-term rewards.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is the result in video and the associated learning curves&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;img style=&#34;max-width:100%&#34; src=&#34;./img/learning_curve_unitree.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;Learning curve on the Unitree A1 task using 1024 envs.&lt;/p&gt;
&lt;video controls src=&#34;https://b2drop.eudat.eu/public.php/dav/files/q6aM4WCSNF28ErD/&#34;&gt;
&lt;/video&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;Trained SAC agent after automatic tuning.&lt;/p&gt;
&lt;p&gt;With these tuned hyperparameters, SAC learns faster (than in part I), achieves higher performance, and the learned gaits look better (no more feet in the air!).
What more could you ask for?&lt;/p&gt;
&lt;!-- ### Improving Convergence

To improve the convergence of SAC (see the oscillations in the learning curve), I replaced the constant learning rate with a linear schedule:
```python
# Decrease the LR linearly from 5e-4 to 1e-4 over 7.5M steps (0.15 * 50_000_000, where 50M is the max training budget)
learning_rate = LinearSchedule(start=5e-4, end=1e-5, end_fraction=0.15)
```

TODO: show learning curve before and after + with PPO as reference + SAC vs PPO
and also the effect on the trained policy (no more leg up in the air) --&gt;
&lt;h2 id=&#34;does-it-work---more-environments&#34;&gt;Does it work? - More Environments&lt;/h2&gt;
&lt;!-- So far, I have only optimized and tested the hyperparameters in one environment. --&gt;
&lt;!-- The goal is to make it work in any locomotion environment. --&gt;
&lt;p&gt;After it successfully learned on the flat Unitree A1 environment, I tested the same hyperparameters (with the same recipe&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;) on the GO1, GO2, Anymal-B, and Anymal-C environments, as well as the flat 
&lt;a href=&#34;https://github.com/louislelay/disney_bdx_rl_isaaclab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Disney BD-X&lt;/a&gt; environment and &amp;hellip; it worked!&lt;/p&gt;
&lt;video controls src=&#34;https://b2drop.eudat.eu/public.php/dav/files/RKCWddABEtj5MFT/&#34;&gt;
&lt;/video&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;Trained SAC agent in different environments, using the same tuned hyperparameters.&lt;/p&gt;
&lt;!-- In those environments, SAC learns as fast as PPO but is more sample efficient. --&gt;
&lt;p&gt;Then, I trained SAC on the &amp;ldquo;rough&amp;rdquo; locomotion environments, which are harder environments where the robot has to learn to navigate steps and uneven, accidented terrain (with additional randomization).
And &amp;hellip; it worked partially.&lt;/p&gt;
&lt;h2 id=&#34;solving-harder-environments&#34;&gt;Solving Harder Environments&lt;/h2&gt;
&lt;h3 id=&#34;identifying-the-problem-why-it-doesnt-work&#34;&gt;Identifying the problem: Why it doesn&amp;rsquo;t work?&lt;/h3&gt;
&lt;p&gt;In the &amp;ldquo;Rough&amp;rdquo; environment, the SAC-trained agent exhibits inconsistent behavior.
For example, one time the robot successfully climbs down the pyramid steps without falling; at other times, however, it does nothing.
Additionally, no matter how long it is trained, SAC does not seem to be able to learn to solve the &amp;ldquo;inverted pyramid&amp;rdquo;, which is probably one of the hardest tasks:&lt;/p&gt;
&lt;img style=&#34;max-width: 100%&#34; src=&#34;./img/inverted_pyramid.jpg&#34; alt=&#34;The inverted pyramid task.&#34; /&gt;
&lt;p style=&#34;font-size: 12pt; text-align:center;&#34;&gt;The inverted pyramid task.&lt;/p&gt;
&lt;p&gt;I decided to isolate this task by training SAC only on the inverted pyramid.
Upon further inspection, it appeared to be an exploration problem; that is, SAC never experiences successful stepping when executing random movements.
This reminded me of SAC failing on the 
&lt;a href=&#34;https://github.com/rail-berkeley/softlearning/issues/76&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mountain car problem&lt;/a&gt; because the exploration was inconsistent (the default high-frequency noise is usually a 
&lt;a href=&#34;https://openreview.net/forum?id=TSuSGVkjuXd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bad default&lt;/a&gt; for robots).&lt;/p&gt;
&lt;h3 id=&#34;improving-exploration-and-performance&#34;&gt;Improving Exploration and Performance&lt;/h3&gt;
&lt;p&gt;To test this hypothesis, I simplified the problem by lowering the step of the inverted pyramid and used a more consistent exploration scheme, 
&lt;a href=&#34;https://openreview.net/forum?id=TSuSGVkjuXd&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;gSDE&lt;/a&gt; (that I developed during my PhD to train RL directly on real robots).&lt;/p&gt;
&lt;p&gt;In its simplest form, gSDE repeats the noise vector for $n$-steps, instead of sampling it at every timestep.
In other words, instead of selecting actions following $a_t = \mu_\theta(s_t) + \epsilon_t$&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt; and sampling $\epsilon_t \sim N(0, \sigma^2)$ at every step during exploration, gSDE samples $\epsilon \sim N(0, \sigma^2)$ once and keeps $\epsilon$ constant for $n$-steps.
With this improved exploration, the robot could finally learn to partially solve this task.&lt;/p&gt;
&lt;!-- (note: gSDE also allowed to have better performance on the flat terrain, maybe my PhD was useful ^^?) --&gt;
&lt;video controls src=&#34;https://b2drop.eudat.eu/public.php/dav/files/TRkAKzNyWZ83Q7K/&#34;&gt;
&lt;/video&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;Trained SAC agent with gSDE and n-step return in the &#34;Rough&#34; Anymal-C environment.&lt;/p&gt;
&lt;p&gt;There was still a big gap in final performance between SAC and PPO.
To close the gap, I drew inspiration from the recent 
&lt;a href=&#34;https://github.com/younggyoseo/FastTD3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FastTD3&lt;/a&gt; paper and implemented 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/pull/2144&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;n-step returns&lt;/a&gt; for all off-policy algorithms in SB3.
Using &lt;code&gt;n_steps=3&lt;/code&gt; allowed SAC to finally solve the hardest task&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;!&lt;/p&gt;
&lt;p&gt;In summary, here are the additional manual changes I made to the hyperparameters of SAC compared to those optimized automatically:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;# Note: we must use train_freq &amp;gt; 1 to enable gSDE
# which resamples the noise every n steps (here every 10 steps)
train_freq: 10
# Scaling the gradient steps accordingly, to keep the same replay ratio:
# 32 * train_freq = 320
gradient_steps: 320 
use_sde: True
# N-step return
n_steps: 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And here are the associated learning curves&lt;sup id=&#34;fnref1:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;(plotting the current curriculum level on the y-axis&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;):&lt;/p&gt;
&lt;img style=&#34;max-width:100%&#34; src=&#34;./img/learning_curve_rough.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;Learning curve on the Anymal-C &#34;Rough&#34; task using 1024 envs (except for PPO).&lt;/p&gt;
&lt;img style=&#34;max-width:100%&#34; src=&#34;./img/learning_curve_rough_efficiency.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;Learning curve in term of sample-effiency on the Anymal-C &#34;Rough&#34; task using 1024 envs (except for PPO).&lt;/p&gt;
&lt;p&gt;In those plots, you can see the effect of gSDE and the use of n-step returns.
SAC is also much more sample efficient than PPO.&lt;/p&gt;
&lt;!-- Note: sde allow to have better performance without linear schedule --&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This concludes the long journey I started a few months ago to make SAC work on a massively parallel simulator.
During this adventure, I addressed a common issue that prevents SAC-like algorithms from working in these environments: the use of an unbounded action space.
In the end, with a proper action space and tuned hyperparameters, SAC is now competitive with PPO&lt;sup id=&#34;fnref1:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt; in terms of training time (while being much more sample efficient) on a large collection of locomotion environments.
I hope my voyage encourages others to use SAC in their experiments and unlock fine-tuning on real robots after pretraining in simulation.&lt;/p&gt;
&lt;h2 id=&#34;appendix-what-i-tried-that-didnt-work&#34;&gt;Appendix: What I Tried That Didn&amp;rsquo;t Work&lt;/h2&gt;
&lt;p&gt;While preparing this blog post, I tried many things to achieve PPO performance and learn good policies in minimal time.
Many of the things I tried didn&amp;rsquo;t work, but they are probably worth investigating further.
I hope you can learn from my failures, too.&lt;/p&gt;
&lt;h3 id=&#34;using-an-unbounded-gaussian-distribution&#34;&gt;Using an Unbounded Gaussian Distribution&lt;/h3&gt;
&lt;p&gt;One approach I tried was to make SAC look more like PPO.
In part one, PPO could handle an unbounded action space because it used a (non-squashed) Gaussian distribution (vs. a squashed one for SAC).
However, replacing SAC&amp;rsquo;s squashed Normal distribution with an unbounded Gaussian distribution led to additional problems.&lt;/p&gt;
&lt;p&gt;Without layer normalization in the critic, it quickly diverges (leading to Inf/NaN).
It seems that, encouraged by the entropy bonus, the actor pushes toward very large action values.
It also appears that this variant requires specific tuning (and that state-dependent std may need to be replaced with state-independent std, as is done for PPO).&lt;/p&gt;
&lt;p&gt;If you manage to reliably make SAC work with an unbounded Gaussian distribution, please reach out!&lt;/p&gt;
&lt;!-- Note: tried with both state-dependent std and independent std --&gt;
&lt;!-- TODO: try with fixed std? more tuning, tune notably the target entropy, any other mechanism to avoid explosion of losses/divergence? --&gt;
&lt;h3 id=&#34;kl-divergence-adaptive-learning-rate&#34;&gt;KL Divergence Adaptive Learning Rate&lt;/h3&gt;
&lt;p&gt;One component of PPO that allows for better performance is the learning rate schedule (although it is not critical, it ease hyperparameter tuning).
It automatically adjusts the learning rate to maintain a constant KL divergence between two updates, ensuring that the new policy remains close to the previous one (and ensuring that the learning rate is large enough too).
It should be possible to do something similar with SAC.
However, when I tried to approximate the KL divergence using either the log probability or the extracted Gaussian parameters (mean and standard deviation), it didn&amp;rsquo;t work.
The KL divergence values were too large and inconsistent.
SAC would probably need a trust region mechanism as well.&lt;/p&gt;
&lt;p&gt;Again, if you find a way to make it work, please reach out!&lt;/p&gt;
&lt;h3 id=&#34;truncated-quantile-critics-tqc&#34;&gt;Truncated Quantile Critics (TQC)&lt;/h3&gt;
&lt;p&gt;One idea I had to improve performance was to replace the SAC algorithm with its 
&lt;a href=&#34;https://araffin.github.io/slides/recent-advances-rl/#/8/0/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;distributional&lt;/a&gt; counterpart 
&lt;a href=&#34;https://sb3-contrib.readthedocs.io/en/master/modules/tqc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Truncated Quantile Critics (TQC)&lt;/a&gt;.
Rather than approximating only the expected return, TQC models the distribution of returns.
TQC&amp;rsquo;s performance tends to be on par with SAC&amp;rsquo;s, but it can outperform SAC in 
&lt;a href=&#34;%28https://araffin.github.io/slides/recent-advances-rl/#/9%29&#34;&gt;harder environments&lt;/a&gt; (at the cost of a slightly more expensive gradient step).
TQC also has a parameter that controls the overestimation bias of the Q-value function (how many top quantiles to drop).&lt;/p&gt;
&lt;p&gt;While writting this blog (and doing experiments), TQC tended to be more easy to tune.
However, after finding good hyperparmaters for speed, SAC was faster and reach equivalent performance compared to TQC (except on the Disney robot env where TQC tend to work better).&lt;/p&gt;
&lt;!-- and also tried to limit the overestimation of the $Q$-value by dropping more quantiles:
```python
top_quantiles_to_drop_per_net = 5  # The default value is 2
``` --&gt;
&lt;h3 id=&#34;en-vrac---other-things-i-tried&#34;&gt;En Vrac - Other Things I Tried&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;penalty to be away from action bounds (hard to tune)&lt;/li&gt;
&lt;li&gt;action space schedule (start with small action space, make it bigger over time, tricky to schedule and didn&amp;rsquo;t improve performance)&lt;/li&gt;
&lt;li&gt;linear schedule (&lt;code&gt;learning_rate = LinearSchedule(start=5e-4, end=1e-5, end_fraction=0.15)&lt;/code&gt;), it helped for convergence when using &lt;code&gt;n_steps=1&lt;/code&gt; and &lt;code&gt;use_sde=False&lt;/code&gt;, but was not needed at the end&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
To try:
- TD3 instead of SAC
- normalize input partially (not height scan) -&gt; doesn&#39;t work?
- use trained PPO net as feature extractor -&gt; not needed
- add an history for the height scan -&gt; not needed
- KL penalty for SAC (trust region, already tried I guess?) --&gt;
&lt;h2 id=&#34;appendix-sb3-ppo-pytorch-vs-sbx-ppo-jax---a-small-change-in-the-code-a-big-change-in-performance&#34;&gt;Appendix: SB3 PPO (PyTorch) vs. SBX PPO (Jax) - A Small Change in the Code, a Big Change in Performance&lt;/h2&gt;
&lt;img style=&#34;max-width: 100%&#34; src=&#34;https://cdn.bsky.app/img/feed_fullsize/plain/did:plc:ux3nlsvhsagmx3yxjvvaimdv/bafkreibxnbcdikdbflwiufvkfjhbmhljnrghfqgkavf6eo6go3sj4ffita@jpeg&#34; alt=&#34;SB3 PPO vs SBX PPO&#34; /&gt;
&lt;p style=&#34;font-size: 12pt; text-align:center;&#34;&gt;Learning curves for SB3 PPO and SBX PPO before and after the fix. SB3 PPO is the blue line. SBX PPO before is the yellow line, and SBX PPO after the fix is the grey line.&lt;/p&gt;
&lt;p&gt;While writing this blog post, I regularly compared SAC to PPO. I have two implementations of PPO: SB3 PPO in PyTorch and SBX PPO in JAX.
While comparing, I noticed two things.
First, SBX PPO did not learn anything when observation normalization was turned off, whereas SB3 PPO did.
Second, the dynamics of the standard deviation (its evolution over time) of the Gaussian distribution were different.&lt;/p&gt;
&lt;p&gt;I investigated where the difference came from.
SBX and SB3 share quite a bit of code, so I was surprised by such a significant difference.
My main suspects were Jax vs. PyTorch because the Adam implementation and network initialization are different.
I tried to use the same initialization for the weights and the same optimizer parameters, but I couldn&amp;rsquo;t get similar behavior at that time.&lt;/p&gt;
&lt;p&gt;To dig deeper, I checked the statistics of the collected data to understand why the standard deviation was growing with the SBX implementation (instead of decreasing).
I noticed something odd.
The mean of the actions was not zero at the very beginning of training, and the standard deviation of the actions was much larger than expected (I was expecting std around 1.0, but got std=3.0 for instance).
I realized that this was due to the last layer initialization, which was not producing actions close to zero at the beginning of training.
Fixing this initialization problem solved my original issue (and the std of the actions during exploration): I could get similar performance with SB3 PPO and SBX PPO.&lt;/p&gt;
&lt;!-- One line of code changed, big difference in learning curves: --&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@article{raffin2025isaacsim,
  title   = &amp;quot;Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms&amp;quot;,
  author  = &amp;quot;Raffin, Antonin&amp;quot;,
  journal = &amp;quot;araffin.github.io&amp;quot;,
  year    = &amp;quot;2025&amp;quot;,
  month   = &amp;quot;Feb&amp;quot;,
  url     = &amp;quot;https://araffin.github.io/post/sac-massive-sim/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;I would like to thank Anssi, Leon, Ria and Costa for their feedback =).&lt;/p&gt;
&lt;!-- All the graphics were made using [excalidraw](https://excalidraw.com/). --&gt;
&lt;h3 id=&#34;did-you-find-this-post-helpful-consider-sharing-it-&#34;&gt;Did you find this post helpful? Consider sharing it ð&lt;/h3&gt;
&lt;h2 id=&#34;footnotes&#34;&gt;Footnotes&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;I present the ones that didn&amp;rsquo;t work and could use help (open-problems) at the end of this post.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;I repeat the same process for any new environment where those boundaries would not work (taking sometime the 0.5 and 99.5 percentiles to have a larger space).&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;Here, I only optimized for the Unitree A1 flat task due to limited computation power. It would be interesting to tune SAC directly for the &amp;ldquo;Rough&amp;rdquo; variant, including &lt;code&gt;n_steps&lt;/code&gt; and gSDE train frequency as hyperparameters.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;The results are plotted for only three independent runs (random seeds). This is usually insufficient for RL due to the stochasticity of the results. However, in this case, the results tend to be consistent between runs (limited variability). I observed this during the many runs I did while debugging (and writting this blog post), so the trend is likely correct, even with a limited number of seeds. I only have one machine to run the tests, but I will try to run more tests in the coming weeks and update the plots.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;I updated the limits for each family of robots. The PPO percentiles technique worked nicely.&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;$\mu_\theta(s_t)$ is the actor network output, it represents the mean of the Gaussian distribution.&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;Although there is still a slight performance gap between SAC and PPO, after reading the FastTD3 paper and conducting my own experiments, I believe that the environment rewards were tuned for PPO to encourage a desired behavior. In other words, I suspect that the weighting of the reward terms was adjusted for PPO. To achieve similar performance, SAC probably needs different weights.&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref1:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;I&amp;rsquo;m plotting the current state of the terrain curriculum (the higher the number, the harder the task/terrain) as the reward magnitude doesn&amp;rsquo;t tell the whole story for the &amp;ldquo;Rough&amp;rdquo; task.&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Automatic Hyperparameter Tuning - In Practice (Part 2)</title>
      <link>/post/optuna/</link>
      <pubDate>Wed, 23 Apr 2025 00:00:00 +0000</pubDate>
      <guid>/post/optuna/</guid>
      <description>&lt;p&gt;This is the second (and last) post on automatic hyperparameter optimization.
In the 
&lt;a href=&#34;https://araffin.github.io/post/hyperparam-tuning/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;first part&lt;/a&gt;, I introduced the challenges and main components of hyperparameter tuning (samplers, pruners, objective function, &amp;hellip;).
This second part is about the practical application of this technique with the 
&lt;a href=&#34;https://github.com/optuna/optuna&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optuna library&lt;/a&gt;, in a reinforcement learning setting (using the 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable-Baselines3 (SB3)&lt;/a&gt; library).&lt;/p&gt;
&lt;p&gt;Code: 
&lt;a href=&#34;https://gist.github.com/araffin/d16e77aa88ffc246856f4452ab8a2524&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/araffin/d16e77aa88ffc246856f4452ab8a2524&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Note: if you prefer to learn with video, I gave this tutorial at ICRA 2022.
The 
&lt;a href=&#34;https://araffin.github.io/tools-for-robotic-rl-icra2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;, notebooks and videos are online:&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/ihP7E76KGOI?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;div style=&#34;margin-top: 50px&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;ppo-on-pendulum-v1---when-default-hyperparameters-dont-work&#34;&gt;PPO on Pendulum-v1 - When default hyperparameters don&amp;rsquo;t work&lt;/h2&gt;
&lt;p&gt;To make this post more concrete, let&amp;rsquo;s take a simple example where the default hyperparameters don&amp;rsquo;t work.
In the 
&lt;a href=&#34;https://gymnasium.farama.org/environments/classic_control/pendulum/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;Pendulum-v1&lt;/code&gt;&lt;/a&gt; environment, the RL agent controls a pendulum that &amp;ldquo;starts in a random position, and the goal is to swing it up so it stays upright&amp;rdquo;.&lt;/p&gt;
&lt;video controls src=&#34;https://huggingface.co/sb3/sac-Pendulum-v1/resolve/main/replay.mp4&#34;&gt;
&lt;/video&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;Trained SAC agent on the &lt;code&gt;Pendulum-v1&lt;/code&gt; environment.
&lt;/p&gt;
&lt;p&gt;The agent receives the state of the pendulum as input (cos and sine of the angle $\theta$ and angular velocity $\dot{\theta}$) and outputs the desired torque (1D).
The agent is rewarded for keeping the pendulum upright ($\theta = 0$ and $\dot{\theta} = 0$) and penalized for using high torques.
An episode ends after a timeout of 200 steps (
&lt;a href=&#34;https://www.youtube.com/watch?v=eZ6ZEpCi6D8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;truncation&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;If you try to run the 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Proximal Policy Optimization (PPO)&lt;/a&gt; algorithm on the &lt;code&gt;Pendulum-v1&lt;/code&gt; environment, with a budget of 100,000 timesteps (SAC can solve this task in only 5,000 steps), it will not converge&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.
With the default hyperparameters, you will get an average return of about -1000, far from the best performance you can get, which is around -200:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_baselines3 import PPO
# Faster, with Jax: from sbx import PPO

# Default hyperparameters don&#39;t work well
PPO(&amp;quot;MlpPolicy&amp;quot;, &amp;quot;Pendulum-v1&amp;quot;, verbose=1).learn(100_000, progress_bar=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;defining-the-search-space&#34;&gt;Defining the Search Space&lt;/h2&gt;
&lt;p&gt;The first thing to define when optimizing hyperparameters is the search space: what parameters to optimize and what range to explore?
You need also to decide from which distribution to sample from.
For example, in the case of continuous variables (like the discount factor $\gamma$ or the learning rate $\alpha$), values can be sampled from a uniform or 
&lt;a href=&#34;https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.loguniform.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;log-uniform&lt;/a&gt; distribution.&lt;/p&gt;
&lt;h3 id=&#34;sampling-methods&#34;&gt;Sampling methods&lt;/h3&gt;
&lt;p&gt;In practice, Optuna provides several &lt;code&gt;trial.suggest_...&lt;/code&gt; methods to define which parameter to optimize with which distribution.
For instance, to sample the discount factor $\gamma$ uniformly from the range $[0, 1]$, you would use &lt;code&gt;gamma = trial.suggest_float(&amp;quot;gamma&amp;quot;, 0.0, 1.0)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I recommend reading the 
&lt;a href=&#34;https://optuna.readthedocs.io/en/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optuna documentation&lt;/a&gt; to have a better understanding of the library and its features.
In the meantime, you need to know about some other useful methods for sampling hyperparameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;trial.suggest_float(..., log=True)&lt;/code&gt; to sample from a log-uniform distribution (ex: learning rate)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;trial.suggest_int(&amp;quot;name&amp;quot;, low, high)&lt;/code&gt; to sample integers (ex: mini-batch size), &lt;code&gt;low&lt;/code&gt; and &lt;code&gt;high&lt;/code&gt; are included&lt;/li&gt;
&lt;li&gt;&lt;code&gt;trial.suggest_categorical(&amp;quot;name&amp;quot;, choices)&lt;/code&gt; for sampling from a list of choices (ex: choosing an activation function)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Back to the PPO example on the &lt;code&gt;Pendulum-v1&lt;/code&gt; task, what hyperparameters can be optimized and what range should be explored for each of them?&lt;/p&gt;
&lt;h3 id=&#34;ppo-hyperparameters&#34;&gt;PPO hyperparameters&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PPO&lt;/a&gt; has many hyperparameters, but to keep the search small (and this blog post short), I will limit the search to four parameters: the learning rate $\alpha$, the discount factor $\gamma$, the activation function of the neural networks and the number of steps for data collection (&lt;code&gt;n_steps&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;Tuning the learning rate $\alpha$ is crucial for fast but stable training. If $\alpha$ is too big, the training tends to be unstable and usually leads to NaNs (or other numerical instability). If it is too small, it will take forever to converge.&lt;/p&gt;
&lt;p&gt;Since the learning rate $\alpha$ is a continuous variable (it is a float) and distinguishing between small learning rates is important, it is recommended to use a log-uniform distribution for sampling.
To search around the default learning rate $\alpha_0 = 3e^{-4}$, I define the search space to be in $[\alpha_0 / 10, 10 \alpha_0] = [3e^{-5}, 3e^{-3}]$.
This translates to &lt;code&gt;learning_rate = trial.suggest_float(&amp;quot;learning_rate&amp;quot;, 3e-5, 3e-3, log=True)&lt;/code&gt; with Optuna.&lt;/p&gt;
&lt;p&gt;The discount factor $\gamma$ represents a trade-off between optimizing short-term rewards and long-term rewards.
In general, we want to maximize the sum of undiscounted rewards ($\gamma = 1$), but in practice $\gamma &amp;lt; 1$ works best (while keeping $\gamma \approx 1$).
A recommended range for the discount factor $\gamma$ is $[0.97, 0.9999]$&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; (default is 0.99), or in Python: &lt;code&gt;gamma = trial.suggest_float(&amp;quot;gamma&amp;quot;, 0.97, 0.9999)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m considering two activation functions in this example: 
&lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tanh&lt;/a&gt; and 
&lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ReLU&lt;/a&gt;.
Because the activation function is sampled from a list of options, &lt;code&gt;activation_fn = trial.suggest_categorical(&amp;quot;activation_fn&amp;quot;, [&amp;quot;tanh&amp;quot;, &amp;quot;relu&amp;quot;])&lt;/code&gt; is the corresponding code&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Finally, PPO has a &lt;code&gt;n_steps&lt;/code&gt; parameter that controls the &amp;ldquo;number of steps to run for each environment per update&amp;rdquo;.
That is to say, PPO updates its policy every &lt;code&gt;n_steps * n_envs&lt;/code&gt; steps (and collect &lt;code&gt;n_steps * n_envs&lt;/code&gt; transitions to sample from).
This hyperparameter also affects the value and advantage estimation (larger &lt;code&gt;n_steps&lt;/code&gt; leads to less biased estimates).
It is recommended to use a power of two for its value&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;, i.e., we sample the exponent instead of the value directly, which translates to &lt;code&gt;n_steps_pow = trial.suggest_int(&amp;quot;n_steps_pow&amp;quot;, 5, 12)&lt;/code&gt; (from $2^5=32$ to $2^{12}=4096$).&lt;/p&gt;
&lt;p&gt;To summarize, this is the overall sampling function:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from typing import Any

import optuna

def sample_ppo_params(trial: optuna.Trial) -&amp;gt; dict[str, Any]:
    &amp;quot;&amp;quot;&amp;quot;Sampler for PPO hyperparameters.&amp;quot;&amp;quot;&amp;quot;
    # From 2**5=32 to 2**12=4096
    n_steps_pow = trial.suggest_int(&amp;quot;n_steps_pow&amp;quot;, 5, 12)
    gamma = trial.suggest_float(&amp;quot;one_minus_gamma&amp;quot;, 0.97, 0.9999)
    learning_rate = trial.suggest_float(&amp;quot;learning_rate&amp;quot;, 3e-5, 3e-3, log=True)
    activation_fn = trial.suggest_categorical(&amp;quot;activation_fn&amp;quot;, [&amp;quot;tanh&amp;quot;, &amp;quot;relu&amp;quot;])
    # Convert power of two to number of steps
    n_steps = 2**n_steps_pow
    # Display true values
    trial.set_user_attr(&amp;quot;n_steps&amp;quot;, n_steps)
    # Convert to PyTorch objects
    activation_fn = {&amp;quot;tanh&amp;quot;: nn.Tanh, &amp;quot;relu&amp;quot;: nn.ReLU}[activation_fn]

    return {
        &amp;quot;n_steps&amp;quot;: n_steps,
        &amp;quot;gamma&amp;quot;: gamma,
        &amp;quot;learning_rate&amp;quot;: learning_rate,
        &amp;quot;policy_kwargs&amp;quot;: {
            &amp;quot;activation_fn&amp;quot;: activation_fn,
        },
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;defining-the-objective-function&#34;&gt;Defining the Objective Function&lt;/h2&gt;
&lt;p&gt;After choosing the search space, you need to define the objective function.
In reinforcement learning, we usually want to get the best performance for a given budget (either in terms of samples or training time), i.e., we try to maximize the episodic reward.&lt;/p&gt;
&lt;p&gt;One way to measure the performance is to periodically evaluate the agent on a test environment for multiple episodes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_baselines3.common.evaluation import evaluate_policy
# model = PPO(&amp;quot;MlpPolicy&amp;quot;, &amp;quot;Pendulum-v1&amp;quot;)
# eval_env = gym.make(&amp;quot;Pendulum-v1&amp;quot;)
# Note: by default, evaluate_policy uses the deterministic policy
mean_return, std_return = evaluate_policy(model, eval_env, n_eval_episodes=20)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In practice, with SB3, I use a custom 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;callback&lt;/a&gt; to trigger evaluations at different stages of training:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_baselines3.common.callbacks import BaseCallback

class TrialEvalCallback(BaseCallback):
    &amp;quot;&amp;quot;&amp;quot;Callback used for evaluating and reporting a trial.&amp;quot;&amp;quot;&amp;quot;

    def _on_step(self) -&amp;gt; bool:
        if self.eval_freq &amp;gt; 0 and self.n_calls % self.eval_freq == 0:
            # Evaluate the current policy every n_calls
            mean_return, _ = evaluate_policy(self.model, self.eval_env)
            self.eval_idx += 1
            # Send report to Optuna
            self.trial.report(mean_return, self.eval_idx)
            # Prune (stop training) trial if needed
            if self.trial.should_prune():
                self.is_pruned = True
                return False
        return True
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This callback also allows to stop training early if a trial is too bad and should be pruned (by checking the value of &lt;code&gt;trial.should_prune()&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;The full objective method contains additional code to create the training environment, sample the hyperparameters, instantiate the RL agent and train it:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_baselines3.common.env_util import make_vec_env

N_ENVS = 5
N_TIMESTEPS = 40_000
# Evaluate every 20_000 steps
# each vec_env.step() is N_ENVS steps
EVAl_FREQ = 20_000 // N_ENVS

def objective(trial: optuna.Trial) -&amp;gt; float:
    # Create train and eval envs,
    # I use multiple envs in parallel for faster training
    vec_env = make_vec_env(&amp;quot;Pendulum-v1&amp;quot;, n_envs=N_ENVS)
    eval_env = make_vec_env(&amp;quot;Pendulum-v1&amp;quot;, n_envs=N_ENVS)
    # Sample hyperparameters. and create the RL model
    model = PPO(&amp;quot;MlpPolicy&amp;quot;, vec_env, **sample_ppo_params(trial))

    # Create the callback that will periodically evaluate and report the performance.
    eval_callback = TrialEvalCallback(
        eval_env,
        trial,
        n_eval_episodes=20,
        eval_freq=EVAl_FREQ, 
    )
    # Train the RL agent
    model.learn(N_TIMESTEPS, callback=eval_callback)

    if eval_callback.is_pruned:
        raise optuna.exceptions.TrialPruned()
    # Report final performance
    return eval_callback.last_mean_reward
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;choosing-sampler-and-pruner&#34;&gt;Choosing Sampler and Pruner&lt;/h2&gt;
&lt;p&gt;Finally, after defining the search space and the objective function, you have to choose a sampler and (optionally) a pruner (see 
&lt;a href=&#34;../hyperparam-tuning/&#34;&gt;part one&lt;/a&gt;).
If you don&amp;rsquo;t know what to choose, Optuna now has an 
&lt;a href=&#34;https://hub.optuna.org/samplers/auto_sampler/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AutoSampler&lt;/a&gt; which choosees a recommended sampler for you (between &lt;code&gt;TPESampler&lt;/code&gt;, &lt;code&gt;GPSampler&lt;/code&gt; and &lt;code&gt;CmaEsSampler&lt;/code&gt;), based on heuristics.&lt;/p&gt;
&lt;p&gt;Here, I selected &lt;code&gt;TPESampler&lt;/code&gt; and &lt;code&gt;MedianPruner&lt;/code&gt; because they tend to be good default choices. Don&amp;rsquo;t forget to pass &lt;code&gt;n_startup_trials&lt;/code&gt; to both to warm up the optimization with a &lt;code&gt;RandomSampler&lt;/code&gt; (uniform sampler) and to avoid premature convergence (like pruning potentially good trials too early):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler, RandomSampler
 
# Select the sampler, can be random, TPESampler, CMAES, ...
sampler = TPESampler(n_startup_trials=5)
# Do not prune before 1/3 of the max budget is used
pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=N_EVALUATIONS // 3)
	
# Create the study and start the hyperparameter optimization
study = optuna.create_study(sampler=sampler, pruner=pruner, direction=&amp;quot;maximize&amp;quot;)
# This script can be launch in parallel when using a database
# We pass the objective function defined previously
study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT)
# Best result
best_trial = study.best_trial
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Et voilÃ ! That&amp;rsquo;s all you need to run automatic hyperparameter optimization.
If you now run the 
&lt;a href=&#34;https://gist.github.com/araffin/d16e77aa88ffc246856f4452ab8a2524&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;final script&lt;/a&gt; for five minutes, it should quickly find hyperparameters that give good results.&lt;/p&gt;
&lt;p&gt;For example, in one of the runs I did, I was able to get in just two minutes:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-yaml&#34;&gt;Number of finished trials: 21
Best trial:
  Value:  -198.01224440000001
  Params: 
    n_steps_pow: 8
    gamma: 0.9707141699579157
    learning_rate: 0.0014974865679170315
    activation_fn: relu
  User attrs:
    n_steps: 256
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;To verify that these hyperparameters actually work (more on that soon), you can use:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
import torch as th

vec_env = make_vec_env(&amp;quot;Pendulum-v1&amp;quot;, n_envs=5)
# Using optimized hyperparameters
policy_kwargs = dict(activation_fn=th.nn.ReLU)
hyperparams = dict(n_steps=256, gamma=0.97, learning_rate=1.5e-3)

model = PPO(&amp;quot;MlpPolicy&amp;quot;, vec_env, verbose=1, **hyperparams)
model.learn(40_000, progress_bar=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It should give you better results than the default hyperparameters with half of the training budget.&lt;/p&gt;
&lt;img style=&#34;max-width:80%&#34; src=&#34;./img/Results_Pendulum-v1.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;Learning curve for PPO with default and tuned hyperparameters&lt;/p&gt;
&lt;p&gt;Note: I recommend using the 
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL Zoo&lt;/a&gt; for more complex settings.
It includes automatic hyperparameter tuning, loading trial and distributed optimization.
Example command to optimize PPO on the Pendulum-v1 environment:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;python -m rl_zoo3.train --algo ppo --env Pendulum-v1 -optimize --storage ./demo.log --study-name demo
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;distributed-optimization&#34;&gt;Distributed Optimization&lt;/h2&gt;
&lt;p&gt;A simple way to speed up the optimization process is to run multiple trials in parallel.
To do this, you need to use a 
&lt;a href=&#34;https://optuna.readthedocs.io/en/stable/reference/storages.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;database&lt;/a&gt; and pass it to Optuna&amp;rsquo;s &lt;code&gt;create_study()&lt;/code&gt; method.&lt;/p&gt;
&lt;p&gt;The easiest way to distribute tuning is to use a 
&lt;a href=&#34;https://optuna.readthedocs.io/en/stable/reference/generated/optuna.storages.JournalStorage.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;log file&lt;/a&gt; for storage and start the same script in multiple terminals (and potentially on multiple machines):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;storage_file = &amp;quot;./my_studies.log&amp;quot;
study_name = &amp;quot;ppo-Pendulum-v1_1&amp;quot;

storage = optuna.storages.JournalStorage(
    optuna.storages.journal.JournalFileBackend(storage_file),
)
study = optuna.create_study(
    ...
    storage=storage,
    study_name=study_name,
    load_if_exists=True,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you use a database (you should), you can also use 
&lt;a href=&#34;https://github.com/optuna/optuna-dashboard&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optuna dashboard&lt;/a&gt; to monitor the optimization progress.&lt;/p&gt;
&lt;img style=&#34;max-width:100%&#34; src=&#34;https://optuna-dashboard.readthedocs.io/en/latest/_images/optuna-dashboard.gif&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;Optuna dashboard demo&lt;/p&gt;
&lt;h2 id=&#34;tips-and-tricks&#34;&gt;Tips and Tricks&lt;/h2&gt;
&lt;p&gt;Before concluding this blog post, I would like to give you some tips and tricks to keep in mind when doing hyperparameter tuning.&lt;/p&gt;
&lt;h3 id=&#34;start-simple&#34;&gt;Start simple&lt;/h3&gt;
&lt;p&gt;First, as with any RL problem, starting simple is the 
&lt;a href=&#34;https://www.youtube.com/watch?v=eZ6ZEpCi6D8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;key to success&lt;/a&gt;.
Do not try to optimize too many hyperparameters at once, using large ranges.
My advice would be to start with a minimal number of parameters (i.e., start with a small search space) and increase their number and ranges only as needed.&lt;/p&gt;
&lt;p&gt;For example, to decide whether to increase or decrease the search range, you can look at the best trials so far.
If the best trials are close to the limits of the search space (saturation), this is a sign that you should increase the limits.
On the other hand, if above a certain threshold for a parameter, all trials are bad, you can probably reduce the search space.&lt;/p&gt;
&lt;p&gt;Another thing to keep in mind is that most of the time, you don&amp;rsquo;t need automatic tuning.
Simply training for a longer time (i.e., using a larger training budget) can improve the results without changing the hyperparameters.&lt;/p&gt;
&lt;h3 id=&#34;post-evaluation-to-remove-noise&#34;&gt;Post-Evaluation to Remove Noise&lt;/h3&gt;
&lt;p&gt;Last but not least (and perhaps the most important tip), do not forget that RL training is a stochastic process.
This means that the performance reported for each trial can be noisy: if you run the same trial but with a different random seed, you might get different results.&lt;/p&gt;
&lt;p&gt;I tend to approach this problem in two ways.&lt;/p&gt;
&lt;p&gt;To filter out the evaluation noise, I usually re-evaluate the top trials multiple times to find out which ones were &amp;ldquo;lucky seeds&amp;rdquo; and which ones work consistently.
Another way to deal with this problem is to do multiple runs per trial: each run uses the same hyperparameters but starts with a different random seed.
However, this technique is expensive in terms of computation time and makes it difficult to prune out bad trials early.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this second part, I went through the process of doing automatic hyperparameter tuning in practice, using the Optuna library.
I&amp;rsquo;ve covered:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;defining the search space and the objective function&lt;/li&gt;
&lt;li&gt;choosing a sampler and a pruner&lt;/li&gt;
&lt;li&gt;speeding up the tuning process with distributed optimization&lt;/li&gt;
&lt;li&gt;tips and tricks to keep in mind when doing automatic hyperparameter tuning&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As a conclusion and transition to the 
&lt;a href=&#34;../tune-sac-isaac-sim/&#34;&gt;next blog post&lt;/a&gt;, I will use this technique to 
&lt;a href=&#34;../sac-massive-sim/&#34;&gt;tune SAC for fast training&lt;/a&gt; when using a massively parallel environment like Isaac Sim.&lt;/p&gt;
&lt;p&gt;PS: In case you missed it, you can find the final script here: 
&lt;a href=&#34;https://gist.github.com/araffin/d16e77aa88ffc246856f4452ab8a2524&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gist.github.com/araffin/d16e77aa88ffc246856f4452ab8a2524&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;appendix---the-optuna-library&#34;&gt;Appendix - The Optuna Library&lt;/h2&gt;
&lt;p&gt;Among the various open-source libraries for hyperparameter optimization (such as 
&lt;a href=&#34;https://github.com/hyperopt/hyperopt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;hyperopt&lt;/a&gt; or 
&lt;a href=&#34;https://github.com/facebook/Ax&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ax&lt;/a&gt;), I chose 
&lt;a href=&#34;https://optuna.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optuna&lt;/a&gt; for multitple reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it has a clean API and good 
&lt;a href=&#34;https://optuna.readthedocs.io/en/stable/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;it supports many samplers and pruners&lt;/li&gt;
&lt;li&gt;it has some nice additional features (like easy distributed optimization support, multi-objective support or the 
&lt;a href=&#34;https://optuna-dashboard.readthedocs.io/en/latest/getting-started.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;optuna-dashboard&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@article{raffin2025optuna,
  title   = &amp;quot;Automatic Hyperparameter Tuning - In Practice&amp;quot;,
  author  = &amp;quot;Raffin, Antonin&amp;quot;,
  journal = &amp;quot;araffin.github.io&amp;quot;,
  year    = &amp;quot;2025&amp;quot;,
  month   = &amp;quot;April&amp;quot;,
  url     = &amp;quot;https://araffin.github.io/post/optuna/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;I would like to thank Anssi and Costa for their feedback =).&lt;/p&gt;
&lt;h3 id=&#34;did-you-find-this-post-helpful-consider-sharing-it-&#34;&gt;Did you find this post helpful? Consider sharing it ð&lt;/h3&gt;
&lt;h2 id=&#34;footnotes&#34;&gt;Footnotes&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Without proper 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues/633&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;truncation handling&lt;/a&gt;, PPO will actually not converge even in 1 million steps with default hyperparameters.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;A common way to define the param range is to start small and later increase the search space if the best parameters found are at the boundary of the defined range.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;I convert strings to PyTorch objects later because options need to be serializable to be stored by Optuna.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;One of the main reasons for choosing a power of two is that the GPU kernel/hardware is optimized for power of two operations. Also, in practice, &lt;code&gt;n_steps=4096&lt;/code&gt; vs. &lt;code&gt;n_steps=4000&lt;/code&gt; doesn&amp;rsquo;t make much difference, so using a power of two reduces the search space.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms (Part I)</title>
      <link>/post/sac-massive-sim/</link>
      <pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate>
      <guid>/post/sac-massive-sim/</guid>
      <description>&lt;p&gt;This post details how I managed to get the Soft-Actor Critic (SAC) and other off-policy reinforcement learning algorithms to work on massively parallel simulators (think Isaac Sim with thousands of robots simulated in parallel).
If you follow the journey, you will learn about overlooked details in task design and algorithm implementation that can have a big impact on performance.&lt;/p&gt;
&lt;p&gt;Spoiler alert: 
&lt;a href=&#34;#appendix---affected-paperscode&#34;&gt;quite a few papers/code&lt;/a&gt; are affected by the problem described below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Part I is about identifying the problem and trying out quick fixes on SAC.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;../tune-sac-isaac-sim/&#34;&gt;Part II&lt;/a&gt; is about tuning SAC for speed and making it work as good as PPO.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;a-suspicious-trend-ppo-ppo-ppo-&#34;&gt;A Suspicious Trend: PPO, PPO, PPO, &amp;hellip;&lt;/h2&gt;
&lt;p&gt;The story begins a few months ago when I saw another paper using the same recipe for learning locomotion: train a PPO agent in simulation using thousands of environments in parallel and domain randomization, then deploy it on the real robot.
This recipe has become the standard since 2021, when ETH Zurich and NVIDIA&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; showed that it was possible to learn locomotion in minutes on a single workstation.
The codebase and the simulator (called Isaac Gym at that time) that were published became the basis for much follow-up work&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;As an RL researcher focused on 
&lt;a href=&#34;https://proceedings.mlr.press/v164/raffin22a/raffin22a.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;learning directly on real robots&lt;/a&gt;, I was curious and suspicious about one aspect of this trend: why is no one trying an algorithm other than PPO?&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;
PPO benefits from fast and parallel environments&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;, but PPO is not the only deep reinforcement learning (DRL) algorithm for continuous control tasks and there are alternatives like SAC or TQC that can lead to better performance&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;So I decided to investigate why these off-policy algorithms are not used by practitioners, and maybe why they don&amp;rsquo;t work with massively parallel simulators.&lt;/p&gt;
&lt;h2 id=&#34;why-it-matters---fine-tuning-on-real-robots&#34;&gt;Why It Matters? - Fine Tuning on Real Robots&lt;/h2&gt;
&lt;p&gt;If we could make SAC work with these simulators, then it would be possible to train in simulation and fine-tune on the real robot using the same algorithm (PPO is too sample-inefficient to train on a single robot) .&lt;/p&gt;
&lt;p&gt;By using other algorithms it might also be possible to get better performance.
Finally, it is always good to have a better understanding of what works or not and why.
As researchers, we tend to publish only positive results, but I think a lot of valuable insights are lost in our unpublished failures.&lt;/p&gt;
&lt;a href=&#34;https://araffin.github.io/slides/design-real-rl-experiments/&#34;&gt;
  &lt;img style=&#34;max-width: 50%&#34; src=&#34;https://araffin.github.io/slides/tips-reliable-rl/images/bert/real_bert.jpg&#34; alt=&#34;The DLR bert quadruped robot, standing on a stone.&#34; /&gt;
&lt;/a&gt;
  &lt;p style=&#34;font-size: 12pt; text-align:center;&#34;&gt;The DLR bert elastic quadruped&lt;/p&gt;
&lt;h2 id=&#34;the-path-of-least-resistance-hypothesis&#34;&gt;(The Path of Least Resistance) Hypothesis&lt;/h2&gt;
&lt;p&gt;Before digging any further, I had some hypotheses as to why PPO was the only algorithm used:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;PPO is fast to train (in terms of computation time) and was tuned for the massively parallel environment.&lt;/li&gt;
&lt;li&gt;As researchers, we tend to take the path of least resistance and build on proven solutions (the original training code is open source and the simulator is freely available) to get new interesting results&lt;sup id=&#34;fnref:6&#34;&gt;&lt;a href=&#34;#fn:6&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;li&gt;There may be some peculiarities in the environment design that favor PPO over other algorithms. In other words, the massively parallel environments might be optimized for PPO.&lt;/li&gt;
&lt;li&gt;SAC/TQC and derivatives are tuned for sample efficiency, not fast wall clock time. In the case of massively parallel simulation, what matters is how long it takes to train, not how many samples are used. They probably need to be tuned/adjusted for this new setting.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: during my journey, I will (obviously) be using 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable-Baselines3&lt;/a&gt; and its fast Jax version 
&lt;a href=&#34;https://github.com/araffin/sbx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SBX&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-hunt-begins&#34;&gt;The Hunt Begins&lt;/h2&gt;
&lt;p&gt;There are now many massively parallel simulators available (Isaac Sim, Brax, MJX, Genesis, &amp;hellip;), here, I chose to focus on Isaac Sim because it was one of the first and is probably the most influential one.&lt;/p&gt;
&lt;p&gt;As with any RL problem, starting simple is the 
&lt;a href=&#34;https://www.youtube.com/watch?v=eZ6ZEpCi6D8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;key to success&lt;/a&gt;.&lt;/p&gt;
&lt;video controls src=&#34;https://b2drop.eudat.eu/public.php/dav/files/z5LFrzLNfrPMd9o/ppo_trained.mp4&#34;&gt;
&lt;/video&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;A PPO agent trained on the &lt;code&gt;Isaac-Velocity-Flat-Unitree-A1-v0&lt;/code&gt; locomotion task.
  &lt;br&gt;
  Green arrow is the desired velocity, blue arrow represents the current velocity
&lt;/p&gt;
&lt;p&gt;Therefore, I decided to focus on the &lt;code&gt;Isaac-Velocity-Flat-Unitree-A1-v0&lt;/code&gt; locomotion task first, because it is simple but representative.
The goal is to learn a policy that can move the Unitree A1 quadruped in any direction on a flat ground, following a commanded velocity (the same way you would control a robot with a joystick).
The agent receives information about its current task as input (joint positions, velocities, desired velocity, &amp;hellip;) and outputs desired joint positions (12D vector, 3 joints per leg).
The robot is rewarded for following the correct desired velocity (linear and angular) and for other secondary tasks (feet air time, smooth control, &amp;hellip;).
An episode ends when the robot falls over and is timed out (
&lt;a href=&#34;https://www.youtube.com/watch?v=eZ6ZEpCi6D8&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;truncation&lt;/a&gt;) after 1000 steps&lt;sup id=&#34;fnref:7&#34;&gt;&lt;a href=&#34;#fn:7&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;After some 
&lt;a href=&#34;https://github.com/isaac-sim/IsaacLab/pull/2022&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quick optimizations&lt;/a&gt; (SB3 now runs 4x faster, at 60 000 fps for 2048 envs with PPO), I did some sanity checks.
First, I ran PPO with the tuned hyperparameters found in the repository, and it was able to quickly solve the task.
In 5 minutes, it gets an average episode return of ~30 (above an episode return of 15, the task is almost solved).
Then I tried SAC and TQC, with default hyperparameters (and observation normalization), and, as expected, it didn&amp;rsquo;t work.
No matter how long it was training, there was no sign of improvement.&lt;/p&gt;
&lt;p&gt;Looking at the simulation GUI, something struck me: the robots were making very large random movements.
Something was wrong.&lt;/p&gt;
&lt;video controls src=&#34;https://b2drop.eudat.eu/public.php/dav/files/z5LFrzLNfrPMd9o/limits_train.mp4&#34;&gt;
&lt;/video&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;SAC out of the box on Isaac Sim during training.&lt;/p&gt;
&lt;p&gt;Because of the very large movements, my suspicion was towards what action the robot is allowed to do.
Looking at the code, the RL agent commands a (scaled) 
&lt;a href=&#34;https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab/isaaclab/envs/mdp/actions/joint_actions.py#L134&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;delta&lt;/a&gt; with respect to a default 
&lt;a href=&#34;https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py#L112&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;joint position&lt;/a&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Note desired_joint_pos is of dimension 12 (3 joints per leg)
desired_joint_pos = default_joint_pos + scale * action
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then, let&amp;rsquo;s look at the action space itself (I&amp;rsquo;m using &lt;code&gt;ipdb&lt;/code&gt; to have an interactive debugger):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import ipdb; ipdb.set_trace()
&amp;gt;&amp;gt; vec_env.action_space
Box(-100.0, 100.0, (12,), float32)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ah ah!
The action space defines continuous actions of dimension 12 (nothing wrong here), but the limits $[-100, 100]$ are surprisingly large, e.g., it allows a delta of +/- 1432 deg!! in joint angle when 
&lt;a href=&#34;https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/a1/rough_env_cfg.py#L30&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scale=0.25&lt;/a&gt;, like for the Unitree A1 robot.
To understand why 
&lt;a href=&#34;https://www.youtube.com/watch?v=Ikngt0_DXJg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;normalizing&lt;/a&gt; the action space 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;matters&lt;/a&gt; (usually a bounded space in $[-1, 1]$), we need to dig deeper into how PPO works.&lt;/p&gt;
&lt;h2 id=&#34;ppo-gaussian-distribution&#34;&gt;PPO Gaussian Distribution&lt;/h2&gt;
&lt;p&gt;Like many RL algorithms, 
&lt;a href=&#34;https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PPO&lt;/a&gt; relies on a probability distribution to select actions.
During training, at each timestep, it samples an action $a_t \sim N(\mu_\theta(s_t), \sigma^2)$ from a Gaussian distribution in the case of continuous actions&lt;sup id=&#34;fnref:8&#34;&gt;&lt;a href=&#34;#fn:8&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;8&lt;/a&gt;&lt;/sup&gt;.
The mean of the Gaussian $\mu_\theta(s_t)$ is the output of the actor neural network (with parameters $\theta$) and the standard deviation is a 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/blob/55d6f18dbd880c62d40a276349b8bac7ebf453cd/stable_baselines3/common/distributions.py#L150&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;learnable parameter&lt;/a&gt; $\sigma$, usually 
&lt;a href=&#34;https://github.com/leggedrobotics/rsl_rl/blob/f80d4750fbdfb62cfdb0c362b7063450f427cf35/rsl_rl/modules/actor_critic.py#L26&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;initialized&lt;/a&gt; with $\sigma_0 = 1.0$.&lt;/p&gt;
&lt;p&gt;This means that at the beginning of training, most of the sampled actions will be in $[-3, 3]$ (from the 
&lt;a href=&#34;https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Three Sigma Rule&lt;/a&gt;):&lt;/p&gt;
&lt;img style=&#34;max-width:80%&#34; src=&#34;./img/gaussian.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;The initial Gaussian distribution used by PPO for sampling actions.&lt;/p&gt;
&lt;p&gt;Back to our original topic, because of the way $\sigma$ is initialized, if the action space has large bounds (upper/lower bounds &amp;raquo; 1), PPO will almost never sample actions near the limits.
In practice, the actions taken by PPO will even be far away from them.
Now, let&amp;rsquo;s compare the initial PPO action distribution with the Unitree A1 action space:&lt;/p&gt;
&lt;img style=&#34;max-width:80%&#34; src=&#34;./img/gaussian_large_bounds.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;The same initial Gaussian distribution but with the perspective of the Unitree A1 action space $[-100, 100]$&lt;/p&gt;
&lt;p&gt;For reference, we can plot the action distribution of PPO after training&lt;sup id=&#34;fnref:9&#34;&gt;&lt;a href=&#34;#fn:9&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;9&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;img src=&#34;./img/dist_actions_trained_ppo.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;Distribution of actions for PPO after training (on 64 000 steps).&lt;/p&gt;
&lt;p&gt;The min/max values per dimension:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;&amp;gt;&amp;gt; actions.min(axis=0)
array([-3.6, -2.5, -3.1, -1.8, -4.5, -4.2, -4. , -3.9, -2.8, -2.8, -2.9, -2.7])
&amp;gt;&amp;gt; actions.max(axis=0)
array([ 3.2,  2.8,  2.7,  2.8,  2.9,  2.7,  3.2,  2.9,  7.2,  5.7,  5. ,  5.8])

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Again, most of the actions are centered around zero (which makes sense, since it corresponds to the quadruped initial position, which is usually chosen to be stable), and there are almost no actions outside $[-5, 5]$ (less than 0.1%): PPO uses less than 5% of the action space!&lt;/p&gt;
&lt;p&gt;Now that we know that we need less than 5% of the action space to solve the task, let&amp;rsquo;s see why this might explain why SAC doesn&amp;rsquo;t work in this case&lt;sup id=&#34;fnref:10&#34;&gt;&lt;a href=&#34;#fn:10&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;10&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;!-- Note: if in rad, 3 rad is already 171 degrees (but action scale = 0.25, so ~40 deg, action scale = 0.5 for Anymal). --&gt;
&lt;h2 id=&#34;sac-squashed-gaussian&#34;&gt;SAC Squashed Gaussian&lt;/h2&gt;
&lt;p&gt;SAC and other off-policy algorithms for continuous actions (such as DDPG, TD3 or 
&lt;a href=&#34;https://sb3-contrib.readthedocs.io/en/master/modules/tqc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TQC&lt;/a&gt;) have an additional transformation at the end of the actor network.
In SAC, actions are sampled from an unbounded Gaussian distribution and then passed through a 
&lt;a href=&#34;https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;$tanh()$&lt;/a&gt; function to squash them to the range $[-1, 1]$.
SAC then linearly rescales the sampled action to match the action space definition, i.e. it transforms the action from $[-1, 1]$ to $[\text{low}, \text{high}]$&lt;sup id=&#34;fnref:11&#34;&gt;&lt;a href=&#34;#fn:11&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;11&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;What does this mean?
Assuming we start with a standard deviation similar to PPO, this is what the sampled action distribution looks like after squashing&lt;sup id=&#34;fnref:12&#34;&gt;&lt;a href=&#34;#fn:12&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;12&lt;/a&gt;&lt;/sup&gt;:&lt;/p&gt;
&lt;img src=&#34;./img/squashed_vs_gaussian.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;The equivalent initial squashed Gaussian distribution.&lt;/p&gt;
&lt;p&gt;And after rescaling to the environment limits (with PPO distribution to put it in perspective):&lt;/p&gt;
&lt;img src=&#34;./img/squashed_rescaled.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;The same initial squashed Gaussian distribution but rescaled to the Unitree A1 action space $[-100, 100]$&lt;/p&gt;
&lt;p&gt;As you can see, these are two completely different initial distributions at the beginning of training!
The fact that the actions are rescaled to fit the action space boundaries explains the very large movements seen during training, and also explains why it was impossible for SAC to learn anything useful.&lt;/p&gt;
&lt;h2 id=&#34;quick-fix&#34;&gt;Quick Fix&lt;/h2&gt;
&lt;p&gt;When I discovered that the action limits were way too large, my first reflex was to re-train SAC, but with only 3% of the action space, to more or less match the effective action space of PPO.
Although it didn&amp;rsquo;t reach PPO performance, there was finally some sign of life (an average episodic return slightly positive after a while).&lt;/p&gt;
&lt;p&gt;What I tried next was to use a neural network similar to the one used by PPO for this task and reduce SAC exploration by having a smaller entropy coefficient&lt;sup id=&#34;fnref:13&#34;&gt;&lt;a href=&#34;#fn:13&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;13&lt;/a&gt;&lt;/sup&gt; at the beginning of training.
Bingo!
SAC finally learned to solve the task!&lt;/p&gt;
&lt;img style=&#34;max-width:100%&#34; src=&#34;./img/learning_curve.svg&#34;/&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;Learning curve on the Unitree A1 task using 1024 envs.&lt;/p&gt;
&lt;video controls src=&#34;https://b2drop.eudat.eu/public.php/dav/files/z5LFrzLNfrPMd9o/sac_trained_cut_1.mp4&#34;&gt;
&lt;/video&gt;
&lt;p style=&#34;font-size: 14pt; text-align:center;&#34;&gt;Trained SAC agent after the quick fix.&lt;/p&gt;
&lt;p&gt;SAC Hyperparameters (the ones not specified are 
&lt;a href=&#34;https://github.com/araffin/sbx/blob/8238fccc19048340870e4869813835b8fb02e577/sbx/sac/sac.py#L54-L64&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SB3 defaults&lt;/a&gt;):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;sac_hyperparams = dict(
    policy_kwargs={
        # Similar to PPO network tuned for Unitree A1 task
        &amp;quot;activation_fn&amp;quot;: jax.nn.elu,
        &amp;quot;net_arch&amp;quot;: [512, 256, 128],
    },
    # When using 2048 envs, gradient_steps=512 corresponds
    # to an update-to-data ratio of 1/4
    gradient_steps=512,
    ent_coef=&amp;quot;auto_0.006&amp;quot;,
)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;thats-all-folks&#34;&gt;That&amp;rsquo;s all folks?&lt;/h2&gt;
&lt;p&gt;Although SAC can now solve this locomotion task, it takes more time to train, is not consistent, and the performance is slightly below PPO&amp;rsquo;s.
In addition, SAC&amp;rsquo;s learned gaits are not as pleasing as PPO&amp;rsquo;s, for example, SAC agents tend to keep one leg up in the air&amp;hellip;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;../tune-sac-isaac-sim/&#34;&gt;Part II&lt;/a&gt; explores these aspects (and more environments), review SAC design decisions (for example, try to remove the squashed Gaussian), and tune it for speed, but for now let&amp;rsquo;s see what this means for the RL community.&lt;/p&gt;
&lt;h2 id=&#34;outro-what-does-that-mean-for-the-rl-community&#34;&gt;Outro: What Does That Mean for the RL Community?&lt;/h2&gt;
&lt;p&gt;When I found out about this problem, I was curious to see how widespread it was in the community.
After a quick search, it turns out that there are a lot of papers/code affected&lt;sup id=&#34;fnref:14&#34;&gt;&lt;a href=&#34;#fn:14&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;14&lt;/a&gt;&lt;/sup&gt; by this large boundary problem (see a non-exhaustive 
&lt;a href=&#34;#appendix---affected-paperscode&#34;&gt;list of affected papers/code below&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Although the initial choice of bounds may be a conscious and convenient one (no need to specify the real bounds, PPO will figure it out), it seems to have worked a bit by accident for those who built on top of it, and probably discouraged practitioners from trying other algorithms.&lt;/p&gt;
&lt;p&gt;My recommendation would be to always have properly defined action bounds, and if they are not known in advance, you can always 
&lt;a href=&#34;https://gist.github.com/araffin/e069945a68aa0d51fcdff3f01e945c70&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;plot the action distribution&lt;/a&gt; and adjust the limits when iterating on the environment design.&lt;/p&gt;
&lt;!-- TODO: get feedback if this is an overlooked problem or known issue but PPO is nice because it can decide which action space to choose? --&gt;
&lt;!-- Quick tuning: use TQC (equal or better perf than SAC), faster training with JIT and multi gradient steps, policy delay and train_freq, bigger batch size.

Note: entropy coeff is inverse reward scale in maximum entropy RL --&gt;
&lt;!-- ## Tuning for speed

Automatic hyperparameter optimization with Optuna.
Good and fast results (not as fast as PPO but more sample efficient).
Try schedule of action space (start small and make it bigger over time): not so satifying,
looking into unbounded action space. --&gt;
&lt;!-- ## PPO Gaussian dist vs Squashed Gaussian

Difference between log std computation (state-dependent with clipping vs independent global param).

Trying to make SAC looks like PPO, move to unbounded Gaussian dist, instabilities.
Fixes: clip max action, l2 loss (like [SAC original implementation](https://github.com/haarnoja/sac/blob/8258e33633c7e37833cc39315891e77adfbe14b2/sac/distributions/normal.py#L69-L70))
Replace state-dependent std with independent: auto-tuning entropy coeff broken, need to fix it (TODO: investigate why). --&gt;
&lt;!-- SAC initial commit https://github.com/haarnoja/sac/blob/fa226b0dcb244d69639416995311cc5b4092c8f7/sac/distributions/gmm.py#L122 --&gt;
&lt;!-- Note: SAC work on MuJoCo like env

Note: two variations of the same issue: unbounded (matches Gaussian dist real domain)
and clipped to high limits

Note: brax PPO seems to implement tanh Gaussian dist (action limited to [-1, 1]): 
https://github.com/google/brax/blob/241f9bc5bbd003f9cfc9ded7613388e2fe125af6/brax/training/agents/ppo/networks.py#L78
MuJoCo playground and Brax clip: https://github.com/google-deepmind/mujoco_playground/blob/0f3adda84f2a2ab55e9d9aaf7311c917518ec25c/mujoco_playground/_src/wrapper_torch.py#L158
but not really defined explicitly in the env (for the limits)

Note: rescale action doesn&#39;t work for PPO, need retuning? need tanh normal? --&gt;
&lt;h3 id=&#34;appendix---affected-paperscode&#34;&gt;Appendix - Affected Papers/Code&lt;/h3&gt;
&lt;p&gt;Please find here a non-exhaustive list of papers/code affected by the large bound problem:&lt;/p&gt;
&lt;!-- - [MuJoCo Playground](https://github.com/google-deepmind/mujoco_playground/blob/0f3adda84f2a2ab55e9d9aaf7311c917518ec25c/mujoco_playground/_src/locomotion/go1/joystick.py#L239) --&gt;
&lt;!-- https://github.com/Argo-Robot/quadrupeds_locomotion/blob/45eec904e72ff6bafe1d5378322962003aeff88d/src/go2_env.py#L173 --&gt;
&lt;!-- https://github.com/leggedrobotics/legged_gym/blob/17847702f90d8227cd31cce9c920aa53a739a09a/legged_gym/envs/base/legged_robot.py#L85 --&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/isaac-sim/IsaacLab/blob/c4bec8fe01c2fd83a0a25da184494b37b3e3eb61/source/isaaclab_rl/isaaclab_rl/sb3.py#L154&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IsaacLab&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/leggedrobotics/legged_gym/blob/17847702f90d8227cd31cce9c920aa53a739a09a/legged_gym/envs/base/legged_robot_config.py#L164&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning to Walk in Minutes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/nico-bohlinger/one_policy_to_run_them_all/blob/d9d166c348496c9665dd3ebabc20efb6d8077161/one_policy_to_run_them_all/environments/unitree_a1/environment.py#L140&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;One Policy to Run Them All&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/Argo-Robot/quadrupeds_locomotion/blob/45eec904e72ff6bafe1d5378322962003aeff88d/src/go2_train.py#L104&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Genesis env&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/LeCAR-Lab/ASAP/blob/c78664b6d2574f62bd2287e4b54b4f8c2a0a47a5/humanoidverse/config/robot/g1/g1_29dof_anneal_23dof.yaml#L161&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ASAP Humanoid&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/LeCAR-Lab/ABS/blob/9b95329ffb823c15dead02be620ff96938e4d0a3/training/legged_gym/legged_gym/envs/base/legged_robot_config.py#L169&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Agile But Robust&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/Improbable-AI/rapid-locomotion-rl/blob/f5143ef940e934849c00284e34caf164d6ce7b6e/mini_gym/envs/base/legged_robot_config.py#L209&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rapid Locomotion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/MarkFzp/Deep-Whole-Body-Control/blob/8159e4ed8695b2d3f62a40d2ab8d88205ac5021a/legged_gym/legged_gym/envs/widowGo1/widowGo1_config.py#L114&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Whole Body Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/ZiwenZhuang/parkour/blob/789e83c40b95fdd49fda7c1725c8c573df42d2a9/legged_gym/legged_gym/envs/base/legged_robot_config.py#L169&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Robot Parkour Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can probably find many more looking at 
&lt;a href=&#34;https://scholar.google.com/scholar?cites=8503164023891275626&amp;amp;as_sdt=2005&amp;amp;sciodt=0,5&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;works that cite the ETH paper&lt;/a&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Seems to be fixed in 
&lt;a href=&#34;https://github.com/chengxuxin/extreme-parkour/blob/d2ffe27ba59a3229fad22a9fc94c38010bb1f519/legged_gym/legged_gym/envs/base/legged_robot_config.py#L120&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Extreme Parkour&lt;/a&gt; (clip action 1.2)&lt;/li&gt;
&lt;li&gt;Almost fixed in 
&lt;a href=&#34;https://github.com/Improbable-AI/walk-these-ways/blob/0e7236bdc81ce855cbe3d70345a7899452bdeb1c/scripts/train.py#L200&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Walk this way&lt;/a&gt; (clip action 10)&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- 
Related:
- [Parallel Q Learning (PQL)](https://github.com/Improbable-AI/pql) but only tackles classic MuJoCo locomotion envs --&gt;
&lt;h3 id=&#34;appendix---note-on-unbounded-action-spaces&#34;&gt;Appendix - Note on Unbounded Action Spaces&lt;/h3&gt;
&lt;p&gt;While discussing this blog post with 
&lt;a href=&#34;https://github.com/nico-bohlinger&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Nico Bohlinger&lt;/a&gt;, he raised another point that could explain why people might choose unbounded action space.&lt;/p&gt;
&lt;p&gt;In short, policies can learn to produce actions outside the joint limits to trick the underlying 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Proportional%E2%80%93integral%E2%80%93derivative_controller&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PD controller&lt;/a&gt; into outputting desired torques.
For example, when recovering from a strong push, what matters is not to accurately track a desired position, but to quickly move the joints in the right direction.
This makes training almost invariant to the chosen PD gains.&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;Full quote&lt;/summary&gt;
&lt;blockquote&gt;
&lt;p&gt;So in theory you could clip [the actor output] to the min and max ranges of the joints, but what happens quite often is that these policies learn to produce actions that sets the target joint position outside of the joint limits.
This happens because the policies don&amp;rsquo;t care about the tracking accuracy of the underlying 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Proportional%E2%80%93integral%E2%80%93derivative_controller&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PD controller&lt;/a&gt;, they just want to command: in which direction should the joint angle change, and by how much.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;In control, the magnitude is done through the P and D gains, but we fix them during training, so when the policy wants to move the joints in a certain direction very quickly (especially needed during recovery of strong pushes or strong domain randomization in general), it learns to command actions that are far away to move into this direction quickly, i.e. to produce more torque.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;It essentially learns to trick the PD control to output whatever torques it needs. Of course, this also depends on the PD gains you set; if they are well chosen, actions outside of the joint limits are less frequent. A big benefit is that this makes the whole training pipeline quite invariant to the PD gains you choose at the start, which makes tuning easier.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/details&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@article{raffin2025isaacsim,
  title   = &amp;quot;Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms&amp;quot;,
  author  = &amp;quot;Raffin, Antonin&amp;quot;,
  journal = &amp;quot;araffin.github.io&amp;quot;,
  year    = &amp;quot;2025&amp;quot;,
  month   = &amp;quot;Feb&amp;quot;,
  url     = &amp;quot;https://araffin.github.io/post/sac-massive-sim/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;I would like to thank Anssi, Leon, Ria and Costa for their feedback =).&lt;/p&gt;
&lt;!-- All the graphics were made using [excalidraw](https://excalidraw.com/). --&gt;
&lt;h3 id=&#34;did-you-find-this-post-helpful-consider-sharing-it-&#34;&gt;Did you find this post helpful? Consider sharing it ð&lt;/h3&gt;
&lt;h2 id=&#34;footnotes&#34;&gt;Footnotes&lt;/h2&gt;
&lt;div class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;
&lt;p&gt;Rudin, Nikita, et al. 
&lt;a href=&#34;https://arxiv.org/abs/2109.11978&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Learning to walk in minutes using massively parallel deep reinforcement learning.&amp;rdquo;&lt;/a&gt; Conference on Robot Learning. PMLR, 2022.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;
&lt;p&gt;Like the 
&lt;a href=&#34;https://www.youtube.com/watch?v=7_LW7u-nk6Q&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BD-1 Disney robot&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34;&gt;
&lt;p&gt;I was not the only one asking why SAC doesn&amp;rsquo;t work: 
&lt;a href=&#34;https://forums.developer.nvidia.com/t/poor-performance-of-soft-actor-critic-sac-in-omniverseisaacgym/266970&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nvidia forum&lt;/a&gt; 
&lt;a href=&#34;https://www.reddit.com/r/reinforcementlearning/comments/lcx0cm/scaling_up_sac_with_parallel_environments/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reddit1&lt;/a&gt; 
&lt;a href=&#34;https://www.reddit.com/r/reinforcementlearning/comments/12h1faq/isaac_gym_with_offpolicy_algorithms&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;reddit2&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34;&gt;
&lt;p&gt;Berner C, Brockman G, Chan B, Cheung V, DÄbiak P, Dennison C, Farhi D, Fischer Q, Hashme S, Hesse C, JÃ³zefowicz R. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680. 2019 Dec 13.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34;&gt;
&lt;p&gt;See results from Huang, Shengyi, et al. &amp;ldquo;
&lt;a href=&#34;https://wandb.ai/openrlbenchmark/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open rl benchmark&lt;/a&gt;: Comprehensive tracked experiments for reinforcement learning.&amp;rdquo; arXiv preprint arXiv:2402.03046 (2024).&amp;#160;&lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:6&#34;&gt;
&lt;p&gt;Yes, we tend to be lazy.&amp;#160;&lt;a href=&#34;#fnref:6&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:7&#34;&gt;
&lt;p&gt;The control loop runs at 
&lt;a href=&#34;https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py#L302&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;50 Hz&lt;/a&gt;, so after 20s.&amp;#160;&lt;a href=&#34;#fnref:7&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:8&#34;&gt;
&lt;p&gt;This is not true for the PPO implementation in Brax which uses a squashed Gaussian like SAC.&amp;#160;&lt;a href=&#34;#fnref:8&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:9&#34;&gt;
&lt;p&gt;The code to record and plot action distribution is on 
&lt;a href=&#34;https://gist.github.com/araffin/e069945a68aa0d51fcdff3f01e945c70&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:9&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:10&#34;&gt;
&lt;p&gt;Action spaces that are too small are also problematic. See 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SB3 RL Tips and Tricks&lt;/a&gt;.&amp;#160;&lt;a href=&#34;#fnref:10&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:11&#34;&gt;
&lt;p&gt;Rescale from [-1, 1] to [low, high] using &lt;code&gt;action = low + (0.5 * (scaled_action + 1.0) * (high - low))&lt;/code&gt;.&amp;#160;&lt;a href=&#34;#fnref:11&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:12&#34;&gt;
&lt;p&gt;Common PPO implementations clip the actions to fit the desired boundaries, which has the effect of oversampling actions at the boundaries when the limits are smaller than ~4.&amp;#160;&lt;a href=&#34;#fnref:12&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:13&#34;&gt;
&lt;p&gt;The entropy coeff is the coeff that does the trade-off between RL objective and entropy maximization.&amp;#160;&lt;a href=&#34;#fnref:13&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:14&#34;&gt;
&lt;p&gt;A notable exception are Brax-based environments because their PPO implementation uses a squashed Gaussian, so the boundaries of the environments had to be properly defined.&amp;#160;&lt;a href=&#34;#fnref:14&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Automatic Hyperparameter Tuning - A Visual Guide (Part 1)</title>
      <link>/post/hyperparam-tuning/</link>
      <pubDate>Mon, 15 May 2023 00:00:00 +0000</pubDate>
      <guid>/post/hyperparam-tuning/</guid>
      <description>&lt;p&gt;When you&amp;rsquo;re building a machine learning model, you want to find the best hyperparameters to make it shine. But who has the luxury of trying out every possible combination?&lt;/p&gt;
&lt;p&gt;The good news is that automatic hyperparameter tuning can help you.
The trick is to allocate your &amp;ldquo;budget&amp;rdquo; (aka time and resources) wisely. You want to try out as many combinations as possible, but you don&amp;rsquo;t have an infinite amount of time.
By pruning the bad trials early and focusing on the promising ones, you can find the best hyperparameters quickly and efficiently.&lt;/p&gt;
&lt;p&gt;As a personal and concrete example, I used this technique on a 
&lt;a href=&#34;https://arxiv.org/abs/2209.07171&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;real elastic quadruped&lt;/a&gt; to optimize the parameters of a controller directly on the real robot (it can also be good 
&lt;a href=&#34;https://arxiv.org/abs/2310.05808&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;baseline&lt;/a&gt; for locomotion).&lt;/p&gt;
&lt;p&gt;In this blog post, I&amp;rsquo;ll explore some of the techniques for automatic hyperparameter tuning, using reinforcement learning as a concrete example.
I&amp;rsquo;ll discuss the challenges of hyperparameter optimization, and introduce different samplers and schedulers for exploring the hyperparameter space.

&lt;a href=&#34;../optuna/&#34;&gt;Part two&lt;/a&gt; shows how to use the 
&lt;a href=&#34;https://github.com/optuna/optuna&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optuna library&lt;/a&gt; to put these techniques into practice.&lt;/p&gt;
&lt;p&gt;If you prefer to learn with video, I gave this tutorial at ICRA 2022.
The 
&lt;a href=&#34;https://araffin.github.io/tools-for-robotic-rl-icra2022/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;, notebooks and videos are online:&lt;/p&gt;


    
    &lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
      &lt;iframe allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&#34; allowfullscreen=&#34;allowfullscreen&#34; loading=&#34;eager&#34; referrerpolicy=&#34;strict-origin-when-cross-origin&#34; src=&#34;https://www.youtube.com/embed/AidFTOdGNFQ?autoplay=0&amp;controls=1&amp;end=0&amp;loop=0&amp;mute=0&amp;start=0&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; title=&#34;YouTube video&#34;
      &gt;&lt;/iframe&gt;
    &lt;/div&gt;

&lt;div style=&#34;margin-top: 50px&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;hyperparameter-optimization-the-n-vs-bn-tradeoff&#34;&gt;Hyperparameter Optimization: The &amp;ldquo;n vs B/n&amp;rdquo; tradeoff&lt;/h2&gt;
&lt;p&gt;When you do hyperparameter tuning, you want to try a bunch of configurations &amp;ldquo;n&amp;rdquo; on a given problem.
Depending on how each trial goes, you may decide to continue or stop it early.&lt;/p&gt;
&lt;p&gt;The tradeoff you have is that you want to try as many configurations (aka sets of hyperparameters) as possible, but you don&amp;rsquo;t have an infinite budget (B).
So you have to allocate the budget you give to each configuration wisely (B/n, budget per configuration).&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./img/successive_halving_comment.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;As shown in the figure above, one way to achieve this goal is to start by giving all trials the same budget.
After some time, say 25% of the total budget, you decide to prune the least promising trials and allocate more resources to the most promising ones.&lt;/p&gt;
&lt;p&gt;You can repeat this process several times (here at 50% and 75% of the maximum budget) until you reach the budget limit.&lt;/p&gt;
&lt;p&gt;The two main components of hyperparameter tuning deal with this tradeoff:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the sampler (or search algorithm) decides which configuration to try&lt;/li&gt;
&lt;li&gt;the pruner (or scheduler) decides how to allocate the computational budget and when to stop a trial&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;samplers&#34;&gt;Samplers&lt;/h2&gt;
&lt;p&gt;So how do you sample configurations, how do you choose which set of parameters to try?&lt;/p&gt;
&lt;h3 id=&#34;the-performance-landscape&#34;&gt;The Performance Landscape&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s take a simple 2D example to illustrate the high-level idea.&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./img/perf_landscape.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;In this example, we want to obtain high returns (red area).
The performance depends on two parameters that we can tune.&lt;/p&gt;
&lt;p&gt;Of course, if we knew the performance landscape in advance, we wouldn&amp;rsquo;t need any tuning, we could directly choose the optimal parameters for the task.&lt;/p&gt;
&lt;p&gt;In this particular example, you can notice that one parameter must be tuned precisely (parameter 1), while the second one can be chosen more loosely (it doesn&amp;rsquo;t impact performance much). Again, you don&amp;rsquo;t know this in advance.&lt;/p&gt;
&lt;h3 id=&#34;grid-search&#34;&gt;Grid Search&lt;/h3&gt;
&lt;p&gt;A common and inefficient way to sample hyperparameters is to discretize the search space and try all configurations: this is called grid search.&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./img/grid_search_comb.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;Grid search is simple but should be avoided.
As shown in the image above, you have to be very careful when discretizing the space:
if you are unlucky, you might completely miss the optimal parameters (the high return region in red is not part of the sampled parameters).&lt;/p&gt;
&lt;p&gt;You can have a finer discretization, but then the number of configurations will grow rapidly.
Grid search also scales very poorly with dimensions: the number of configurations you have to try grows exponentially!&lt;/p&gt;
&lt;p&gt;Finally, you may have noticed that grid search wastes resources: it allocates the same budget to important and unimportant parameters.&lt;/p&gt;
&lt;p&gt;A better but still simpler alternative to grid search is 
&lt;a href=&#34;https://www.jmlr.org/papers/v13/bergstra12a.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;random search&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;random-search&#34;&gt;Random Search&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://papers.nips.cc/paper/2011/hash/86e8f7ab32cfd12577bc2619bc635690-Abstract.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Random search&lt;/a&gt; samples the search space uniformly.&lt;/p&gt;
&lt;p&gt;It may seem counterintuitive at first that random search is better than grid search, but hopefully the diagram below will be of some help:&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./img/grid_vs_rs.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;By sampling uniformly, random search no longer depends on the discretization, making it a better starting point.
This is especially true once you have more dimensions.&lt;/p&gt;
&lt;p&gt;Of course, random search is pretty naive, so can we do better?&lt;/p&gt;
&lt;h3 id=&#34;bayesian-optimization&#34;&gt;Bayesian Optimization&lt;/h3&gt;
&lt;p&gt;One of the main ideas of 
&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-05318-5_1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bayesian Optimization&lt;/a&gt; (BO) is to learn a surrogate model that estimates, with some uncertainty, the performance of a configuration (before trying it).
In the figure below, this is the solid black line.&lt;/p&gt;
&lt;p&gt;It tries to approximate the real (unknown) objective function (dotted line).
The surrogate model comes with some uncertainty (blue area), which allows you to choose which configuration to try next.&lt;/p&gt;
&lt;p&gt;&lt;object style=&#34;margin: auto; display: block;&#34; width=&#34;60%&#34; type=&#34;image/svg+xml&#34; data=&#34;./img/bayesian_optim.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;A BO algorithm works in three steps. First, you have a current estimate of the objective function, which comes from your previous observations (configurations that have been tried).
Around these observations, the uncertainty of the surrogate model will be small.&lt;/p&gt;
&lt;p&gt;To select the next configuration to sample, BO relies on an acquisition function. This function takes into account the value of the surrogate model and the uncertainty.&lt;/p&gt;
&lt;p&gt;Here the acquisition function samples the most optimistic set of parameters given the current model (maximum of surrogate model value + uncertainty): you want to sample the point that might give you the best performance.&lt;/p&gt;
&lt;p&gt;Once you have tried this configuration, the surrogate model and acquisition function are updated with the new observation (the uncertainty around this new observation decreases), and a new iteration begins.&lt;/p&gt;
&lt;p&gt;In this example, you can see that the sampler quickly converges to a value that is close to the optimum.&lt;/p&gt;
&lt;p&gt;Gaussian Process (GP) and 
&lt;a href=&#34;https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Tree of Parzen Estimators&lt;/a&gt; (TPE) algorithms both use this technique to optimize hyperparameters.&lt;/p&gt;
&lt;h3 id=&#34;other-black-box-optimization-bbo-algorithms&#34;&gt;Other Black Box Optimization (BBO) Algorithms&lt;/h3&gt;
&lt;p&gt;I won&amp;rsquo;t cover them in detail but you should also know about two additional classes of black box optimization (BBO) algorithms: 
&lt;a href=&#34;https://blog.otoro.net/2017/10/29/visual-evolution-strategies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Evolution Strategies&lt;/a&gt; (ES, CMA-ES) and 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Particle_swarm_optimization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Particle Swarm Optimization&lt;/a&gt; (PSO).
Both of those approaches optimize a population of solutions that evolves over time.&lt;/p&gt;
&lt;h2 id=&#34;schedulers--pruners&#34;&gt;Schedulers / Pruners&lt;/h2&gt;
&lt;p&gt;The job of the pruner is to identify and discard poorly performing hyperparameter configurations, eliminating them from further consideration.
This ensures that your resources are focused on the most promising candidates, saving valuable time and computating power.&lt;/p&gt;
&lt;p&gt;Deciding when to prune a trial can be tricky.
If you don&amp;rsquo;t allocate enough resources to a trial, you won&amp;rsquo;t be able to judge whether it&amp;rsquo;s a good trial or not.&lt;/p&gt;
&lt;p&gt;If you prune too aggressively, you will favor the candidates that perform well early (and then plateau) to the detriment of those that perform better with more budget.&lt;/p&gt;
&lt;h3 id=&#34;median-pruner&#34;&gt;Median Pruner&lt;/h3&gt;
&lt;p&gt;A simple but effective scheduler is the 
&lt;a href=&#34;https://optuna.readthedocs.io/en/stable/reference/generated/optuna.pruners.MedianPruner.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;median pruner&lt;/a&gt;, used in 
&lt;a href=&#34;https://research.google/pubs/pub46180/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Vizier&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The idea is to prune if the intermediate result of the trial is worse than the median of the intermediate results of previous trials at the same step.
In other words, at a given time, you look at the current candidate.
If it performs worse than half of the candidates at the same time, you stop it, otherwise you let it continue.&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./img/median_pruner.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;To avoid biasing the optimization toward candidates that perform well early in training, you can play with a &amp;ldquo;warmup&amp;rdquo; parameter that prevents any trial from being pruned until a minimum budget is reached.&lt;/p&gt;
&lt;h3 id=&#34;successive-halving&#34;&gt;Successive Halving&lt;/h3&gt;
&lt;p&gt;Successive halving is a slightly more advanced algorithm.
You start with many configurations and give them all a minimum budget.&lt;/p&gt;
&lt;p&gt;Then, at some intermediate step, you reduce the number of candidates and keep only the most promising ones.&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./img/successive_halving_comment.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;One limitation with this algorithm is that it has three hyperparameters (to be tuned :p!): the minimum budget, the initial number of trials and the reduction factor (what percentage of trials are discarded at each intermediate step).&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s where the 
&lt;a href=&#34;https://arxiv.org/abs/1603.06560&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hyperband&lt;/a&gt; algorithm comes in (I highly recommend reading the paper). Hyperband does a grid search on the successive halving parameters (in parallel) and thus tries different tradeoffs (remember the &amp;ldquo;n&amp;rdquo; vs. &amp;ldquo;n/B&amp;rdquo; tradeoff ;)?).&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, I introduced the challenges and basic components of automatic hyperparameter tuning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the trade-off between the number of trials and the resources allocated per trial&lt;/li&gt;
&lt;li&gt;the different samplers that choose which set of parameters to try&lt;/li&gt;
&lt;li&gt;the various schedulers that decide how to allocate resources and when to stop a trial&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The 
&lt;a href=&#34;../optuna/&#34;&gt;second part&lt;/a&gt; is about applying hyperparameter tuning in practice with the 
&lt;a href=&#34;https://github.com/optuna/optuna&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optuna&lt;/a&gt; library, using reinforcement learning as an example.&lt;/p&gt;
&lt;h2 id=&#34;citation&#34;&gt;Citation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;@article{raffin2023hyperparameter,
  title   = &amp;quot;Automatic Hyperparameter Tuning - A Visual Guide&amp;quot;,
  author  = &amp;quot;Raffin, Antonin&amp;quot;,
  journal = &amp;quot;araffin.github.io&amp;quot;,
  year    = &amp;quot;2023&amp;quot;,
  month   = &amp;quot;May&amp;quot;,
  url     = &amp;quot;https://araffin.github.io/post/hyperparam-tuning/&amp;quot;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;All the graphics were made using 
&lt;a href=&#34;https://excalidraw.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;excalidraw&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;did-you-find-this-post-helpful-consider-sharing-it-&#34;&gt;Did you find this post helpful? Consider sharing it ð&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Rliable: Better Evaluation for Reinforcement Learning - A Visual Explanation</title>
      <link>/post/rliable/</link>
      <pubDate>Fri, 29 Oct 2021 00:00:00 +0000</pubDate>
      <guid>/post/rliable/</guid>
      <description>&lt;p&gt;It is critical for Reinforcement Learning (RL) practitioners to properly evaluate and compare results.
Reporting results with poor comparison leads to a progress mirage and may underestimate the stochasticity of the results. To this end, 
&lt;a href=&#34;https://arxiv.org/abs/2108.13264&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep RL at the Edge of the Statistical Precipice&lt;/a&gt; (Neurips Oral) provides recommendations for a more rigorous evaluation of DeepRL algorithms. The paper comes with an open-source library named 
&lt;a href=&#34;https://github.com/google-research/rliable&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rliable&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This blog post is meant to be a visual explanation of the tools used by the 
&lt;a href=&#34;https://agarwl.github.io/rliable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rliable&lt;/a&gt; library to better evaluate and compare RL algorithms.
We will go through the different recommendations of the authors and give a visual explanation for each of them.&lt;/p&gt;
&lt;h2 id=&#34;score-normalization&#34;&gt;Score Normalization&lt;/h2&gt;
&lt;p&gt;To have more datapoints that just 10 random seeds, rliable recommends aggregating all N runs across all M tasks (e.g., aggregating all Atari games results) so we have a total of NxM runs from which we can sample from. To have comparable scores across tasks, we first need to normalize the scores of each run per task as follows:&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./score_norm.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;Note: the score may depend on what you want to compare. It is usually the final performance of the RL agent, after training.&lt;/p&gt;
&lt;h2 id=&#34;stratified-bootstrap-confidence-intervals&#34;&gt;Stratified Bootstrap Confidence Intervals&lt;/h2&gt;
&lt;p&gt;To account for uncertainty in aggregate performance, rliable uses stratified bootstrap confidence intervals.
This may sound complicated, but let&amp;rsquo;s go slowly through the meaning of each of those terms.&lt;/p&gt;
&lt;p&gt;First, bootstrap means sampling with replacement. For instance, if we sample four times with replacement 3 runs of indices [1, 2, 3] on a task A, we may get: [2, 2, 3, 1] the first time, [3, 1, 1, 1] the second time, &amp;hellip;&lt;/p&gt;
&lt;p&gt;Stratified bootstrap means that we first group our datapoints into buckets (or strata), and then sample with replacement each of those buckets according to their size:&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./stratified_bootstrap.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;In RL, the buckets are the different tasks or environments. With stratified bootstrap, all tasks are always represented in the sampled runs. This avoids computing the aggregate metrics only on a subset of all the environments:&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./bootstrap_rl.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;Each time we sample with replacement the runs, we compute the different metrics (for instance, mean score) for those sampled runs. To report uncertainty, rliable computes 
&lt;a href=&#34;https://acclab.github.io/bootstrap-confidence-intervals.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;bootstrap confidence intervals&lt;/a&gt; (CIs) following the 
&lt;a href=&#34;https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading24.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;percentiles&amp;rsquo; method&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./bootstrap_ci.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;Note: there are other methods for computing CI with bootstrap, but percentiles was found by the authors to work well in practice.&lt;/p&gt;
&lt;h2 id=&#34;interquartile-mean-iqm&#34;&gt;Interquartile Mean (IQM)&lt;/h2&gt;
&lt;p&gt;To summarize benchmark performance, it is common to report mean/median performance of the runs.
However, mean is known to be sensible to outliers and median may not reflect enough the distribution of scores, so rliable suggests to use Interquartile Mean (IQM) instead:&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./iqm.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;h2 id=&#34;performance-profiles&#34;&gt;Performance Profiles&lt;/h2&gt;
&lt;p&gt;To report performance variability across tasks and runs, the authors proposes to use performance profiles.
It tells for a given target performance (for example, 60% of the reference performance) the proportion of runs that achieve it.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Performance Profile&#34; src=&#34;/post/rliable/perf_profile.jpg&#34;&gt;
Source: image from the authors of the rliable library&lt;/p&gt;
&lt;h2 id=&#34;probability-of-improvement&#34;&gt;Probability of Improvement&lt;/h2&gt;
&lt;p&gt;Finally, to test whether an algorithm X is probably better or not than an algorithm Y, rliable uses the U-statistic from a 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MannâWhitney U test&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;object width=&#34;100%&#34; type=&#34;image/svg+xml&#34; data=&#34;./proba_improvement.svg&#34;&gt;&lt;/object&gt;&lt;/p&gt;
&lt;p&gt;The probability of improvement is then average over the tasks.
A probability of improvement around 0.5 means that the two algorithms have similar performances.&lt;/p&gt;
&lt;h2 id=&#34;in-practice-using-the-rl-zoo&#34;&gt;In Practice: Using the RL Zoo&lt;/h2&gt;
&lt;p&gt;To allow more users to use rliable, we added basic support of it in the 
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo#plot-with-the-rliable-library&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL Baselines3 Zoo&lt;/a&gt;, a training framework for 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable-Baselines3&lt;/a&gt;. Fore more information, please follow the instructions in the 
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo#plot-with-the-rliable-library&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;README&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;In this post, we have seen the different tools used by rliable to better evaluate RL algorithms:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;score normalization to aggregate scores across tasks&lt;/li&gt;
&lt;li&gt;stratified bootstrap to provide proper confidence intervals&lt;/li&gt;
&lt;li&gt;interquartile mean (IQM) to summarize benchmark performance&lt;/li&gt;
&lt;li&gt;performance profile for an overview of the results and their variability&lt;/li&gt;
&lt;li&gt;probability of improvement to compare two algorithms&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;acknowledgement&#34;&gt;Acknowledgement&lt;/h2&gt;
&lt;p&gt;I would like to thank 
&lt;a href=&#34;https://psc-g.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Pablo Samuel Castro&lt;/a&gt; and 
&lt;a href=&#34;https://agarwl.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rishabh Agarwal&lt;/a&gt; for checking the correctness of the visuals.&lt;/p&gt;
&lt;p&gt;All the graphics were made using 
&lt;a href=&#34;https://excalidraw.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;excalidraw&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;did-you-find-this-post-helpful-consider-sharing-it-&#34;&gt;Did you find this post helpful? Consider sharing it ð&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Stable-Baselines3: Reliable Reinforcement Learning Implementations</title>
      <link>/post/sb3/</link>
      <pubDate>Sun, 28 Feb 2021 00:00:00 +0000</pubDate>
      <guid>/post/sb3/</guid>
      <description>&lt;p&gt;After several months of beta, we are happy to announce the release of 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable-Baselines3 (SB3)&lt;/a&gt; v1.0, a set of reliable implementations of reinforcement learning (RL) algorithms in PyTorch =D! It is the next major version of 
&lt;a href=&#34;https://github.com/hill-a/stable-baselines&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable Baselines&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The implementations have been 
&lt;a href=&#34;https://arxiv.org/abs/2005.05719&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;benchmarked&lt;/a&gt; against reference codebases, and automated unit tests cover 95% of the code.&lt;/p&gt;
&lt;p&gt;In this blog post, we give you an overview of Stable-Baselines3: the motivation behind it, its design principles and features, how we ensure high-quality implementations and some concrete examples.&lt;/p&gt;
&lt;!-- The algorithms follow a consistent interface and are accompanied by extensive documentation, making it simple to train and compare different RL algorithms. --&gt;
&lt;h2 id=&#34;tldr&#34;&gt;TL;DR:&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable-Baselines3 (SB3)&lt;/a&gt; is a library providing &lt;em&gt;reliable&lt;/em&gt; implementations of reinforcement learning algorithms in PyTorch. It provides a &lt;em&gt;clean and simple interface&lt;/em&gt;, giving you access to off-the-shelf state-of-the-art model-free RL algorithms.&lt;/p&gt;
&lt;p&gt;The library is &lt;em&gt;
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;fully documented&lt;/a&gt;&lt;/em&gt;, tested and its interface allows to train an RL agent in only few lines of code =):&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gym
from stable_baselines3 import SAC
# Train an agent using Soft Actor-Critic on Pendulum-v0
env = gym.make(&amp;quot;Pendulum-v0&amp;quot;)
model = SAC(&amp;quot;MlpPolicy&amp;quot;, env, verbose=1)
# Train the model
model.learn(total_timesteps=20000)
# Save the model
model.save(&amp;quot;sac_pendulum&amp;quot;)
# Load the trained model
model = SAC.load(&amp;quot;sac_pendulum&amp;quot;)
# Start a new episode
obs = env.reset()
# What action to take in state `obs`?
action, _ = model.predict(obs, deterministic=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where defining and training a RL agent can be written in two lines of code:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_baselines3 import PPO
# Train an agent using Proximal Policy Optimization on CartPole-v1
model = PPO(&amp;quot;MlpPolicy&amp;quot;, &amp;quot;CartPole-v1&amp;quot;).learn(total_timesteps=20000)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;links&#34;&gt;Links&lt;/h2&gt;
&lt;p&gt;GitHub repository: 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/DLR-RM/stable-baselines3&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Paper: 
&lt;a href=&#34;http://jmlr.org/papers/v22/20-1364.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://jmlr.org/papers/v22/20-1364.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Documentation: 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://stable-baselines3.readthedocs.io/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RL Baselines3 Zoo: 
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/DLR-RM/rl-baselines3-zoo&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Contrib: 
&lt;a href=&#34;https://github.com/Stable-Baselines-Team/stable-baselines3-contrib&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/Stable-Baselines-Team/stable-baselines3-contrib&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;RL Tutorial: 
&lt;a href=&#34;https://github.com/araffin/rl-tutorial-jnrr19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/araffin/rl-tutorial-jnrr19&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Deep reinforcement learning (RL) research has grown rapidly in recent years, yet results are often 
&lt;a href=&#34;https://arxiv.org/abs/1709.06560&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;difficult to reproduce&lt;/a&gt;.
A major challenge is that small implementation details can have a substantial effect on performance &amp;ndash; often greater than the 
&lt;a href=&#34;https://iclr.cc/virtual_2020/poster_r1etN1rtPB.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;difference between algorithms&lt;/a&gt;.
It is particularly important that implementations used as experimental &lt;em&gt;baselines&lt;/em&gt; are reliable; otherwise, novel algorithms compared to weak baselines lead to inflated estimates of performance improvements.&lt;/p&gt;
&lt;p&gt;To help with this problem, we present Stable-Baselines3 (SB3), an open-source framework implementing seven commonly used model-free deep RL algorithms, relying on the 
&lt;a href=&#34;https://github.com/openai/gym&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI Gym interface&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We take great care to adhere to software engineering best practices to achieve high-quality implementations that match prior results.&lt;/p&gt;
&lt;h2 id=&#34;history&#34;&gt;History&lt;/h2&gt;
&lt;p&gt;SB3 builds on our experience maintaining &lt;em&gt;
&lt;a href=&#34;https://github.com/hill-a/stable-baselines&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stable Baselines&lt;/a&gt;&lt;/em&gt; (SB2), a fork of OpenAI Baselines built on TensorFlow 1.x.
If you haven&amp;rsquo;t heard of it, Stable-Baselines (SB2) is a trusted library and has already been used in &lt;em&gt;many 
&lt;a href=&#34;https://stable-baselines.readthedocs.io/en/master/misc/projects.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;projects&lt;/a&gt;&lt;/em&gt; and &lt;em&gt;
&lt;a href=&#34;https://scholar.google.fr/scholar?oi=bibs&amp;amp;hl=fr&amp;amp;cites=7029285800852969820&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;papers&lt;/a&gt;&lt;/em&gt; with already more than 300+ citations!&lt;/p&gt;
&lt;p&gt;Those two years of maintaining SB2 have been a rewarding exchange with our users, where tons of bugs where fixed and new features like callbacks where added to ease the use of the library.&lt;/p&gt;
&lt;p&gt;However, SB2 was still relying on OpenAI Baselines initial codebase and with the upcoming release of Tensorflow 2, more and more internal TF code was being deprecated.&lt;/p&gt;
&lt;p&gt;After discussing the matter with the community, we decided to go for a complete rewrite in PyTorch (cf issues 
&lt;a href=&#34;https://github.com/hill-a/stable-baselines/issues/366&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#366&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/hill-a/stable-baselines/issues/576&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#576&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/hill-a/stable-baselines/issues/733&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#733&lt;/a&gt;), codename: Stable-Baselines3&lt;sup&gt;1&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Stable-Baselines3 keeps the same easy-to-use API while improving a lot on the internal code, in particular by adding static type checking.&lt;/p&gt;
&lt;p&gt;Re-starting almost from scratch is long-term investment: it took 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;quite some effort&lt;/a&gt; and time but we now have a smaller, cleaner and reliable core that is easier to maintain and extend =).&lt;/p&gt;
&lt;p&gt;There are already 
&lt;a href=&#34;https://github.com/search?p=1&amp;amp;q=reinforcement&amp;#43;learning&amp;#43;library&amp;amp;type=Repositories&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;many&lt;/a&gt; open source reinforcement learning libraries (almost one new every week), so why did we create a new one? In the next sections you will learn about the design principles and main features of the Stable-Baselines3 library that differenciate it from others.&lt;/p&gt;
&lt;p&gt;&lt;sup&gt;1&lt;/sup&gt; The very first name of the new version was &amp;ldquo;torchy-baselines&amp;rdquo;&lt;/p&gt;
&lt;h2 id=&#34;design-principles&#34;&gt;Design Principles&lt;/h2&gt;
&lt;p&gt;Our main goal is to provide a user-friendly and reliable RL library.
To keep SB3 simple to use and maintain, we focus on model-free, single-agent RL algorithms, and rely on external projects to extend the scope to 
&lt;a href=&#34;https://github.com/HumanCompatibleAI/imitation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;imitation&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/takuseno/d3rlpy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;offline&lt;/a&gt; learning.&lt;/p&gt;
&lt;p&gt;We prioritize maintaining &lt;em&gt;stable&lt;/em&gt; implementations over adding new features or algorithms, and avoid making breaking changes.
We provide a consistent, clean and fully documented API, inspired by the 
&lt;a href=&#34;https://scikit-learn.org/stable/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;scikit-learn&lt;/a&gt; API.&lt;/p&gt;
&lt;p&gt;Our code is 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/developer.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;easily modifiable&lt;/a&gt; by users as we favour readability and simplicity over modularity, although we make use of object-oriented programming to reduce code duplication.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;p&gt;Stable-Baselines3 provides many features, ranging from a simple API to a complete 
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;experimental framework&lt;/a&gt; that allows advanced usage like automatic hyperparameters tuning.&lt;/p&gt;
&lt;h3 id=&#34;simple-api&#34;&gt;Simple API&lt;/h3&gt;
&lt;p&gt;Training agents in Stable-Baselines3 takes just a few lines of code, after which the agent can be queried for actions (see quick example below).
This allows you to easily use the baseline algorithms and components in your experiments (eg. 
&lt;a href=&#34;https://xbpeng.github.io/projects/Robotic_Imitation/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Imitating Animals&lt;/a&gt;, 
&lt;a href=&#34;https://github.com/hardmaru/slimevolleygym&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slime Volleyball&lt;/a&gt;, 
&lt;a href=&#34;https://adversarialpolicies.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adversarial Policies&lt;/a&gt;), as well as apply RL to novel tasks and environments, like 
&lt;a href=&#34;https://pwnagotchi.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;continual learning&lt;/a&gt; when attacking WiFi networks or 
&lt;a href=&#34;https://github.com/jaberkow/WaveRL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;dampening bridge vibrations&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import gym

from stable_baselines3 import A2C
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback

# Save a checkpoint every 1000 steps
checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=&amp;quot;./logs/&amp;quot;,
                                         name_prefix=&amp;quot;rl_model&amp;quot;)

# Evaluate the model periodically
# and auto-save the best model and evaluations
# Use a monitor wrapper to properly report episode stats
eval_env = Monitor(gym.make(&amp;quot;LunarLander-v2&amp;quot;))
# Use deterministic actions for evaluation
eval_callback = EvalCallback(eval_env, best_model_save_path=&amp;quot;./logs/&amp;quot;,
                             log_path=&amp;quot;./logs/&amp;quot;, eval_freq=2000,
                             deterministic=True, render=False)

# Train an agent using A2C on LunarLander-v2
model = A2C(&amp;quot;MlpPolicy&amp;quot;, &amp;quot;LunarLander-v2&amp;quot;, verbose=1)
model.learn(total_timesteps=20000, callback=[checkpoint_callback, eval_callback])

# Retrieve and reset the environment
env = model.get_env()
obs = env.reset()

# Query the agent (stochastic action here)
action, _ = model.predict(obs, deterministic=False)

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;documentation&#34;&gt;Documentation&lt;/h3&gt;
&lt;p&gt;SB3 comes with 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;extensive documentation&lt;/a&gt; of the code API.
We also include a user guide, covering both basic and more advanced usage with a collection of concrete examples.
Moreover, we have developed a 
&lt;a href=&#34;https://github.com/araffin/rl-tutorial-jnrr19&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab notebook based RL tutorial&lt;/a&gt;, so you can demo the library directly in the browser.
Additionally, we include 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;common tips&lt;/a&gt; for running RL experiments and a 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/developer.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;developer guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We also pay close attention to 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues?q=is%3Aissue&amp;#43;is%3Aopen&amp;#43;label%3Aquestion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;questions&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues?q=is%3Aissue&amp;#43;is%3Aopen&amp;#43;label%3Adocumentation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;uncertainties&lt;/a&gt; from SB3 users, updating the documentation to address these.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Documentation&#34; src=&#34;/post/sb3/doc.png&#34;&gt;
&lt;em&gt;Stable-Baselines3 Documentation&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;high-quality-implementations&#34;&gt;High-Quality Implementations&lt;/h3&gt;
&lt;p&gt;Algorithms are verified against published results by comparing the agent learning curves (cf issues 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues/48&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#48&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues/49&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;#48&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;As an example, to compare against TD3 and SAC original implementation, we integrated SB3 callbacks and made sure both SB3 and original implementations were using the same hyperparameters (the code diff for SAC and TD3 repos can be found 
&lt;a href=&#34;https://github.com/rail-berkeley/softlearning/compare/master...Artemis-Skade:master&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; and 
&lt;a href=&#34;https://github.com/sfujim/TD3/compare/master...araffin:master&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;there&lt;/a&gt;).&lt;/p&gt;
&lt;!-- https://github.com/Artemis-Skade/softlearning --&gt;
&lt;!-- https://github.com/araffin/TD3 --&gt;
&lt;p&gt;During this period, that&amp;rsquo;s how we realized some tricky details that made a big difference.
For example, PyTorch RMSProp is different from TensorFlow one (we include a 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/sb2_compat/rmsprop_tf_like.py&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;custom version&lt;/a&gt; inside our codebase), and the &lt;code&gt;epsilon&lt;/code&gt; value of the optimizer can make a 
&lt;a href=&#34;https://twitter.com/araffin2/status/1329382226421837825&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;big difference&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;A2C&#34; src=&#34;/post/sb3/a2c.png&#34;&gt;
&lt;em&gt;A and B are actually the same RL algorithm (A2C), sharing the exact same code, same hardware, same hyperparameters&amp;hellip; except the epsilon value to avoid division by zero in the optimizer (one is &lt;code&gt;eps=1e-5&lt;/code&gt;, the other &lt;code&gt;eps=1e-7&lt;/code&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Despite all those tricky details (and other 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/issues/105&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;nasty bugs&lt;/a&gt;), at the end, we managed to match SB2 results and original implementations closely:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;A2C&#34; src=&#34;/post/sb3/a2c_comp.png&#34;&gt;
&lt;em&gt;Stable-Baselines (SB2) vs Stable-Baselines3 (SB3) A2C result on CartPole-v1&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;Breakout&#34; src=&#34;/post/sb3/Result_Breakout-1.png&#34;&gt;
&lt;em&gt;Stable-Baselines (SB2) vs Stable-Baselines3 (SB3) results on BreakoutNoFrameskip-v4&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&#34;HalfCheetah&#34; src=&#34;/post/sb3/Result_HalfCheetah-1.png&#34;&gt;
&lt;em&gt;Stable-Baselines3 (SB3) vs original implementations results on HalfCheetahBulletEnv-v0&lt;/em&gt;&lt;/p&gt;
&lt;!-- Moreover, all functions are typed (parameter and return types) and documented with a consistent style, and most functions are covered by unit tests. --&gt;
&lt;!-- Continuous integration checks that all changes pass unit tests and type check, as well as validating the code style and documentation. --&gt;
&lt;h3 id=&#34;comprehensive&#34;&gt;Comprehensive&lt;/h3&gt;
&lt;p&gt;Stable-Baselines3 contains the following state-of-the-art on- and off-policy algorithms, commonly used as experimental baselines: A2C, DDPG, DQN, HER, PPO, SAC and TD3.&lt;/p&gt;
&lt;p&gt;Moreover, SB3 provides various algorithm-independent features. We support logging to CSV files and 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/tensorboard.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TensorBoard&lt;/a&gt;. Users can log custom metrics and modify training via user-provided 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;callbacks&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To speed up training, we support parallel (or &amp;ldquo;vectorized&amp;rdquo;) environments. To simplify training, we implement common environment wrappers, like preprocessing Atari observations to match the original DQN experiments.&lt;/p&gt;
&lt;video controls&gt;
 &lt;source src=&#34;./tb_video.mp4&#34; type=&#34;video/mp4&#34;&gt;
Your browser does not support the video tag.
&lt;/video&gt;
&lt;p&gt;&lt;em&gt;Tensorboard video integration&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;experimental-framework&#34;&gt;Experimental Framework&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL Baselines Zoo&lt;/a&gt; provides scripts to train and evaluate agents, tune hyperparameters, record videos, store experiment setup and visualize results.
We also include a collection of pre-trained reinforcement learning agents together with tuned hyperparameters for simple control tasks, 
&lt;a href=&#34;https://github.com/bulletphysics/bullet3/tree/master/examples/pybullet/gym/pybullet_envs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyBullet&lt;/a&gt; environments and Atari games, optimized using 
&lt;a href=&#34;https://github.com/pfnet/optuna&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optuna&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We follow best practices for training and evaluation, such as evaluating in a separate environment, using deterministic evaluation where required (SAC) and storing all hyperparameters necessary to replicate the experiment.&lt;/p&gt;
&lt;p&gt;Below, you can see basic usage of the RL zoo (training, loading, tuning hyperparameters), which has a simple command line:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;# Train an A2C agent on Atari breakout using tuned hyperparameters,
# evaluate the agent every 10k steps and save a checkpoint every 50k steps
python train.py --algo a2c --env BreakoutNoFrameskip-v4 \
    --eval-freq 10000 --save-freq 50000
# Plot the learning curve
python scripts/all_plots.py -a a2c -e BreakoutNoFrameskip-v4 -f logs/

# Load and evaluate a trained agent for 1000 steps
# optionally, you can also load a checkpoint using --load-checkpoint
python enjoy.py --algo sac --env Pendulum-v0 -n 1000

# Tune the hyperparameters of ppo on BipedalWalker-v3 with a budget of 50 trials
# using 2 parallel jobs, a TPE sampler and median pruner
python train.py --algo ppo --env BipedalWalker-v3 -optimize --n-trials 50 \
    --n-jobs 2 --sampler tpe --pruner median

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;stable-baselines3-contrib&#34;&gt;Stable-Baselines3 Contrib&lt;/h3&gt;
&lt;p&gt;We implement experimental features in a separate 
&lt;a href=&#34;https://github.com/Stable-Baselines-Team/stable-baselines3-contrib&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;contrib repository&lt;/a&gt;.
This allows SB3 to maintain a stable and compact core, while still providing the latest features, like 
&lt;a href=&#34;https://sb3-contrib.readthedocs.io/en/master/modules/qrdqn.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantile Regression DQN (QR-DQN)&lt;/a&gt; or 
&lt;a href=&#34;https://sb3-contrib.readthedocs.io/en/master/modules/tqc.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Truncated Quantile Critics (TQC)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Implementations in contrib need not be tightly integrated with the main SB3 codebase, but we maintain the same stringent review requirements to ensure users can trust the contrib implementations.
Implementations from contrib that have stood the test of time may be integrated into the main repository.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from sb3_contrib import QRDQN, TQC

# Train an agent using QR-DQN on Acrobot-v0
model = QRDQN(&amp;quot;MlpPolicy&amp;quot;, &amp;quot;Acrobot-v0&amp;quot;).learn(total_timesteps=20000)
# Train an agent using Truncated Quantile Critics on Pendulum-v0
model = TQC(&amp;quot;MlpPolicy&amp;quot;, &amp;quot;Pendulum-v0&amp;quot;).learn(total_timesteps=20000)
&lt;/code&gt;&lt;/pre&gt;
&lt;!-- ## What&#39;s new? --&gt;
&lt;h2 id=&#34;migration-from-stable-baselines-sb2&#34;&gt;Migration from Stable-Baselines (SB2)&lt;/h2&gt;
&lt;p&gt;If you are Stable-Baselines (SB2) user and would like to switch to SB3, we have a 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/migration.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;migration guide&lt;/a&gt; waiting for you ;)&lt;/p&gt;
&lt;p&gt;Most of the time, it only requires to change the import &lt;code&gt;from stable_baselines&lt;/code&gt; by &lt;code&gt;from stable_baselines3&lt;/code&gt; and rename some parameters.&lt;/p&gt;
&lt;p&gt;For instance, if your code was like that for Stable-Baselines:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_baselines import PPO2
from stable_baselines.common.cmd_util import make_atari_env

env = make_atari_env(&amp;quot;BreakoutNoFrameskip-v4&amp;quot;, num_env=8, seed=21)

model = PPO2(&amp;quot;MlpPolicy&amp;quot;, env, n_steps=128, nminibatches=4,
              noptepochs=4, ent_coef=0.01, verbose=1)

model.learn(int(1e5))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;the corresponding SB3 code is:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from stable_baselines3 import PPO
# cmd_util was renamed env_util for clarity
from stable_baselines3.common.env_util import make_atari_env

# num_env was renamed n_envs
env = make_atari_env(&amp;quot;BreakoutNoFrameskip-v4&amp;quot;, n_envs=8, seed=21)

# we use batch_size instead of nminibatches which
#Â was dependent on the number of environments
# batch_size = (n_steps * n_envs) // nminibatches = 256
# noptepochs was renamed n_epochs
model = PPO(&amp;quot;MlpPolicy&amp;quot;, env, n_steps=128, batch_size=256,
            n_epochs=4, ent_coef=0.01, verbose=1)

model.learn(int(1e5))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For a complete migration example, you can also compare the 
&lt;a href=&#34;https://github.com/araffin/rl-baselines-zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RL Zoo of SB2&lt;/a&gt; with the 
&lt;a href=&#34;https://github.com/DLR-RM/rl-baselines3-zoo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;one from SB3&lt;/a&gt;.&lt;/p&gt;
&lt;div style=&#34;margin-top:3em&#34;&gt;&lt;/div&gt;
&lt;hr/&gt;
&lt;div style=&#34;margin-top:3em&#34;&gt;&lt;/div&gt;
&lt;h2 id=&#34;examples&#34;&gt;Examples&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s see now how we can now use the library in practice with some examples. We&amp;rsquo;re going to see how to easily customize the network architecture, train an agent to play Atari games and normalize observations when training on continuous control tasks like PyBullet environments.&lt;/p&gt;
&lt;p&gt;For each of them, you can try it online using 
&lt;a href=&#34;https://github.com/Stable-Baselines-Team/rl-colab-notebooks/tree/sb3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google colab notebook&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;custom-policy-network&#34;&gt;Custom Policy Network&lt;/h3&gt;
&lt;p&gt;To
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;customize a policy&lt;/a&gt; with SB3, all you need to do is choose a network architecture and pass a &lt;code&gt;policy_kwargs&lt;/code&gt; (&amp;ldquo;policy keyword arguments&amp;rdquo;) to the algorithm constructor.&lt;/p&gt;
&lt;p&gt;The following snippet shows how to customize the architecture and activation function for one on-policy (PPO) and one off-policy (SAC) algorithm:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import torch as th

from stable_baselines3 import PPO, SAC

# Custom actor (pi) and value function (vf) networks
# of two layers of size 32 each with Relu activation function
policy_kwargs = dict(activation_fn=th.nn.ReLU,
                     net_arch=dict(pi=[32, 32], vf=[32, 32]))
# Create the agent
model = PPO(&amp;quot;MlpPolicy&amp;quot;, &amp;quot;CartPole-v1&amp;quot;, policy_kwargs=policy_kwargs, verbose=1)

# Custom actor architecture with two layers of 64 units each
# Custom critic architecture with two layers of 400 and 300 units
policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[400, 300]))
# Create the agent
model = SAC(&amp;quot;MlpPolicy&amp;quot;, &amp;quot;Pendulum-v0&amp;quot;, policy_kwargs=policy_kwargs, verbose=1)
model.learn(5000)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;atari-games&#34;&gt;Atari Games&lt;/h3&gt;
&lt;p&gt;Training a RL agent on Atari games is straightforward thanks to &lt;code&gt;make_atari_env&lt;/code&gt; helper function and the 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecframestack&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VecFrameStack&lt;/a&gt; wrapper. It will do all the 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;preprocessing&lt;/a&gt; and multiprocessing for you.&lt;/p&gt;
&lt;p&gt;Colab link: 
&lt;a href=&#34;https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/atari_games.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Try it online&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;
from stable_baselines3.common.env_util import make_atari_env
from stable_baselines3.common.vec_env import VecFrameStack
from stable_baselines3 import A2C

# There already exists an environment generator
# that will make and wrap atari environments correctly.
# Here we are also multi-worker training (n_envs=4 =&amp;gt; 4 environments)
env = make_atari_env(&#39;PongNoFrameskip-v4&#39;, n_envs=4, seed=0)
# Frame-stacking with 4 frames
env = VecFrameStack(env, n_stack=4)

model = A2C(&#39;CnnPolicy&#39;, env, verbose=1)
model.learn(total_timesteps=25000)

obs = env.reset()
while True:
    # By default, deterministic=False, so we use the stochastic policy
    action, _states = model.predict(obs)
    obs, rewards, dones, info = env.step(action)
    env.render()
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;pybullet-normalizing-input-features&#34;&gt;PyBullet: Normalizing Input Features&lt;/h3&gt;
&lt;p&gt;Normalizing input features may be essential to successful training of an RL agent (by default, images are scaled but not other types of input), for instance when training on 
&lt;a href=&#34;https://github.com/bulletphysics/bullet3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PyBullet&lt;/a&gt; environments. For that, a wrapper exists and will compute a running average and standard deviation of input features (it can do the same for rewards).&lt;/p&gt;
&lt;p&gt;Colab link: 
&lt;a href=&#34;https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/pybullet.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Try it online&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import os
import gym
import pybullet_envs

from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
from stable_baselines3 import PPO

env = DummyVecEnv([lambda: gym.make(&amp;quot;HalfCheetahBulletEnv-v0&amp;quot;)])
# Automatically normalize the input features and reward
env = VecNormalize(env, norm_obs=True, norm_reward=True,
                   clip_obs=10.)

model = PPO(&#39;MlpPolicy&#39;, env)
model.learn(total_timesteps=2000)

# Don&#39;t forget to save the VecNormalize statistics when saving the agent
log_dir = &amp;quot;/tmp/&amp;quot;
model.save(log_dir + &amp;quot;ppo_halfcheetah&amp;quot;)
stats_path = os.path.join(log_dir, &amp;quot;vec_normalize.pkl&amp;quot;)
env.save(stats_path)

# To demonstrate loading
del model, env

# Load the agent
model = PPO.load(log_dir + &amp;quot;ppo_halfcheetah&amp;quot;)

# Load the saved statistics
env = DummyVecEnv([lambda: gym.make(&amp;quot;HalfCheetahBulletEnv-v0&amp;quot;)])
env = VecNormalize.load(stats_path, env)
#  do not update them at test time
env.training = False
# reward normalization is not needed at test time
env.norm_reward = False

&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;more-examples&#34;&gt;More Examples&lt;/h3&gt;
&lt;p&gt;You can find more examples and associated colab notebooks in the 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/guide/examples.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;documentation&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;to-the-infinity-and-beyond&#34;&gt;To the Infinity and Beyond!&lt;/h2&gt;
&lt;p&gt;We presented Stable-Baselines3 v1.0, a set of reliable reinforcement learning implementations and the next major version of the Stable-Baselines.&lt;/p&gt;
&lt;p&gt;If you want to follow the updates of the library, we encourage you to star the repo on 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GitHub&lt;/a&gt; and click on &amp;ldquo;Watch -&amp;gt; Custom -&amp;gt; Releases&amp;rdquo; to be notified each time a new version is released ;) (you can also follow 
&lt;a href=&#34;https://twitter.com/ARGleave&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adam&lt;/a&gt; or 
&lt;a href=&#34;https://twitter.com/araffin2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Antonin&lt;/a&gt; on Twitter).
Coming soon, one of our long-time requested feature: 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/pull/243&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;mixed observations&lt;/a&gt; (aka dict obs) support.&lt;/p&gt;
&lt;p&gt;In case you want to contribute, make sure to read the 
&lt;a href=&#34;https://github.com/DLR-RM/stable-baselines3/blob/master/CONTRIBUTING.md&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;contributing guide&lt;/a&gt; first.&lt;/p&gt;
&lt;p&gt;Finally, if you make a cool project using Stable-Baselines3, please tell us when you want it to appear in 
&lt;a href=&#34;https://stable-baselines3.readthedocs.io/en/master/misc/projects.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;our project page&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;about-the-authors&#34;&gt;About the Authors&lt;/h2&gt;
&lt;p&gt;This blog post was co-written by Stable-Baselines3 maintainers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/araffin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Antonin Raffin&lt;/a&gt; (@araffin)&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/hill-a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ashley Hill&lt;/a&gt; (@hill-a)&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/ernestum&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Maximilian Ernestus&lt;/a&gt; (@ernestum)&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/adamgleave&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Adam Gleave&lt;/a&gt; (@AdamGleave)&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/Miffyli&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anssi Kanervisto&lt;/a&gt; (@Miffyli).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;citing-the-project&#34;&gt;Citing the Project&lt;/h2&gt;
&lt;p&gt;To cite Stable-Baselines3 in publications:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;@article{stable-baselines3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;did-you-find-this-post-helpful-consider-sharing-it-&#34;&gt;Did you find this post helpful? Consider sharing it ð&lt;/h3&gt;
</description>
    </item>
    
    <item>
      <title>Learning to Drive Smoothly in Minutes</title>
      <link>/post/learning-to-drive/</link>
      <pubDate>Sat, 26 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/post/learning-to-drive/</guid>
      <description>&lt;p&gt;Read the full article on 
&lt;a href=&#34;https://medium.com/data-science/learning-to-drive-smoothly-in-minutes-450a7cdb35f4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Stable Baselines: a Fork of OpenAI BaselinesâââReinforcement Learning Made Easy</title>
      <link>/post/stable-baselines/</link>
      <pubDate>Mon, 20 Aug 2018 00:00:00 +0000</pubDate>
      <guid>/post/stable-baselines/</guid>
      <description>&lt;p&gt;Read the full article on 
&lt;a href=&#34;https://medium.com/data-science/stable-baselines-a-fork-of-openai-baselines-reinforcement-learning-made-easy-df87c4b2fc82&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simple and Robust {ComputerâââArduino} Serial Communication</title>
      <link>/post/arduino-serial/</link>
      <pubDate>Sat, 10 Feb 2018 00:00:00 +0000</pubDate>
      <guid>/post/arduino-serial/</guid>
      <description>&lt;p&gt;Read the full article on 
&lt;a href=&#34;https://medium.com/@araffin/simple-and-robust-computer-arduino-serial-communication-f91b95596788&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Autonomous Racing Robot With an Arduino, a Raspberry Pi and a Pi Camera</title>
      <link>/post/racing-robot/</link>
      <pubDate>Fri, 10 Nov 2017 00:00:00 +0000</pubDate>
      <guid>/post/racing-robot/</guid>
      <description>&lt;p&gt;Read the full article on 
&lt;a href=&#34;https://becominghuman.ai/autonomous-racing-robot-with-an-arduino-a-raspberry-pi-and-a-pi-camera-3e72819e1e63&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
