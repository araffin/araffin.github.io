<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Antonin Raffin">

  
  
  
    
  
  <meta name="description" content="This is the second (and last) post on automatic hyperparameter optimization. In the first part, I introduced the challenges and main components of hyperparameter tuning (samplers, pruners, objective function, &hellip;). This second part is about the practical application of this technique with the Optuna library, in a reinforcement learning setting (using the Stable-Baselines3 (SB3) library).">

  
  <link rel="alternate" hreflang="en-us" href="/post/optuna/">

  


  
  
  
  <meta name="theme-color" content="hsl(16, 89%, 60%)">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-light.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-light.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
   
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_huc6f7a4edf58249363e4a58dcffa16205_339058_32x32_fill_lanczos_center_3.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_huc6f7a4edf58249363e4a58dcffa16205_339058_192x192_fill_lanczos_center_3.png">

  <link rel="canonical" href="/post/optuna/">

  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Antonin Raffin | Homepage">
  <meta property="og:url" content="/post/optuna/">
  <meta property="og:title" content="Automatic Hyperparameter Tuning - In Practice (Part 2) | Antonin Raffin | Homepage">
  <meta property="og:description" content="This is the second (and last) post on automatic hyperparameter optimization. In the first part, I introduced the challenges and main components of hyperparameter tuning (samplers, pruners, objective function, &hellip;). This second part is about the practical application of this technique with the Optuna library, in a reinforcement learning setting (using the Stable-Baselines3 (SB3) library)."><meta property="og:image" content="img/map[gravatar:%!s(bool=false) shape:circle]">
  <meta property="twitter:image" content="img/map[gravatar:%!s(bool=false) shape:circle]"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2025-04-23T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2025-04-23T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/optuna/"
  },
  "headline": "Automatic Hyperparameter Tuning - In Practice (Part 2)",
  
  "datePublished": "2025-04-23T00:00:00Z",
  "dateModified": "2025-04-23T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Antonin Raffin"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Antonin Raffin | Homepage",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/icon_huc6f7a4edf58249363e4a58dcffa16205_339058_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "This is the second (and last) post on automatic hyperparameter optimization. In the first part, I introduced the challenges and main components of hyperparameter tuning (samplers, pruners, objective function, \u0026hellip;). This second part is about the practical application of this technique with the Optuna library, in a reinforcement learning setting (using the Stable-Baselines3 (SB3) library)."
}
</script>

  

  


  


  





  <title>Automatic Hyperparameter Tuning - In Practice (Part 2) | Antonin Raffin | Homepage</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Antonin Raffin | Homepage</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Antonin Raffin | Homepage</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications_selected"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Automatic Hyperparameter Tuning - In Practice (Part 2)</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Apr 23, 2025
  </span>
  

  

  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>This is the second (and last) post on automatic hyperparameter optimization.
In the 
<a href="https://araffin.github.io/post/hyperparam-tuning/" target="_blank" rel="noopener">first part</a>, I introduced the challenges and main components of hyperparameter tuning (samplers, pruners, objective function, &hellip;).
This second part is about the practical application of this technique with the 
<a href="https://github.com/optuna/optuna" target="_blank" rel="noopener">Optuna library</a>, in a reinforcement learning setting (using the 
<a href="https://github.com/DLR-RM/stable-baselines3" target="_blank" rel="noopener">Stable-Baselines3 (SB3)</a> library).</p>
<p>Code: 
<a href="https://gist.github.com/araffin/d16e77aa88ffc246856f4452ab8a2524" target="_blank" rel="noopener">https://gist.github.com/araffin/d16e77aa88ffc246856f4452ab8a2524</a></p>
<p>Note: if you prefer to learn with video, I gave this tutorial at ICRA 2022.
The 
<a href="https://araffin.github.io/tools-for-robotic-rl-icra2022/" target="_blank" rel="noopener">slides</a>, notebooks and videos are online:</p>


    
    <div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
      <iframe allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen" loading="eager" referrerpolicy="strict-origin-when-cross-origin" src="https://www.youtube.com/embed/ihP7E76KGOI?autoplay=0&controls=1&end=0&loop=0&mute=0&start=0" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" title="YouTube video"
      ></iframe>
    </div>

<div style="margin-top: 50px"></div>
<h2 id="ppo-on-pendulum-v1---when-default-hyperparameters-dont-work">PPO on Pendulum-v1 - When default hyperparameters don&rsquo;t work</h2>
<p>To make this post more concrete, let&rsquo;s take a simple example where the default hyperparameters don&rsquo;t work.
In the 
<a href="https://gymnasium.farama.org/environments/classic_control/pendulum/" target="_blank" rel="noopener"><code>Pendulum-v1</code></a> environment, the RL agent controls a pendulum that &ldquo;starts in a random position, and the goal is to swing it up so it stays upright&rdquo;.</p>
<video controls src="https://huggingface.co/sb3/sac-Pendulum-v1/resolve/main/replay.mp4">
</video>
<p style="font-size: 14pt; text-align:center;">Trained SAC agent on the <code>Pendulum-v1</code> environment.
</p>
<p>The agent receives the state of the pendulum as input (cos and sine of the angle $\theta$ and angular velocity $\dot{\theta}$) and outputs the desired torque (1D).
The agent is rewarded for keeping the pendulum upright ($\theta = 0$ and $\dot{\theta} = 0$) and penalized for using high torques.
An episode ends after a timeout of 200 steps (
<a href="https://www.youtube.com/watch?v=eZ6ZEpCi6D8" target="_blank" rel="noopener">truncation</a>).</p>
<p>If you try to run the 
<a href="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html" target="_blank" rel="noopener">Proximal Policy Optimization (PPO)</a> algorithm on the <code>Pendulum-v1</code> environment, with a budget of 100,000 timesteps (SAC can solve this task in only 5,000 steps), it will not converge<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.
With the default hyperparameters, you will get an average return of about -1000, which is far from the best performance you can get, which is around -200:</p>
<pre><code class="language-python">from stable_baselines3 import PPO
# Faster, with Jax: from sbx import PPO

# Default hyperparameters don't work well
PPO(&quot;MlpPolicy&quot;, &quot;Pendulum-v1&quot;, verbose=1).learn(100_000, progress_bar=True)
</code></pre>
<h2 id="defining-the-search-space">Defining the Search Space</h2>
<p>The first thing to define when optimizing hyperparameters is the search space: what parameters to optimize and what range to explore?
You need also to decide from which distribution to sample from.
For example, in the case of continuous variables (like the discount factor $\gamma$ or the learning rate $\alpha$), values can be sampled from a uniform or 
<a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.loguniform.html" target="_blank" rel="noopener">log-uniform</a> distribution.</p>
<h3 id="sampling-methods">Sampling methods</h3>
<p>In practice, Optuna provides several <code>trial.suggest_...</code> methods to define which parameter to optimize with which distribution.
For instance, to sample the discount factor $\gamma$ uniformly from the range $[0, 1]$, you would use <code>gamma = trial.suggest_float(&quot;gamma&quot;, 0.0, 1.0)</code>.</p>
<p>I recommend reading the 
<a href="https://optuna.readthedocs.io/en/stable/" target="_blank" rel="noopener">Optuna documentation</a> to have a better understanding of the library and its features.
In the meantime, you need to know about some other useful methods for sampling hyperparameters:</p>
<ul>
<li><code>trial.suggest_float(..., log=True)</code> to sample from a log-uniform distribution (ex: learning rate)</li>
<li><code>trial.suggest_int(&quot;name&quot;, low, high)</code> to sample from integers (ex: mini-batch size), <code>low</code> and <code>high</code> are included</li>
<li><code>trial.suggest_categorical(&quot;name&quot;, choices)</code> for sampling from a list of choices (ex: choosing an activation function)</li>
</ul>
<p>Back to the PPO example on the <code>Pendulum-v1</code> task, what hyperparameters can be optimized and what range should be explored for each of them?</p>
<h3 id="ppo-hyperparameters">PPO hyperparameters</h3>
<p>
<a href="https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html" target="_blank" rel="noopener">PPO</a> has many hyperparameters, but to keep the search small (and this blog post short), I will limit the search to four parameters: the learning rate $\alpha$, the discount factor $\gamma$, the activation function of the neural networks and the number of steps for data collection (<code>n_steps</code>).</p>
<p>Tuning the learning rate $\alpha$ is crucial for fast but stable training. If $\alpha$ is too big, the training tends to be unstable and usually leads to NaNs (or other numerical instability). If it is too small, it will take forever to converge.
Since the learning rate $\alpha$ is a continuous variable (it is a float) and distinguishing between small learning rates is important, it is recommended to use a log-uniform distribution for sampling.
For the range, the PPO default learning rate value is $\alpha_0 = 3e^{-4}$, so I defined the search space to be between $\alpha_{\text{min}} = \alpha_0 / 10 = 3e^{-5}$ and $\alpha_{\text{min}} = 10 \alpha_0 = 3e^{-3}$.
This translates to <code>learning_rate = trial.suggest_float(&quot;learning_rate&quot;, 3e-5, 3e-3, log=True)</code> with Optuna.</p>
<p>The discount factor $\gamma$ represents a trade-off between optimizing short-term rewards and long-term rewards.
In general, we want to maximize the sum of undiscounted rewards ($\gamma = 1$), but in practice $\gamma &lt; 1$ works best (while keeping $\gamma \approx 1$).
A recommended range for the discount factor $\gamma$ is $[0.97, 0.9999]$<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> (default is 0.99), so in Python <code>gamma = trial.suggest_float(&quot;gamma&quot;, 0.97, 0.9999)</code>.</p>
<p>I&rsquo;m considering two activation functions in this example: 
<a href="https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html" target="_blank" rel="noopener">Tanh</a> and 
<a href="https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html" target="_blank" rel="noopener">ReLU</a>.
Because the activation function is sampled from a list of options, <code>activation_fn = trial.suggest_categorical(&quot;activation_fn&quot;, [&quot;tanh&quot;, &quot;relu&quot;])</code> is the corresponding code<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<p>Finally, PPO has a <code>n_steps</code> parameter that controls the &ldquo;number of steps to run for each environment per update&rdquo;.
That is to say, PPO will update its policy every <code>n_steps * n_envs</code> steps (and collect <code>n_steps * n_envs</code> transitions to sample from).
This hyperparameter also affects the value and advantage estimation (larger <code>n_steps</code> leads to less biased estimates).
It is recommended to use power of two for its value<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>, so it can be sampled with <code>n_steps_pow = trial.suggest_int(&quot;n_steps_pow&quot;, 5, 12)</code> (from $2^5=32$ to $2^{12}=4096$).</p>
<p>The overall sampling function looks like that:</p>
<pre><code class="language-python">from typing import Any

import optuna

def sample_ppo_params(trial: optuna.Trial) -&gt; dict[str, Any]:
    &quot;&quot;&quot;Sampler for PPO hyperparameters.&quot;&quot;&quot;
    # From 2**5=32 to 2**12=4096
    n_steps_pow = trial.suggest_int(&quot;n_steps_pow&quot;, 5, 12)
    gamma = trial.suggest_float(&quot;one_minus_gamma&quot;, 0.97, 0.9999)
    learning_rate = trial.suggest_float(&quot;learning_rate&quot;, 3e-5, 3e-3, log=True)
    activation_fn = trial.suggest_categorical(&quot;activation_fn&quot;, [&quot;tanh&quot;, &quot;relu&quot;])

    n_steps = 2**n_steps_pow
    # Display true values
    trial.set_user_attr(&quot;n_steps&quot;, n_steps)
    # Convert to PyTorch objects
    activation_fn = {&quot;tanh&quot;: nn.Tanh, &quot;relu&quot;: nn.ReLU}[activation_fn]

    return {
        &quot;n_steps&quot;: n_steps,
        &quot;gamma&quot;: gamma,
        &quot;learning_rate&quot;: learning_rate,
        &quot;policy_kwargs&quot;: {
            &quot;activation_fn&quot;: activation_fn,
        },
    }
</code></pre>
<h2 id="defining-the-objective-function">Defining the Objective Function</h2>
<p>After choosing the search space, you need to define the objective function.
In reinforcement learning, we usually want to get the best performance for a given budget (either in terms of samples or training time), so we optimize for maximum episodic reward.</p>
<p>One way to measure the performance is to periodically evaluate the agent on a test environment for multiple episodes:</p>
<pre><code class="language-python">from stable_baselines3.common.evaluation import evaluate_policy
# model = PPO(&quot;MlpPolicy&quot;, &quot;Pendulum-v1&quot;)
# eval_env = gym.make(&quot;Pendulum-v1&quot;)
# Note: by default, evaluate_policy uses the deterministic policy
mean_return, std_return = evaluate_policy(model, eval_env, n_eval_episodes=20)
</code></pre>
<p>With SB3, I will use a custom 
<a href="https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html" target="_blank" rel="noopener">callback</a> to trigger evaluations at different stages of training:</p>
<pre><code class="language-python">from stable_baselines3.common.callbacks import BaseCallback

class TrialEvalCallback(BaseCallback):
    &quot;&quot;&quot;Callback used for evaluating and reporting a trial.&quot;&quot;&quot;

    def _on_step(self) -&gt; bool:
        if self.eval_freq &gt; 0 and self.n_calls % self.eval_freq == 0:
            # Evaluate the current policy every n_steps
            mean_return, _ = evaluate_policy(self.model, self.eval_env)
            self.eval_idx += 1
            # Send report to Optuna
            self.trial.report(mean_return, self.eval_idx)
            # Prune (stop training) trial if needed
            if self.trial.should_prune():
                self.is_pruned = True
                return False
        return True
</code></pre>
<p>This callback also allows to stop training early if a trial is too bad and should be pruned (by checking <code>trial.should_prune()</code>).</p>
<p>The full objective method contains additional code to create the training environment, sample the hyperparameters, instantiate the RL agent and train it:</p>
<pre><code class="language-python">from stable_baselines3.common.env_util import make_vec_env

N_ENVS = 5
N_TIMESTEPS = 40_000
# Evaluate every 20_000 steps
# each vec_env.step() is N_ENVS steps
EVAl_FREQ = 20_000 // N_ENVS

def objective(trial: optuna.Trial) -&gt; float:
    # Create train and eval envs,
    # I use multiple envs in parallel for faster training
    vec_env = make_vec_env(&quot;Pendulum-v1&quot;, n_envs=N_ENVS)
    eval_env = make_vec_env(&quot;Pendulum-v1&quot;, n_envs=N_ENVS)
    # Sample hyperparameters. and create the RL model
    model = PPO(&quot;MlpPolicy&quot;, vec_env, **sample_ppo_params(trial))

    # Create the callback that will periodically evaluate and report the performance.
    eval_callback = TrialEvalCallback(
        eval_env,
        trial,
        n_eval_episodes=20,
        eval_freq=EVAl_FREQ, 
    )
    # Train the RL agent
    model.learn(N_TIMESTEPS, callback=eval_callback)

    if eval_callback.is_pruned:
        raise optuna.exceptions.TrialPruned()
    # Report final performance
    return eval_callback.last_mean_reward
</code></pre>
<h2 id="choosing-sampler-and-pruner">Choosing Sampler and Pruner</h2>
<p>Finally, after defining the search space and the objective function, you have to choose a sampler and (optionally) a pruner (see 
<a href="../hyperparam-tuning/">part one</a>).
If you don&rsquo;t know what to choose, Optuna now has an 
<a href="https://hub.optuna.org/samplers/auto_sampler/" target="_blank" rel="noopener">AutoSampler</a> which will choose a recommended sampler (between <code>TPESampler</code>, <code>GPSampler</code> and <code>CmaEsSampler</code>) for you based on heuristics.</p>
<p>Here, I selected <code>TPESampler</code> and <code>MedianPruner</code> because they tend to be good default choices. Don&rsquo;t forget to pass <code>n_startup_trials</code> to both to warm up the optimization with a <code>RandomSampler</code> and to avoid premature convergence (like pruning potentially good trials too early):</p>
<pre><code class="language-python">from optuna.pruners import MedianPruner
from optuna.samplers import TPESampler, RandomSampler
 
# Select the sampler, can be random, TPESampler, CMAES, ...
sampler = TPESampler(n_startup_trials=5)
# Do not prune before 1/3 of the max budget is used
pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=N_EVALUATIONS // 3)
	
# Create the study and start the hyperparameter optimization
study = optuna.create_study(sampler=sampler, pruner=pruner, direction=&quot;maximize&quot;)
# This script can be launch in parallel when using a database
# We pass the objective function defined previously
study.optimize(objective, n_trials=N_TRIALS, timeout=TIMEOUT)
# Best result
best_trial = study.best_trial
</code></pre>
<p>Et voilÃ ! That&rsquo;s all you need to run automatic hyperparameter optimization.
If you now run the 
<a href="https://gist.github.com/araffin/d16e77aa88ffc246856f4452ab8a2524" target="_blank" rel="noopener">final script</a> for five minutes, it should already find hyperparameters that give good results.</p>
<p>For example, in one of the runs I was able to get in just two minutes:</p>
<pre><code class="language-yaml">Number of finished trials: 21
Best trial:
  Value:  -198.01224440000001
  Params: 
    n_steps_pow: 8
    gamma: 0.9707141699579157
    learning_rate: 0.0014974865679170315
    activation_fn: relu
  User attrs:
    n_steps: 256
</code></pre>
<p>To verify that these hyperparameters actually work (more on that soon), you can use:</p>
<pre><code class="language-python">from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
import torch as th

vec_env = make_vec_env(&quot;Pendulum-v1&quot;, n_envs=5)
# Using optimized hyperparameters
policy_kwargs = dict(activation_fn=th.nn.ReLU)
hyperparams = dict(n_steps=256, gamma=0.97, learning_rate=1.5e-3)

model = PPO(&quot;MlpPolicy&quot;, vec_env, verbose=1, **hyperparams)
model.learn(40_000, progress_bar=False)
</code></pre>
<p>It should give you much better results than the default hyperparameters with half the training budget.</p>
<p>Note: I recommend using the 
<a href="https://github.com/DLR-RM/rl-baselines3-zoo" target="_blank" rel="noopener">RL Zoo</a> for more complex settings.
It includes automatic hyperparameter tuning, loading trial and distributed optimization.
Example:</p>
<pre><code class="language-bash">python -m rl_zoo3.train --algo ppo --env Pendulum-v1 -optimize --storage ./demo.log --study-name demo
</code></pre>
<h2 id="distributed-optimization">Distributed Optimization</h2>
<p>A simple way to speed up the optimization process is to run multiple trials in parallel.
To do this, you need to use a 
<a href="https://optuna.readthedocs.io/en/stable/reference/storages.html" target="_blank" rel="noopener">database</a> and pass it to Optuna&rsquo;s <code>create_study()</code> method.</p>
<p>The easiest way to distribute tuning is to use a 
<a href="https://optuna.readthedocs.io/en/stable/reference/generated/optuna.storages.JournalStorage.html" target="_blank" rel="noopener">log file</a> for storage and start the same script in multiple terminals (and potentially on multiple machines):</p>
<pre><code class="language-python">storage_file = &quot;./my_studies.log&quot;
study_name = &quot;ppo-Pendulum-v1_1&quot;

storage = optuna.storages.JournalStorage(
    optuna.storages.journal.JournalFileBackend(storage_file),
)
study = optuna.create_study(
    ...
    storage=storage,
    study_name=study_name,
    load_if_exists=True,
)
</code></pre>
<p>If you use a database (you should), you can also use 
<a href="https://github.com/optuna/optuna-dashboard" target="_blank" rel="noopener">Optuna dashboard</a> to monitor the optimization progress.</p>
<img style="max-width:100%" src="https://optuna-dashboard.readthedocs.io/en/latest/_images/optuna-dashboard.gif"/>
<p style="font-size: 14pt; text-align:center;">Optuna dashboard demo</p>
<h2 id="tips-and-tricks">Tips and Tricks</h2>
<p>Before concluding this blog post, I would like to give you some tips and tricks to keep in mind when doing hyperparameter tuning.</p>
<h3 id="start-simple">Start simple</h3>
<p>First, as with any RL problem, starting simple is the 
<a href="https://www.youtube.com/watch?v=eZ6ZEpCi6D8" target="_blank" rel="noopener">key to success</a>.
Do not try to optimize too many hyperparameters at once, using large ranges.
My advice would be to start with a minimal number of parameters (i.e., start with a small search space) and increase their number and ranges only as needed.</p>
<p>For example, to decide whether to increase or decrease the search range, you can look at the best trials so far.
If the best trials are close to the limits of the search space (saturation), this is a sign that you should increase the limits.
On the other hand, if above a certain threshold for a parameter, all trials are bad, you can probably reduce the search space (to at most the threshold).</p>
<p>Another thing to keep in mind is that most of the time you don&rsquo;t need automatic tuning.
Simply training for a longer time can improve the results without changing the hyperparameters.</p>
<h3 id="post-evaluation-to-remove-noise">Post-Evaluation to Remove Noise</h3>
<p>Last but not least (and perhaps the most important tip), do not forget that RL training is a stochastic process.
This means that the performance reported for each trial can be noisy: if you run the same trial but with a different random seed, you might get different results.</p>
<p>I tend to approach this problem in two ways.</p>
<p>To filter out the evaluation noise, I usually re-evaluate the top trials multiple times to find out which ones were actually &ldquo;lucky seeds&rdquo; and which ones work consistently.
Another way to deal with this problem is to do multiple runs per trial: each run uses the same hyperparameters but a different random seed.
However, this technique is expensive in terms of computation time and makes it difficult to prune out bad trials early.</p>
<h2 id="conclusion">Conclusion</h2>
<p>In this second part, I went through the process of doing automatic hyperparameter tuning in practice, using the Optuna library.
I&rsquo;ve covered:</p>
<ul>
<li>defining the search space and the objective function</li>
<li>choosing a sampler and a pruner</li>
<li>speeding up the tuning process with distributed optimization</li>
<li>tips and tricks to keep in mind when doing automatic hyperparameter tuning</li>
</ul>
<p>As a conclusion and transition to the next blog post, I will use this technique to 
<a href="./sac-massive-sim/">tune SAC for fast training</a> when using a massively parallel environment like Isaac Sim.</p>
<p>PS: In case you missed it, you can find the final script here: 
<a href="https://gist.github.com/araffin/d16e77aa88ffc246856f4452ab8a2524" target="_blank" rel="noopener">https://gist.github.com/araffin/d16e77aa88ffc246856f4452ab8a2524</a></p>
<h2 id="appendix---the-optuna-library">Appendix - The Optuna Library</h2>
<p>Among the various open-source libraries for hyperparameter optimization (such as 
<a href="https://github.com/hyperopt/hyperopt" target="_blank" rel="noopener">hyperopt</a> or 
<a href="https://github.com/facebook/Ax" target="_blank" rel="noopener">Ax</a>), I chose 
<a href="https://optuna.org/" target="_blank" rel="noopener">Optuna</a> for multitple reasons:</p>
<ul>
<li>it has a clean API and good 
<a href="https://optuna.readthedocs.io/en/stable/index.html" target="_blank" rel="noopener">documentation</a></li>
<li>it supports many samplers and pruners</li>
<li>it has some nice additional features (like easy distributed optimization support, multi-objective support or the 
<a href="https://optuna-dashboard.readthedocs.io/en/latest/getting-started.html" target="_blank" rel="noopener">optuna-dashboard</a>)</li>
</ul>
<h2 id="citation">Citation</h2>
<pre><code>@article{raffin2025optuna,
  title   = &quot;Automatic Hyperparameter Tuning - In Practice&quot;,
  author  = &quot;Raffin, Antonin&quot;,
  journal = &quot;araffin.github.io&quot;,
  year    = &quot;2025&quot;,
  month   = &quot;April&quot;,
  url     = &quot;https://araffin.github.io/post/optuna/&quot;
}
</code></pre>
<!-- ## Acknowledgement

All the graphics were made using [excalidraw](https://excalidraw.com/). -->
<h3 id="did-you-find-this-post-helpful-consider-sharing-it-">Did you find this post helpful? Consider sharing it ðŸ™Œ</h3>
<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Without proper 
<a href="https://github.com/DLR-RM/stable-baselines3/issues/633" target="_blank" rel="noopener">truncation handling</a>, PPO will actually not converge even in 1 million steps with default hyperparameters.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>A common way to define the param range is to start small and later increase the search space if the best parameters found are at the boundary of the defined range.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>I convert strings to PyTorch objects later because options need to be serializable to be stored by Optuna.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>One of the main reasons for choosing a power of two is that the GPU kernel/hardware is optimized for power of two operations. Also, in practice, <code>n_steps=4096</code> vs. <code>n_steps=4000</code> doesn&rsquo;t make much difference, so using a power of two reduces the search space.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    </div>

    







<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/optuna/&amp;text=Automatic%20Hyperparameter%20Tuning%20-%20In%20Practice%20%28Part%202%29" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/optuna/&amp;t=Automatic%20Hyperparameter%20Tuning%20-%20In%20Practice%20%28Part%202%29" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Automatic%20Hyperparameter%20Tuning%20-%20In%20Practice%20%28Part%202%29&amp;body=/post/optuna/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/optuna/&amp;title=Automatic%20Hyperparameter%20Tuning%20-%20In%20Practice%20%28Part%202%29" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Automatic%20Hyperparameter%20Tuning%20-%20In%20Practice%20%28Part%202%29%20/post/optuna/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/post/optuna/&amp;title=Automatic%20Hyperparameter%20Tuning%20-%20In%20Practice%20%28Part%202%29" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  





  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hude3a62d3a42e4e03c4c1814b0a41a4f6_54165_270x270_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/">Antonin Raffin</a></h5>
      <h6 class="card-subtitle">Research Engineer in Robotics and Machine Learning</h6>
      <p class="card-text">Robots. Machine Learning. Blues Dance.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/antonin-raffin-106b18a8/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://bsky.app/profile/araffin.bsky.social" target="_blank" rel="noopener">
        <i class="fab fa-bluesky"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.fr/citations?user=kik4AwIAAAAJ&amp;hl=fr" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/araffin" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.twitch.tv/givethatrobotacookie" target="_blank" rel="noopener">
        <i class="fab fa-twitch"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.youtube.com/user/atooo57" target="_blank" rel="noopener">
        <i class="fab fa-youtube"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>












  
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.80e8497da12d94dc7fea279b7993043d.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    Â© 2018 - 2025 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
