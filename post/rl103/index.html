<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Antonin Raffin">

  
  
  
    
  
  <meta name="description" content="This second blog post continues my practical introduction to (deep) reinforcement learning, presenting the main concepts and providing intuitions to understand the more recent Deep RL algorithms.
In a first post (RL102), I started from tabular Q-learning and worked my way up to Deep Q-learning (DQN).">

  
  <link rel="alternate" hreflang="en-us" href="/post/rl103/">

  


  
  
  
  <meta name="theme-color" content="hsl(16, 89%, 60%)">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-light.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-light.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
   
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_huc6f7a4edf58249363e4a58dcffa16205_339058_32x32_fill_lanczos_center_3.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_huc6f7a4edf58249363e4a58dcffa16205_339058_192x192_fill_lanczos_center_3.png">

  <link rel="canonical" href="/post/rl103/">

  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="og:site_name" content="Antonin Raffin | Homepage">
  <meta property="og:url" content="/post/rl103/">
  <meta property="og:title" content="RL103: From Deep Q-Learning (DQN) to Soft Actor-Critic (SAC) and Beyond | Antonin Raffin | Homepage">
  <meta property="og:description" content="This second blog post continues my practical introduction to (deep) reinforcement learning, presenting the main concepts and providing intuitions to understand the more recent Deep RL algorithms.
In a first post (RL102), I started from tabular Q-learning and worked my way up to Deep Q-learning (DQN)."><meta property="og:image" content="/post/rl103/featured.png">
  <meta property="twitter:image" content="/post/rl103/featured.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2025-12-12T00:00:00&#43;00:00">
    
    <meta property="article:modified_time" content="2025-12-12T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/rl103/"
  },
  "headline": "RL103: From Deep Q-Learning (DQN) to Soft Actor-Critic (SAC) and Beyond",
  
  "image": [
    "/post/rl103/featured.png"
  ],
  
  "datePublished": "2025-12-12T00:00:00Z",
  "dateModified": "2025-12-12T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Antonin Raffin"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Antonin Raffin | Homepage",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/icon_huc6f7a4edf58249363e4a58dcffa16205_339058_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "This second blog post continues my practical introduction to (deep) reinforcement learning, presenting the main concepts and providing intuitions to understand the more recent Deep RL algorithms.\nIn a first post (RL102), I started from tabular Q-learning and worked my way up to Deep Q-learning (DQN)."
}
</script>

  

  


  


  





  <title>RL103: From Deep Q-Learning (DQN) to Soft Actor-Critic (SAC) and Beyond | Antonin Raffin | Homepage</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Antonin Raffin | Homepage</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Antonin Raffin | Homepage</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications_selected"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#talks"><span>Talks</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      

      

    </ul>

  </div>
</nav>


  <article class="article">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>RL103: From Deep Q-Learning (DQN) to Soft Actor-Critic (SAC) and Beyond</h1>

  

  


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Dec 12, 2025
  </span>
  

  

  

  
  
  

  
  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 243px;">
  <div style="position: relative">
    <img src="/post/rl103/featured_hu502674880df54bf641281eab9fe98794_112758_720x0_resize_lanczos_3.png" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p>This second blog post continues my practical introduction to (deep) reinforcement learning, presenting the main concepts and providing intuitions to understand the more recent Deep RL algorithms.</p>
<p>In a 
<a href="../rl102">first post (RL102)</a>, I started from tabular Q-learning and worked my way up to Deep Q-learning (DQN).
In this second post, I continue on to the Soft Actor-Critic (SAC) algorithm and its extensions.</p>
<p>Note: this post is part of my 
<a href="https://mediatum.ub.tum.de/?id=1743420" target="_blank" rel="noopener">PhD Thesis</a> and you can watch part of this blog on 
<a href="https://www.youtube.com/watch?v=Sb0dgmxbGZY" target="_blank" rel="noopener">YouTube</a> (presented at a ML workshop in CERN).</p>
<h2 id="fqi-and-dqn-limitations">FQI and DQN Limitations</h2>
<p>While FQI and DQN algorithms can handle continuous state spaces, they are still limited to discrete action spaces.
Indeed, <span style="color:#5F3DC4">all possible actions</span>
($\color{#5F3DC4}{a \in \mathcal{A}}$) must be enumerated to compute $\max_{a&rsquo; \in \mathcal{A}}Q^{n-1}_\theta(\ldots)$
(see 
<a href="../rl102/#the-full-dqn-algorithm">part I</a>).
This $\max$ operation is used to update the $Q$-value estimate and select the action according to the greedy policy<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>One solution to enable $Q$-learning in continuous action space is to parametrize the $Q$-function so that its maximum can be easily and analytically determined.
This is what the 
<a href="https://arxiv.org/abs/1603.00748" target="_blank" rel="noopener">Normalized Advantage Function (NAF)</a> does by restricting the $Q$-function to a function quadratic in $a$.</p>
<h2 id="extending-dqn-to-continuous-actions-deep-deterministic-policy-gradient-ddpg">Extending DQN to Continuous Actions: Deep Deterministic Policy Gradient (DDPG)</h2>
<p>Another possibility is to train a second network $\pi_{\phi}(s)$ to maximize the learned $Q$-function<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.
In other words, $\pi_{\phi}$ is the actor network with parameters $\phi$ and outputs the action that leads to the highest return according to the $Q$-function:</p>
<p>\begin{align}
\max_{a \in A} Q_\theta(s, a) \approx Q_\theta(s, \pi_{\phi}(s)).
\end{align}</p>
<p>This idea, developed by the 
<a href="https://arxiv.org/abs/1509.02971" target="_blank" rel="noopener">Deep Deterministic Policy Gradient (DDPG)</a> algorithm, provides an explicit deterministic policy $\pi_{\phi}$ for continuous actions.
Since $Q_\theta$ and $\pi_{\phi}$ are both differentiable, the actor network $\pi_{\phi}$ is directly trained to maximize $Q_\theta(s, \pi_{\phi}(s))$ using samples from the replay buffer $\mathcal{D}$ (as illustrated in the figure below).
The DDPG actor&rsquo;s loss is therefore:</p>
<!--\begin{align}-->
  <!--\mathcal{L}_\pi(\phi, \mathcal{D}) = \max_{\phi} \mathbb{E}_{s \sim \mathcal{D}}\left[ Q_\theta(s, \pi_\phi(s)) \right].-->
<!--\end{align}-->
<img style="height: 50px;" src="./img/ddpg.svg"/>
<video controls src="https://araffin.github.io/slides/advances-rl-sota-2026/images/DDPGLoss.mp4">
</video>
<p style="font-size: 14pt; text-align:center;">
  An animation of the DDPG update of the actor network.
</p>
<img width="100%"  src="./img/ddpg_grad_flow.svg"/>
<p style="font-size: 14pt; text-align:center;">
  DDPG update of the actor network.
  The gradient computed using the DDPG loss is backpropagated through the $Q$-network to update the actor network so that it maximizes the $Q$-function.
</p>
<p>For the update of the $Q$-function $Q_\theta$, DDPG uses the same regression target as DQN.</p>
<p>DDPG extends DQN to continuous actions but has some practical limitations.
$\pi_{\phi}$ tends to exploit regions of the state space where the $Q$-function 
<a href="https://arxiv.org/abs/1802.09477" target="_blank" rel="noopener">overestimates the $Q$-value</a>, as shown below.</p>
<img width="100%"  src="./img/q_value_overestimation.svg"/>
<p style="font-size: 14pt; text-align:center;">
  Illustration of the overestimation and extrapolation error when approximating the $Q$-function.
  In regions where there is training data (black dots), the approximation matches the true $Q$-function.
  However, outside the training data support, there may be extrapolation error (in red) and overestimation that the actor network can exploit.
</p>
<p>These regions are usually those that are not well covered by samples from the buffer $\mathcal{D}$.
Because of this interaction between the actor and critic networks, DDPG is also often unstable in practice (divergent behavior).</p>
<h2 id="twin-delayed-ddpg-td3-and-soft-actor-critic-sac">Twin Delayed DDPG (TD3) and Soft Actor-Critic (SAC)</h2>
<p>To overcome the limitations of DDPG, 
<a href="https://arxiv.org/abs/1802.09477" target="_blank" rel="noopener">Twin Delayed DDPG (TD3)</a> employs three key techniques:</p>
<ol>
<li>Twin $Q$-networks: TD3 uses two separate $Q$-networks and selects the minimum $Q$-value estimate from the two networks. This helps to reduce overestimation bias in the $Q$-value estimates.</li>
<li>Delayed policy updates: TD3 updates the policy network less frequently than the $Q$-networks, allowing the policy network to converge before being updated.</li>
<li>Target action noise: TD3 adds noise to the target action during the $Q$-network update step.
This makes it harder for the actor to exploit the learned $Q$-function.</li>
</ol>
<video controls src="https://araffin.github.io/slides/advances-rl-sota-2026/images/QOverestimation.mp4">
</video>
<p style="font-size: 14pt; text-align:center;">
  An animation of the overestimation and extrapolation error and how TD3 tries to reduce it.
</p>
<p>Since TD3 learns a deterministic actor network $\pi_{\phi}$, it relies on external noise during the exploration phase.
A common approach is to use a 
<a href="https://arxiv.org/abs/2005.05719" target="_blank" rel="noopener">step-based Gaussian noise</a>:</p>
<!--\begin{align}
  \mathbf{a}_t = \pi_{\phi}(\mathbf{s}_t) + \epsilon_t,  \epsilon_t \sim \mathcal{N}(0, \sigma^2).
\end{align}-->
<img style="height: 25px;" src="./img/eq2161.svg"/>
<img style="height: 25px;" src="./img/eq2162.svg"/>
<p>While the standard deviation $\sigma$ is usually kept constant, it is a critical hyperparameter that gives a compromise between exploration and exploitation.</p>
<p>To better balance exploration and exploitation, 
<a href="https://arxiv.org/abs/1801.01290" target="_blank" rel="noopener">Soft Actor-Critic (SAC)</a>, successor of 
<a href="https://arxiv.org/abs/1702.08165" target="_blank" rel="noopener">Soft Q-Learning (SQL)</a>, optimizes the maximum-entropy objective, which is slightly different from the 
<a href="../rl102#appendix-rl101">classical RL objective</a>:</p>
<!--\begin{align}
  \label{eq:2_maxent_objective}
  J(\pi) = \sum_{t} \mathbb{E}_{(\mathbf{s}_t, \mathbf{a}_t) \sim \rho_\pi}\!\left[ \gamma^t\, r(\mathbf{s}_t,\mathbf{a}_t) + \color{#006400}{\alpha\,\mathcal{H}(\pi(\cdot \mid \mathbf{s}_t))} \right]
\end{align}-->
<img style="height: 55px;" src="./img/eq217.svg"/>
<p>where <span style="color:#006400">$\mathcal{H}$ is the policy entropy</span> and <span style="color:#006400">$\alpha$ is the entropy temperature</span>, allowing a trade-off between the two objectives.
This objective encourages exploration by maximizing the entropy of the policy while still solving the task by maximizing the expected return (classic RL objective).</p>
<p>SAC learns a stochastic policy using a 
<a href="../sac-massive-sim/#sac-squashed-gaussian">squashed Gaussian distribution</a>, and incorporates the clipped double $Q$-learning trick from TD3.
In its 
<a href="https://arxiv.org/abs/1812.05905" target="_blank" rel="noopener">latest iteration</a>, SAC automatically adjusts the entropy coefficient $\alpha$, eliminating the need to tune this crucial hyperparameter.</p>
<img style="width:100%" src="./img/sac_algo.svg"/>
<p>In summary, as shown in the algorithm block above, SAC combines several key elements from the algorithms presented in this 
<a href="../rl102">blog post series</a>.
It uses the update rule from FQI and adopts the Q-network, target network and replay buffer from DQN to learn the $Q$-function.</p>
<p>SAC also incorporates techniques from DDPG to handle continuous actions, uses the clipped double $Q$-learning trick from TD3 to reduce overestimation bias, and optimizes the maximum entropy objective with a stochastic policy to balance exploration and exploitation.</p>
<p>
<a href="https://github.com/DLR-RM/stable-baselines3/blob/c6ce50fc7020eb8c009d9579e0c0dbbdba024bc0/stable_baselines3/sac/sac.py#L202" target="_blank" rel="noopener">SAC</a> and its variants are the algorithms I used during my PhD to train RL agents 
<a href="https://www.youtube.com/watch?v=3UKVTJU89Lc" target="_blank" rel="noopener">directly on real robots</a>.</p>
<h2 id="beyond-sac-tqc-redq-droq-">Beyond SAC: TQC, REDQ, DroQ, &hellip;</h2>
<p>Several 
<a href="https://araffin.github.io/slides/recent-advances-rl/" target="_blank" rel="noopener">extensions of SAC</a><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> have been proposed, in particular to improve the sample efficiency.
One notable example is 
<a href="https://arxiv.org/abs/2005.04269" target="_blank" rel="noopener">Truncated Quantile Critics (TQC)</a> which builds upon SAC by incorporating 
<a href="https://arxiv.org/abs/1707.06887" target="_blank" rel="noopener">distributional RL</a>.</p>
<p>In distributional RL, the $Q$-function estimates the distribution of returns instead of just the expected return.
The figure below illustrates the benefits of learning the distribution of returns rather than only the expected value in an example.</p>
<img width="80%"  src="./img/distributional_rl.svg"/>
<p style="font-size: 14pt; text-align:center;">
  An example where learning the distribution of returns (distributional RL) instead of the expected value (classic RL) can be useful.
  We plot the distribution of returns for a given state-action pair $(s, a)$.
  In this case, there is a bimodal distribution.
  Learning the expected value of it instead of the distribution itself is harder and does not allow to measure the risk of taking a particular action.
</p>
<p>A key idea to improve sample efficiency is to perform multiple gradient updates for each data collection step.
However, simply increasing the 
<a href="../tune-sac-isaac-sim/#replay-ratio">update-to-data (UTD) ratio</a> may not lead to better performance due to the overestimation bias.</p>
<p>To address this issue, the algorithms 
<a href="https://arxiv.org/abs/2101.05982" target="_blank" rel="noopener">REDQ</a> and 
<a href="https://arxiv.org/abs/2110.02034" target="_blank" rel="noopener">DroQ</a> rely on ensembling techniques (explicit for REDQ, implicit for DroQ with dropout).
Finally, a new algorithm, 
<a href="https://arxiv.org/abs/1902.05605" target="_blank" rel="noopener">CrossQ</a>, takes a different approach by removing the target network and using batch normalization to stabilize learning<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>.</p>
<h2 id="citation">Citation</h2>
<pre><code>@article{raffin2025rl103,
  title   = &quot;RL103: From Deep Q-Learning (DQN) to Soft Actor-Critic (SAC) and Beyond&quot;,
  author  = &quot;Raffin, Antonin&quot;,
  journal = &quot;araffin.github.io&quot;,
  year    = &quot;2025&quot;,
  month   = &quot;Dec&quot;,
  url     = &quot;https://araffin.github.io/post/rl103/&quot;
}
</code></pre>
<h2 id="acknowledgement">Acknowledgement</h2>
<p>I would like to thank Anssi and Alison for their feedback =).</p>
<p>All the graphics were made using 
<a href="https://excalidraw.com/" target="_blank" rel="noopener">excalidraw</a> and 
<a href="https://viereck.ch/latex-to-svg/" target="_blank" rel="noopener">latex-to-svg</a>.</p>
<h3 id="did-you-find-this-post-helpful-consider-sharing-it-">Did you find this post helpful? Consider sharing it ðŸ™Œ</h3>
<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Selecting the best action with $\arg\max_{\color{#5F3DC4}{a \in \mathcal{A}}} Q(s, a)$&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>A third option is to sample the $Q$-value, as explored by 
<a href="https://arxiv.org/abs/1806.10293" target="_blank" rel="noopener">QT-Opt</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Several SAC extensions are available in 
<a href="https://github.com/Stable-Baselines-Team/stable-baselines3-contrib" target="_blank" rel="noopener">SB3 Contrib</a> and 
<a href="https://github.com/araffin/sbx" target="_blank" rel="noopener">SBX (SB3 + Jax)</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>To be continued&hellip;&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    </div>

    







<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/rl103/&amp;text=RL103:%20From%20Deep%20Q-Learning%20%28DQN%29%20to%20Soft%20Actor-Critic%20%28SAC%29%20and%20Beyond" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/rl103/&amp;t=RL103:%20From%20Deep%20Q-Learning%20%28DQN%29%20to%20Soft%20Actor-Critic%20%28SAC%29%20and%20Beyond" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=RL103:%20From%20Deep%20Q-Learning%20%28DQN%29%20to%20Soft%20Actor-Critic%20%28SAC%29%20and%20Beyond&amp;body=/post/rl103/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/rl103/&amp;title=RL103:%20From%20Deep%20Q-Learning%20%28DQN%29%20to%20Soft%20Actor-Critic%20%28SAC%29%20and%20Beyond" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=RL103:%20From%20Deep%20Q-Learning%20%28DQN%29%20to%20Soft%20Actor-Critic%20%28SAC%29%20and%20Beyond%20/post/rl103/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=/post/rl103/&amp;title=RL103:%20From%20Deep%20Q-Learning%20%28DQN%29%20to%20Soft%20Actor-Critic%20%28SAC%29%20and%20Beyond" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  





  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hude3a62d3a42e4e03c4c1814b0a41a4f6_54165_270x270_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/">Antonin Raffin</a></h5>
      <h6 class="card-subtitle">Research Engineer in Robotics and Machine Learning</h6>
      <p class="card-text">Robots. Machine Learning. Blues Dance.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/antonin-raffin-106b18a8/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://bsky.app/profile/araffin.bsky.social" target="_blank" rel="noopener">
        <i class="fab fa-bluesky"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.fr/citations?user=kik4AwIAAAAJ&amp;hl=fr" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/araffin" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.twitch.tv/givethatrobotacookie" target="_blank" rel="noopener">
        <i class="fab fa-twitch"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.youtube.com/user/atooo57" target="_blank" rel="noopener">
        <i class="fab fa-youtube"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>












  
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
      

    

    
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.80e8497da12d94dc7fea279b7993043d.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    Â© 2018 - 2026 &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
