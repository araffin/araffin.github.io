<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">














  <meta name="author" content="Antonin Raffin">






  <meta name="description" content="This post details how I managed to get the Soft-Actor Critic (SAC) and other off-policy reinforcement learning algorithms to work on massively parallel simulators (think Isaac Sim with thousands of robots simulated in parallel).
If you follow the journey, you will learn about overlooked details in task design and algorithm implementation that can have a big impact on performance.">


  <link rel="alternate" hreflang="en-us" href="/post/sac-massive-sim/">







  <meta name="theme-color" content="hsl(16, 89%, 60%)">





  <script src="/js/mathjax-config.js"></script>







    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css" integrity="sha512-Kc323vGBEqzTmouAECnVceyQqyqdsSiqLQISBL29aUW4U/M7pSPA/gEUZQqv1cwx4OnYxTxve5UMg5GT6L4JJg==" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">









        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-light.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/atom-one-light.min.css" crossorigin="anonymous" title="hl-dark" disabled>





















































































        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>






















        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>






















  <link rel="stylesheet" href="/css/academic.css">











  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_huc6f7a4edf58249363e4a58dcffa16205_339058_32x32_fill_lanczos_center_3.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_huc6f7a4edf58249363e4a58dcffa16205_339058_192x192_fill_lanczos_center_3.png">

  <link rel="canonical" href="/post/sac-massive-sim/">









  <meta property="twitter:card" content="summary_large_image">

  <meta property="og:site_name" content="Antonin Raffin | Homepage">
  <meta property="og:url" content="/post/sac-massive-sim/">
  <meta property="og:title" content="Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms (Part I) | Antonin Raffin | Homepage">
  <meta property="og:description" content="This post details how I managed to get the Soft-Actor Critic (SAC) and other off-policy reinforcement learning algorithms to work on massively parallel simulators (think Isaac Sim with thousands of robots simulated in parallel).
If you follow the journey, you will learn about overlooked details in task design and algorithm implementation that can have a big impact on performance."><meta property="og:image" content="/post/sac-massive-sim/featured.jpg">
  <meta property="twitter:image" content="/post/sac-massive-sim/featured.jpg"><meta property="og:locale" content="en-us">


      <meta property="article:published_time" content="2025-02-10T00:00:00&#43;00:00">

    <meta property="article:modified_time" content="2025-02-10T00:00:00&#43;00:00">

















<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "/post/sac-massive-sim/"
  },
  "headline": "Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms (Part I)",

  "image": [
    "/post/sac-massive-sim/featured.jpg"
  ],

  "datePublished": "2025-02-10T00:00:00Z",
  "dateModified": "2025-02-10T00:00:00Z",

  "author": {
    "@type": "Person",
    "name": "Antonin Raffin"
  },

  "publisher": {
    "@type": "Organization",
    "name": "Antonin Raffin | Homepage",
    "logo": {
      "@type": "ImageObject",
      "url": "/images/icon_huc6f7a4edf58249363e4a58dcffa16205_339058_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "This post details how I managed to get the Soft-Actor Critic (SAC) and other off-policy reinforcement learning algorithms to work on massively parallel simulators (think Isaac Sim with thousands of robots simulated in parallel).\nIf you follow the journey, you will learn about overlooked details in task design and algorithm implementation that can have a big impact on performance."
}
</script>















  <title>Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms (Part I) | Antonin Raffin | Homepage</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="dark">

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">

        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">

      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">

      </div>

    </section>
  </div>
</aside>










<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">


    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Antonin Raffin | Homepage</a>
    </div>



    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>



    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Antonin Raffin | Homepage</a>
    </div>




    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">


      <ul class="navbar-nav d-md-inline-flex">
























        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>


























        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>


























        <li class="nav-item">
          <a class="nav-link " href="/#publications_selected"><span>Publications</span></a>
        </li>


























        <li class="nav-item">
          <a class="nav-link " href="/#talks"><span>Talks</span></a>
        </li>


























        <li class="nav-item">
          <a class="nav-link " href="/#posts"><span>Posts</span></a>
        </li>


























        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>







      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>






    </ul>

  </div>
</nav>


  <article class="article">




























<div class="article-container pt-3">
  <h1>Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms (Part I)</h1>






<div class="article-metadata">





  <span class="article-date">




    Feb 10, 2025
  </span>













</div>
















</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 395px;">
  <div style="position: relative">
    <img src="/post/sac-massive-sim/featured_hufd7daa78f73a0ea9a2b095306d3c61af_375575_720x0_resize_q90_lanczos.jpg" alt="" class="featured-image">

  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p>This post details how I managed to get the Soft-Actor Critic (SAC) and other off-policy reinforcement learning algorithms to work on massively parallel simulators (think Isaac Sim with thousands of robots simulated in parallel).
If you follow the journey, you will learn about overlooked details in task design and algorithm implementation that can have a big impact on performance.</p>
<p>Spoiler alert:
<a href="#appendix---affected-paperscode">quite a few papers/code</a> are affected by the problem described below.</p>
<ul>
<li>Part I is about identifying the problem and trying out quick fixes on SAC.</li>
<li>Part II (WIP) will be about tuning SAC for speed and making it work as good as PPO.</li>
</ul>
<h2 id="a-suspicious-trend-ppo-ppo-ppo-">A Suspicious Trend: PPO, PPO, PPO, &hellip;</h2>
<p>The story begins a few months ago when I saw another paper using the same recipe for learning locomotion: train a PPO agent in simulation using thousands of environments in parallel and domain randomization, then deploy it on the real robot.
This recipe has become the standard since 2021, when ETH Zurich and NVIDIA<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> showed that it was possible to learn locomotion in minutes on a single workstation.
The codebase and the simulator (called Isaac Gym at that time) that were published became the basis for much follow-up work<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>As an RL researcher focused on
<a href="https://proceedings.mlr.press/v164/raffin22a/raffin22a.pdf" target="_blank" rel="noopener">learning directly on real robots</a>, I was curious and suspicious about one aspect of this trend: why is no one trying an algorithm other than PPO?<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>
PPO benefits from fast and parallel environments<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>, but PPO is not the only deep reinforcement learning (DRL) algorithm for continuous control tasks and there are alternatives like SAC or TQC that can lead to better performance<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup>.</p>
<p>So I decided to investigate why these off-policy algorithms are not used by practitioners, and maybe why they don&rsquo;t work with massively parallel simulators.</p>
<h2 id="why-it-matters---fine-tuning-on-real-robots">Why It Matters? - Fine Tuning on Real Robots</h2>
<p>If we could make SAC work with these simulators, then it would be possible to train in simulation and fine-tune on the real robot using the same algorithm (PPO is too sample-inefficient to train on a single robot) .</p>
<p>By using other algorithms it might also be possible to get better performance.
Finally, it is always good to have a better understanding of what works or not and why.
As researchers, we tend to publish only positive results, but I think a lot of valuable insights are lost in our unpublished failures.</p>
<a href="https://araffin.github.io/slides/design-real-rl-experiments/">
  <img style="max-width: 50%" src="https://araffin.github.io/slides/tips-reliable-rl/images/bert/real_bert.jpg" alt="The DLR bert quadruped robot, standing on a stone." />
</a>
  <p style="font-size: 12pt; text-align:center;">The DLR bert elastic quadruped</p>
<h2 id="the-path-of-least-resistance-hypothesis">(The Path of Least Resistance) Hypothesis</h2>
<p>Before digging any further, I had some hypotheses as to why PPO was the only algorithm used:</p>
<ul>
<li>PPO is fast to train (in terms of computation time) and was tuned for the massively parallel environment.</li>
<li>As researchers, we tend to take the path of least resistance and build on proven solutions (the original training code is open source and the simulator is freely available) to get new interesting results<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup>.</li>
<li>There may be some peculiarities in the environment design that favor PPO over algorithms. In other words, the massively parallel environments might be optimized for PPO.</li>
<li>SAC/TQC and derivatives are tuned for sample efficiency, not fast wall clock time. In the case of massively parallel simulation, what matters is how long it takes to train, not how many samples are used. They probably need to be tuned/adjusted for this new setting.</li>
</ul>
<p>Note: during my journey, I will (obviously) be using
<a href="https://github.com/DLR-RM/stable-baselines3" target="_blank" rel="noopener">Stable-Baselines3</a> and its fast Jax version
<a href="https://github.com/araffin/sbx" target="_blank" rel="noopener">SBX</a>.</p>
<h2 id="the-hunt-begins">The Hunt Begins</h2>
<p>There are now many massively parallel simulators available (Isaac Sim, Brax, MJX, Genesis, &hellip;), here, I chose to focus on Isaac Sim because it was one of the first and is probably the most influential one.</p>
<p>As with any RL problem, starting simple is the
<a href="https://www.youtube.com/watch?v=eZ6ZEpCi6D8" target="_blank" rel="noopener">key to success</a>.</p>
<video controls src="https://b2drop.eudat.eu/public.php/dav/files/z5LFrzLNfrPMd9o/ppo_trained.mp4">
</video>
<p style="font-size: 14pt; text-align:center;">A PPO agent trained on the <code>Isaac-Velocity-Flat-Unitree-A1-v0</code> locomotion task.
  <br>
  Green arrow is the desired velocity, blue arrow represents the current velocity
</p>
<p>Therefore, I decided to focus on the <code>Isaac-Velocity-Flat-Unitree-A1-v0</code> locomotion task first, because it is simple but representative.
The goal is to learn a policy that can move the Unitree A1 quadruped in any direction on a flat ground, following a commanded velocity (the same way you would control a robot with a joystick).
The agent receives information about its current task as input (joint positions, velocities, desired velocity, &hellip;) and outputs desired joint positions (12D vector, 3 joints per leg).
The robot is rewarded for following the correct desired velocity (linear and angular) and for other secondary tasks (feet air time, smooth control, &hellip;).
An episode ends when the robot falls over and is timed out (
<a href="https://www.youtube.com/watch?v=eZ6ZEpCi6D8" target="_blank" rel="noopener">truncation</a>) after 1000 steps<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup>.</p>
<p>After some
<a href="https://github.com/isaac-sim/IsaacLab/pull/2022" target="_blank" rel="noopener">quick optimizations</a> (SB3 now runs 4x faster, at 60 000 fps for 2048 envs with PPO), I did some sanity checks.
First, I ran PPO with the tuned hyperparameters found in the repo, and it was able to quickly solve the task.
In 5 minutes, it gets an average episode return of ~30 (above an episode return of 15, the task is almost solved).
Then I tried SAC and TQC, with default hyperparameters (and observation normalization), and, as expected, it didn&rsquo;t work.
No matter how long it was training, there was no sign of improvement.</p>
<p>Looking at the simulation GUI, something struck me: the robots were making very large random movements.
Something was wrong.</p>
<video controls src="https://b2drop.eudat.eu/public.php/dav/files/z5LFrzLNfrPMd9o/limits_train.mp4">
</video>
<p style="font-size: 14pt; text-align:center;">SAC out of the box on Isaac Sim during training.</p>
<p>Because of the very large movements, my suspicion was towards what action the robot is allowed to do.
Looking at the code, the RL agent commands a (scaled)
<a href="https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab/isaaclab/envs/mdp/actions/joint_actions.py#L134" target="_blank" rel="noopener">delta</a> with respect to a default
<a href="https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py#L112" target="_blank" rel="noopener">joint position</a>:</p>
<pre><code class="language-python"># Note desired_joint_pos is of dimension 12 (3 joints per leg)
desired_joint_pos = default_joint_pos + scale * action
</code></pre>
<p>Then, let&rsquo;s look at the action space itself (I&rsquo;m using <code>ipdb</code> to have an interactive debugger):</p>
<pre><code class="language-python">import ipdb; ipdb.set_trace()
&gt;&gt; vec_env.action_space
Box(-100.0, 100.0, (12,), float32)
</code></pre>
<p>Ah ah!
The action space defines continuous actions of dimension 12 (nothing wrong here), but the limits $[-100, 100]$ are surprisingly large, e.g., it allows a delta of +/- 1432 deg!! in joint angle when
<a href="https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/config/a1/rough_env_cfg.py#L30" target="_blank" rel="noopener">scale=0.25</a>, like for the Unitree A1 robot.
To understand why
<a href="https://www.youtube.com/watch?v=Ikngt0_DXJg" target="_blank" rel="noopener">normalizing</a> the action space
<a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html" target="_blank" rel="noopener">matters</a> (usually a bounded space in $[-1, 1]$), we need to dig deeper into how PPO works.</p>
<h2 id="ppo-gaussian-distribution">PPO Gaussian Distribution</h2>
<p>Like many RL algorithms,
<a href="https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/" target="_blank" rel="noopener">PPO</a> relies on a probability distribution to select actions.
During training, at each timestep, it samples an action $a_t \sim N(\mu_\theta(s_t), \sigma^2)$ from a Gaussian distribution in the case of continuous actions<sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup>.
The mean of the Gaussian $\mu_\theta(s_t)$ is the output of the actor neural network (with parameters $\theta$) and the standard deviation is a
<a href="https://github.com/DLR-RM/stable-baselines3/blob/55d6f18dbd880c62d40a276349b8bac7ebf453cd/stable_baselines3/common/distributions.py#L150" target="_blank" rel="noopener">learnable parameter</a> $\sigma$, usually
<a href="https://github.com/leggedrobotics/rsl_rl/blob/f80d4750fbdfb62cfdb0c362b7063450f427cf35/rsl_rl/modules/actor_critic.py#L26" target="_blank" rel="noopener">initialized</a> with $\sigma_0 = 1.0$.</p>
<p>This means that at the beginning of training, most of the sampled actions will be in $[-3, 3]$ (from the
<a href="https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule" target="_blank" rel="noopener">Three Sigma Rule</a>):</p>
<img style="max-width:80%" src="./img/gaussian.svg"/>
<p style="font-size: 14pt; text-align:center;">The initial Gaussian distribution used by PPO for sampling actions.</p>
<p>Back to our original topic, because of the way $\sigma$ is initialized, if the action space has large bounds (upper/lower bounds &raquo; 1), PPO will almost never sample actions near the limits.
In practice, the actions taken by PPO will even be far away from them.
Now, let&rsquo;s compare the initial PPO action distribution with the Unitree A1 action space:</p>
<img style="max-width:80%" src="./img/gaussian_large_bounds.svg"/>
<p style="font-size: 14pt; text-align:center;">The same initial Gaussian distribution but with the perspective of the Unitree A1 action space $[-100, 100]$</p>
<p>For reference, we can plot the action distribution of PPO after training<sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup>:</p>
<img src="./img/dist_actions_trained_ppo.svg"/>
<p style="font-size: 14pt; text-align:center;">Distribution of actions for PPO after training (on 64 000 steps).</p>
<p>The min/max values per dimension:</p>
<pre><code class="language-python">&gt;&gt; actions.min(axis=0)
array([-3.6, -2.5, -3.1, -1.8, -4.5, -4.2, -4. , -3.9, -2.8, -2.8, -2.9, -2.7])
&gt;&gt; actions.max(axis=0)
array([ 3.2,  2.8,  2.7,  2.8,  2.9,  2.7,  3.2,  2.9,  7.2,  5.7,  5. ,  5.8])

</code></pre>
<p>Again, most of the actions are centered around zero (which makes sense, since it corresponds to the quadruped initial position, which is usually chosen to be stable), and there are almost no actions outside $[-5, 5]$ (less than 0.1%): PPO uses less than 5% of the action space!</p>
<p>Now that we know that we need less than 5% of the action space to solve the task, let&rsquo;s see why this might explain why SAC doesn&rsquo;t work in this case<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup>.</p>
<!-- Note: if in rad, 3 rad is already 171 degrees (but action scale = 0.25, so ~40 deg, action scale = 0.5 for Anymal). -->
<h2 id="sac-squashed-gaussian">SAC Squashed Gaussian</h2>
<p>SAC and other off-policy algorithms for continuous actions (such as DDPG, TD3 or
<a href="https://sb3-contrib.readthedocs.io/en/master/modules/tqc.html" target="_blank" rel="noopener">TQC</a>) have an additional transformation at the end of the actor network.
SAC squashes the action sampled from an unbounded Gaussian distribution using a
<a href="https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html" target="_blank" rel="noopener">$tanh()$</a> function.
Therefore, the sampled actions are always in $[-1, 1]$.
SAC then linearly rescales the sampled action to match the action space definition, i.e. it transforms the action from $[-1, 1]$ to $[\text{low}, \text{high}]$<sup id="fnref:11"><a href="#fn:11" class="footnote-ref" role="doc-noteref">11</a></sup>.</p>
<p>What does this mean?
Assuming we start with a standard deviation similar to PPO, this is what the sampled action distribution looks like after squashing<sup id="fnref:12"><a href="#fn:12" class="footnote-ref" role="doc-noteref">12</a></sup>:</p>
<img src="./img/squashed_vs_gaussian.svg"/>
<p style="font-size: 14pt; text-align:center;">The equivalent initial squashed Gaussian distribution.</p>
<p>And after rescaling to the environment limits (with PPO distribution to put it in perspective):</p>
<img src="./img/squashed_rescaled.svg"/>
<p style="font-size: 14pt; text-align:center;">The same initial squashed Gaussian distribution but rescaled to the Unitree A1 action space $[-100, 100]$</p>
<p>As you can see, these are two completely different initial distributions at the beginning of training!
The fact that the actions are rescaled to fit the action space boundaries explains the very large movements seen during training, and also explains why it was impossible for SAC to learn anything useful.</p>
<h2 id="quick-fix">Quick Fix</h2>
<p>When I discovered that the action limits were way too large, my first reflex was to re-train SAC, but with only 3% of the action space, to more or less match the effective action space of PPO.
Although it didn&rsquo;t reach PPO performance, there was finally some sign of life (an average episodic return slightly positive after a while).</p>
<p>What I tried next was to reduce SAC exploration by having a smaller entropy coefficient<sup id="fnref:13"><a href="#fn:13" class="footnote-ref" role="doc-noteref">13</a></sup> at the beginning of training.
Bingo!
SAC finally learned to solve the task!</p>
<!-- TODO: image learning curve? -->
<video controls src="https://b2drop.eudat.eu/public.php/dav/files/z5LFrzLNfrPMd9o/sac_trained_cut_1.mp4">
</video>
<p style="font-size: 14pt; text-align:center;">Trained SAC agent after the quick fix.</p>
<h2 id="thats-all-folks">That&rsquo;s all folks?</h2>
<p>Although SAC can now solve this locomotion task, it takes more time to train, is not consistent, and the performance is slightly below PPO&rsquo;s.
In addition, SAC&rsquo;s learned gaits are not as pleasing as PPO&rsquo;s, for example, SAC agents tend to keep one leg up in the air&hellip;</p>
<p>Part II will explore these aspects (and more environments), review SAC design decisions (for example, try to remove the squashed Gaussian), and tune it for speed, but for now let&rsquo;s see what this means for the RL community.</p>
<h2 id="outro-what-does-that-mean-for-the-rl-community">Outro: What Does That Mean for the RL Community?</h2>
<p>When I found out about this problem, I was curious to see how widespread it was in the community.
After a quick search, it turns out that there are a lot of papers/code affected<sup id="fnref:14"><a href="#fn:14" class="footnote-ref" role="doc-noteref">14</a></sup> by this large boundary problem (see a non-exhaustive
<a href="#appendix---affected-paperscode">list of affected papers/code below</a>).</p>
<p>Although the initial choice of bounds may be a conscious and convenient one (no need to specify the real bounds, PPO will figure it out), it seems to have worked a bit by accident for those who built on top of it, and probably discouraged practitioners from trying other algorithms.</p>
<p>My recommendation would be to always have properly defined action bounds, and if they are not known in advance, you can always
<a href="https://gist.github.com/araffin/e069945a68aa0d51fcdff3f01e945c70" target="_blank" rel="noopener">plot the action distribution</a> and adjust the limits when iterating on the environment design.</p>
<!-- TODO: get feedback if this is an overlooked problem or known issue but PPO is nice because it can decide which action space to choose? -->
<!-- Quick tuning: use TQC (equal or better perf than SAC), faster training with JIT and multi gradient steps, policy delay and train_freq, bigger batch size.

Note: entropy coeff is inverse reward scale in maximum entropy RL -->
<!-- ## Tuning for speed

Automatic hyperparameter optimization with Optuna.
Good and fast results (not as fast as PPO but more sample efficient).
Try schedule of action space (start small and make it bigger over time): not so satifying,
looking into unbounded action space. -->
<!-- ## PPO Gaussian dist vs Squashed Gaussian

Difference between log std computation (state-dependent with clipping vs independent global param).

Trying to make SAC looks like PPO, move to unbounded Gaussian dist, instabilities.
Fixes: clip max action, l2 loss (like [SAC original implementation](https://github.com/haarnoja/sac/blob/8258e33633c7e37833cc39315891e77adfbe14b2/sac/distributions/normal.py#L69-L70))
Replace state-dependent std with independent: auto-tuning entropy coeff broken, need to fix it (TODO: investigate why). -->
<!-- SAC initial commit https://github.com/haarnoja/sac/blob/fa226b0dcb244d69639416995311cc5b4092c8f7/sac/distributions/gmm.py#L122 -->
<!-- Note: SAC work on MuJoCo like env

Note: two variations of the same issue: unbounded (matches Gaussian dist real domain)
and clipped to high limits

Note: brax PPO seems to implement tanh Gaussian dist (action limited to [-1, 1]):
https://github.com/google/brax/blob/241f9bc5bbd003f9cfc9ded7613388e2fe125af6/brax/training/agents/ppo/networks.py#L78
MuJoCo playground and Brax clip: https://github.com/google-deepmind/mujoco_playground/blob/0f3adda84f2a2ab55e9d9aaf7311c917518ec25c/mujoco_playground/_src/wrapper_torch.py#L158
but not really defined explicitly in the env (for the limits)

Note: rescale action doesn't work for PPO, need retuning? need tanh normal? -->
<h3 id="appendix---affected-paperscode">Appendix - Affected Papers/Code</h3>
<p>Please find here a non-exhaustive list of papers/code affected by the large bound problem:</p>
<!-- - [MuJoCo Playground](https://github.com/google-deepmind/mujoco_playground/blob/0f3adda84f2a2ab55e9d9aaf7311c917518ec25c/mujoco_playground/_src/locomotion/go1/joystick.py#L239) -->
<!-- https://github.com/Argo-Robot/quadrupeds_locomotion/blob/45eec904e72ff6bafe1d5378322962003aeff88d/src/go2_env.py#L173 -->
<!-- https://github.com/leggedrobotics/legged_gym/blob/17847702f90d8227cd31cce9c920aa53a739a09a/legged_gym/envs/base/legged_robot.py#L85 -->
<ul>
<li>
<a href="https://github.com/isaac-sim/IsaacLab/blob/c4bec8fe01c2fd83a0a25da184494b37b3e3eb61/source/isaaclab_rl/isaaclab_rl/sb3.py#L154" target="_blank" rel="noopener">IsaacLab</a></li>
<li>
<a href="https://github.com/leggedrobotics/legged_gym/blob/17847702f90d8227cd31cce9c920aa53a739a09a/legged_gym/envs/base/legged_robot_config.py#L164" target="_blank" rel="noopener">Learning to Walk in Minutes</a></li>
<li>
<a href="https://github.com/nico-bohlinger/one_policy_to_run_them_all/blob/d9d166c348496c9665dd3ebabc20efb6d8077161/one_policy_to_run_them_all/environments/unitree_a1/environment.py#L140" target="_blank" rel="noopener">One Policy to Run Them All</a></li>
<li>
<a href="https://github.com/Argo-Robot/quadrupeds_locomotion/blob/45eec904e72ff6bafe1d5378322962003aeff88d/src/go2_train.py#L104" target="_blank" rel="noopener">Genesis env</a></li>
<li>
<a href="https://github.com/LeCAR-Lab/ASAP/blob/c78664b6d2574f62bd2287e4b54b4f8c2a0a47a5/humanoidverse/config/robot/g1/g1_29dof_anneal_23dof.yaml#L161" target="_blank" rel="noopener">ASAP Humanoid</a></li>
<li>
<a href="https://github.com/LeCAR-Lab/ABS/blob/9b95329ffb823c15dead02be620ff96938e4d0a3/training/legged_gym/legged_gym/envs/base/legged_robot_config.py#L169" target="_blank" rel="noopener">Agile But Robust</a></li>
<li>
<a href="https://github.com/Improbable-AI/rapid-locomotion-rl/blob/f5143ef940e934849c00284e34caf164d6ce7b6e/mini_gym/envs/base/legged_robot_config.py#L209" target="_blank" rel="noopener">Rapid Locomotion</a></li>
<li>
<a href="https://github.com/MarkFzp/Deep-Whole-Body-Control/blob/8159e4ed8695b2d3f62a40d2ab8d88205ac5021a/legged_gym/legged_gym/envs/widowGo1/widowGo1_config.py#L114" target="_blank" rel="noopener">Deep Whole Body Control</a></li>
<li>
<a href="https://github.com/ZiwenZhuang/parkour/blob/789e83c40b95fdd49fda7c1725c8c573df42d2a9/legged_gym/legged_gym/envs/base/legged_robot_config.py#L169" target="_blank" rel="noopener">Robot Parkour Learning</a></li>
</ul>
<p>You can probably find many more looking at
<a href="https://scholar.google.com/scholar?cites=8503164023891275626&amp;as_sdt=2005&amp;sciodt=0,5" target="_blank" rel="noopener">works that cite the ETH paper</a>.</p>
<ul>
<li>Seems to be fixed in
<a href="https://github.com/chengxuxin/extreme-parkour/blob/d2ffe27ba59a3229fad22a9fc94c38010bb1f519/legged_gym/legged_gym/envs/base/legged_robot_config.py#L120" target="_blank" rel="noopener">Extreme Parkour</a> (clip action 1.2)</li>
<li>Almost fixed in
<a href="https://github.com/Improbable-AI/walk-these-ways/blob/0e7236bdc81ce855cbe3d70345a7899452bdeb1c/scripts/train.py#L200" target="_blank" rel="noopener">Walk this way</a> (clip action 10)</li>
</ul>
<!--
Related:
- [Parallel Q Learning (PQL)](https://github.com/Improbable-AI/pql) but only tackles classic MuJoCo locomotion envs -->
<h2 id="citation">Citation</h2>
<pre><code>@article{raffin2025isaacsim,
  title   = &quot;Getting SAC to Work on a Massive Parallel Simulator: An RL Journey With Off-Policy Algorithms&quot;,
  author  = &quot;Raffin, Antonin&quot;,
  journal = &quot;araffin.github.io&quot;,
  year    = &quot;2025&quot;,
  month   = &quot;Feb&quot;,
  url     = &quot;https://araffin.github.io/post/sac-massive-sim/&quot;
}
</code></pre>
<h2 id="acknowledgement">Acknowledgement</h2>
<p>I would like to thank Anssi and Leon for their feedback =).</p>
<!-- All the graphics were made using [excalidraw](https://excalidraw.com/). -->
<h3 id="did-you-find-this-post-helpful-consider-sharing-it-">Did you find this post helpful? Consider sharing it ðŸ™Œ</h3>
<h2 id="footnotes">Footnotes</h2>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Rudin, Nikita, et al. &ldquo;Learning to walk in minutes using massively parallel deep reinforcement learning.&rdquo; Conference on Robot Learning. PMLR, 2022.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Like the
<a href="https://www.youtube.com/watch?v=7_LW7u-nk6Q" target="_blank" rel="noopener">BD-1 Disney robot</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>I was not the only one asking why SAC doesn&rsquo;t work:
<a href="https://forums.developer.nvidia.com/t/poor-performance-of-soft-actor-critic-sac-in-omniverseisaacgym/266970" target="_blank" rel="noopener">nvidia forum</a>
<a href="https://www.reddit.com/r/reinforcementlearning/comments/lcx0cm/scaling_up_sac_with_parallel_environments/" target="_blank" rel="noopener">reddit1</a>
<a href="https://www.reddit.com/r/reinforcementlearning/comments/12h1faq/isaac_gym_with_offpolicy_algorithms" target="_blank" rel="noopener">reddit2</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>Berner C, Brockman G, Chan B, Cheung V, DÄ™biak P, Dennison C, Farhi D, Fischer Q, Hashme S, Hesse C, JÃ³zefowicz R. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680. 2019 Dec 13.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>See results from Huang, Shengyi, et al. &ldquo;
<a href="https://wandb.ai/openrlbenchmark/" target="_blank" rel="noopener">Open rl benchmark</a>: Comprehensive tracked experiments for reinforcement learning.&rdquo; arXiv preprint arXiv:2402.03046 (2024).&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>Yes, we tend to be lazy.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>The control loop runs at
<a href="https://github.com/isaac-sim/IsaacLab/blob/f1a4975eb7bae8509082a8ff02fd775810a73531/source/isaaclab_tasks/isaaclab_tasks/manager_based/locomotion/velocity/velocity_env_cfg.py#L302" target="_blank" rel="noopener">50 Hz</a>, so after 20s.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>This is not true for the PPO implementation in Brax which uses a squashed Gaussian like SAC.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>The code to record and plot action distribution is on
<a href="https://gist.github.com/araffin/e069945a68aa0d51fcdff3f01e945c70" target="_blank" rel="noopener">GitHub</a>&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>Action spaces that are too small are also problematic. See
<a href="https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html" target="_blank" rel="noopener">SB3 RL Tips and Tricks</a>.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:11">
<p>Rescale from [-1, 1] to [low, high] using <code>action = low + (0.5 * (scaled_action + 1.0) * (high - low))</code>.&#160;<a href="#fnref:11" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:12">
<p>Common PPO implementations clip the actions to fit the desired boundaries, which has the effect of oversampling actions at the boundaries when the limits are smaller than ~4.&#160;<a href="#fnref:12" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:13">
<p>The entropy coeff is the coeff that does the trade-off between RL objective and entropy maximization.&#160;<a href="#fnref:13" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:14">
<p>A notable exception are Brax-based environments because their PPO implementation uses a squashed Gaussian, so the boundaries of the environments had to be properly defined.&#160;<a href="#fnref:14" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    </div>









<div class="share-box" aria-hidden="true">
  <ul class="share">








      <li>
        <a href="https://twitter.com/intent/tweet?url=/post/sac-massive-sim/&amp;text=Getting%20SAC%20to%20Work%20on%20a%20Massive%20Parallel%20Simulator:%20An%20RL%20Journey%20With%20Off-Policy%20Algorithms%20%28Part%20I%29" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>








      <li>
        <a href="https://www.facebook.com/sharer.php?u=/post/sac-massive-sim/&amp;t=Getting%20SAC%20to%20Work%20on%20a%20Massive%20Parallel%20Simulator:%20An%20RL%20Journey%20With%20Off-Policy%20Algorithms%20%28Part%20I%29" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>








      <li>
        <a href="mailto:?subject=Getting%20SAC%20to%20Work%20on%20a%20Massive%20Parallel%20Simulator:%20An%20RL%20Journey%20With%20Off-Policy%20Algorithms%20%28Part%20I%29&amp;body=/post/sac-massive-sim/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>








      <li>
        <a href="https://www.linkedin.com/shareArticle?url=/post/sac-massive-sim/&amp;title=Getting%20SAC%20to%20Work%20on%20a%20Massive%20Parallel%20Simulator:%20An%20RL%20Journey%20With%20Off-Policy%20Algorithms%20%28Part%20I%29" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>








      <li>
        <a href="https://web.whatsapp.com/send?text=Getting%20SAC%20to%20Work%20on%20a%20Massive%20Parallel%20Simulator:%20An%20RL%20Journey%20With%20Off-Policy%20Algorithms%20%28Part%20I%29%20/post/sac-massive-sim/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>








      <li>
        <a href="https://service.weibo.com/share/share.php?url=/post/sac-massive-sim/&amp;title=Getting%20SAC%20to%20Work%20on%20a%20Massive%20Parallel%20Simulator:%20An%20RL%20Journey%20With%20Off-Policy%20Algorithms%20%28Part%20I%29" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>

  </ul>
</div>


























  <div class="media author-card content-widget-hr">


      <img class="avatar mr-3 avatar-circle" src="/authors/admin/avatar_hu_90bd568b0c44ac02.jpg" alt="Avatar">


    <div class="media-body">
      <h5 class="card-title"><a href="/">Antonin Raffin</a></h5>
      <h6 class="card-subtitle">Research Engineer in Robotics and Machine Learning</h6>
      <p class="card-text">Robots. Machine Learning. Blues Dance.</p>
      <ul class="network-icon" aria-hidden="true">












    <li>
      <a href="https://www.linkedin.com/in/antonin-raffin-106b18a8/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>












    <li>
      <a href="https://bsky.app/profile/araffin.bsky.social" target="_blank" rel="noopener">
        <i class="fab fa-bluesky"></i>
      </a>
    </li>










    <li>
      <a href="https://scholar.google.fr/citations?user=kik4AwIAAAAJ&amp;hl=fr" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>












    <li>
      <a href="https://github.com/araffin" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>












    <li>
      <a href="https://www.twitch.tv/givethatrobotacookie" target="_blank" rel="noopener">
        <i class="fab fa-twitch"></i>
      </a>
    </li>












    <li>
      <a href="https://www.youtube.com/user/atooo57" target="_blank" rel="noopener">
        <i class="fab fa-youtube"></i>
      </a>
    </li>

</ul>

    </div>
  </div>

















  </div>
</article>






      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>





        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>










    <script>const code_highlighting = true;</script>




    <script>const isSiteThemeDark = true;</script>








    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>







    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>




    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>






















    <script src="/js/academic.min.80e8497da12d94dc7fea279b7993043d.js"></script>










  <div class="container">
    <footer class="site-footer">


  <p class="powered-by">
    Â© 2018 - 2025 &middot;

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.


    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>

  </p>
</footer>

  </div>



<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
