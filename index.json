[{"authors":["admin"],"categories":null,"content":"Bio Robots. Machine Learning. Blues Dance.\n","date":1631577600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1631577600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Bio Robots. Machine Learning. Blues Dance.","tags":null,"title":"Antonin Raffin","type":"authors"},{"authors":null,"categories":null,"content":"It is critical for Reinforcement Learning (RL) practitioners to properly evaluate and compare results. Reporting results with poor comparison leads to a progress mirage and may underestimate the stochasticity of the results. To this end, Deep RL at the Edge of the Statistical Precipice (Neurips Oral) provides recommendations for a more rigorous evaluation of DeepRL algorithms. The paper comes with an open-source library named rliable.\nThis blog post is meant to be a visual explanation of the tools used by the rliable library to better evaluate and compare RL algorithms. We will go through the different recommendations of the authors and give a visual explanation for each of them.\nScore Normalization To have more datapoints that just 10 random seeds, rliable recommends aggregating all N runs across all M tasks (e.g., aggregating all Atari games results) so we have a total of NxM runs from which we can sample from. To have comparable scores across tasks, we first need to normalize the scores of each run per task as follows:\n\nNote: the score may depend on what you want to compare. It is usually the final performance of the RL agent, after training.\nStratified Bootstrap Confidence Intervals To account for uncertainty in aggregate performance, rliable uses stratified bootstrap confidence intervals. This may sound complicated, but let\u0026rsquo;s go slowly through the meaning of each of those terms.\nFirst, bootstrap means sampling with replacement. For instance, if we sample four times with replacement 3 runs of indices [1, 2, 3] on a task A, we may get: [2, 2, 3, 1] the first time, [3, 1, 1, 1] the second time, \u0026hellip;\nStratified bootstrap means that we first group our datapoints into buckets (or strata), and then sample with replacement each of those buckets according to their size:\n\nIn RL, the buckets are the different tasks or environments. With stratified bootstrap, all tasks are always represented in the sampled runs. This avoids computing the aggregate metrics only on a subset of all the environments:\n\nEach time we sample with replacement the runs, we compute the different metrics (for instance, mean score) for those sampled runs. To report uncertainty, rliable computes bootstrap confidence intervals (CIs) following the percentiles' method:\n\nNote: there are other methods for computing CI with bootstrap, but percentiles was found by the authors to work well in practice.\nInterquartile Mean (IQM) To summarize benchmark performance, it is common to report mean/median performance of the runs. However, mean is known to be sensible to outliers and median may not reflect enough the distribution of scores, so rliable suggests to use Interquartile Mean (IQM) instead:\n\nPerformance Profiles To report performance variability across tasks and runs, the authors proposes to use performance profiles. It tells for a given target performance (for example, 60% of the reference performance) the proportion of runs that achieve it.\nSource: image from the authors of the rliable library\nProbability of Improvement Finally, to test whether an algorithm X is probably better or not than an algorithm Y, rliable uses the U-statistic from a Mannâ€“Whitney U test:\n\nThe probability of improvement is then average over the tasks. A probability of improvement around 0.5 means that the two algorithms have similar performances.\nIn Practice: Using the RL Zoo To allow more users to use rliable, we added basic support of it in the RL Baselines3 Zoo, a training framework for Stable-Baselines3. Fore more information, please follow the instructions in the README.\nConclusion In this post, we have seen the different tools used by rliable to better evaluate RL algorithms:\n score normalization to aggregate scores across tasks stratified bootstrap to provide proper confidence intervals interquartile mean (IQM) to summarize benchmark performance performance profile for an overview of the results and their variability probability of improvement to compare two algorithms  Acknowledgement I would like to thank Pablo Samuel Castro and Rishabh Agarwal for checking the correctness of the visuals.\nAll the graphics were made using excalidraw.\nDid you find this post helpful? Consider sharing it ðŸ™Œ ","date":1635465600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635465600,"objectID":"45ef620010a09a29a398799acdc2765b","permalink":"/post/rliable/","publishdate":"2021-10-29T00:00:00Z","relpermalink":"/post/rliable/","section":"post","summary":"It is critical for Reinforcement Learning (RL) practitioners to properly evaluate and compare results. Reporting results with poor comparison leads to a progress mirage and may underestimate the stochasticity of the results.","tags":null,"title":"Rliable: Better Evaluation for Reinforcement Learning - A Visual Explanation","type":"post"},{"authors":["Antonin Raffin","Jens Kober","Freek Stulp"],"categories":null,"content":"","date":1631577600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1631577600,"objectID":"d551b1d32cc242b5e44a434a9adfbf86","permalink":"/publication/gsde/","publishdate":"2021-09-14T00:00:00Z","relpermalink":"/publication/gsde/","section":"publication","summary":"We extend the original state-dependent exploration (SDE) to apply deep reinforcement learning algorithms directly on real robots. The resulting method, gSDE, yields competitive results in simulation but outperforms the unstructured exploration on the real robot.","tags":["Reinforcement Learning,","Robotics"],"title":"Smooth Exploration for Robotic Reinforcement Learning","type":"publication"},{"authors":["Antonin Raffin"],"categories":null,"content":"","date":1622311200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622311200,"objectID":"0fe3b40dc836986ff33dbcbef5da0bf1","permalink":"/talk/learning-race/","publishdate":"2021-01-01T00:08:00Z","relpermalink":"/talk/learning-race/","section":"talk","summary":"Talk for the DonkeyCar community about learning to race in hours using reinforcement learning.","tags":["Reinforcement Learning"],"title":"Learning to Race in Hours with Reinforcement Learning","type":"talk"},{"authors":["Antonin Raffin","Bastian Deutschmann","Freek Stulp"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1619827200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619827200,"objectID":"aa5ff2774cc2684f78957114cdee1871","permalink":"/publication/fault-tolerant/","publishdate":"2021-05-01T00:00:00Z","relpermalink":"/publication/fault-tolerant/","section":"publication","summary":"Fault-Tolerant 6D Pose Estimation for Soft Robot. We present a simple ensembling method to detect and handle failures on a tendon driven robot.","tags":["Robotics"],"title":"Fault-Tolerant Six-DoF Pose Estimation for Tendon-Driven Continuum Mechanisms","type":"publication"},{"authors":["Antonin Raffin"],"categories":null,"content":"  RL Tips and Tricks Slides  SB3 Hands-on Session slides  SB3 Hands-on Session github repo  ","date":1617958800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617958800,"objectID":"91fa1158d6a5603f9d79c6a374575703","permalink":"/talk/rlvs/","publishdate":"2021-01-01T00:08:00Z","relpermalink":"/talk/rlvs/","section":"talk","summary":"Talk at the reinforcement learning virtual school on applying RL in practice and hands-on session with Stable-Baselines3.","tags":["Reinforcement Learning"],"title":"RL Tips and Tricks / The Challenges of Applying RL to Real Robots","type":"talk"},{"authors":null,"categories":null,"content":"After several months of beta, we are happy to announce the release of Stable-Baselines3 (SB3) v1.0, a set of reliable implementations of reinforcement learning (RL) algorithms in PyTorch =D! It is the next major version of Stable Baselines.\nThe implementations have been benchmarked against reference codebases, and automated unit tests cover 95% of the code.\nIn this blog post, we give you an overview of Stable-Baselines3: the motivation behind it, its design principles and features, how we ensure high-quality implementations and some concrete examples.\nTL;DR:  Stable-Baselines3 (SB3) is a library providing reliable implementations of reinforcement learning algorithms in PyTorch. It provides a clean and simple interface, giving you access to off-the-shelf state-of-the-art model-free RL algorithms.\nThe library is  fully documented, tested and its interface allows to train an RL agent in only few lines of code =):\nimport gym from stable_baselines3 import SAC # Train an agent using Soft Actor-Critic on Pendulum-v0 env = gym.make(\u0026quot;Pendulum-v0\u0026quot;) model = SAC(\u0026quot;MlpPolicy\u0026quot;, env, verbose=1) # Train the model model.learn(total_timesteps=20000) # Save the model model.save(\u0026quot;sac_pendulum\u0026quot;) # Load the trained model model = SAC.load(\u0026quot;sac_pendulum\u0026quot;) # Start a new episode obs = env.reset() # What action to take in state `obs`? action, _ = model.predict(obs, deterministic=True)  where defining and training a RL agent can be written in two lines of code:\nfrom stable_baselines3 import PPO # Train an agent using Proximal Policy Optimization on CartPole-v1 model = PPO(\u0026quot;MlpPolicy\u0026quot;, \u0026quot;CartPole-v1\u0026quot;).learn(total_timesteps=20000)  Links GitHub repository: https://github.com/DLR-RM/stable-baselines3\nDocumentation: https://stable-baselines3.readthedocs.io/\nRL Baselines3 Zoo: https://github.com/DLR-RM/rl-baselines3-zoo\nContrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\nRL Tutorial: https://github.com/araffin/rl-tutorial-jnrr19\nMotivation Deep reinforcement learning (RL) research has grown rapidly in recent years, yet results are often difficult to reproduce. A major challenge is that small implementation details can have a substantial effect on performance \u0026ndash; often greater than the difference between algorithms. It is particularly important that implementations used as experimental baselines are reliable; otherwise, novel algorithms compared to weak baselines lead to inflated estimates of performance improvements.\nTo help with this problem, we present Stable-Baselines3 (SB3), an open-source framework implementing seven commonly used model-free deep RL algorithms, relying on the OpenAI Gym interface.\nWe take great care to adhere to software engineering best practices to achieve high-quality implementations that match prior results.\nHistory SB3 builds on our experience maintaining  Stable Baselines (SB2), a fork of OpenAI Baselines built on TensorFlow 1.x. If you haven\u0026rsquo;t heard of it, Stable-Baselines (SB2) is a trusted library and has already been used in many projects and  papers with already more than 300+ citations!\nThose two years of maintaining SB2 have been a rewarding exchange with our users, where tons of bugs where fixed and new features like callbacks where added to ease the use of the library.\nHowever, SB2 was still relying on OpenAI Baselines initial codebase and with the upcoming release of Tensorflow 2, more and more internal TF code was being deprecated.\nAfter discussing the matter with the community, we decided to go for a complete rewrite in PyTorch (cf issues #366, #576 and #733), codename: Stable-Baselines31.\nStable-Baselines3 keeps the same easy-to-use API while improving a lot on the internal code, in particular by adding static type checking.\nRe-starting almost from scratch is long-term investment: it took quite some effort and time but we now have a smaller, cleaner and reliable core that is easier to maintain and extend =).\nThere are already many open source reinforcement learning libraries (almost one new every week), so why did we create a new one? In the next sections you will learn about the design principles and main features of the Stable-Baselines3 library that differenciate it from others.\n1 The very first name of the new version was \u0026ldquo;torchy-baselines\u0026rdquo;\nDesign Principles Our main goal is to provide a user-friendly and reliable RL library. To keep SB3 simple to use and maintain, we focus on model-free, single-agent RL algorithms, and rely on external projects to extend the scope to imitation and offline learning.\nWe prioritize maintaining stable implementations over adding new features or algorithms, and avoid making breaking changes. We provide a consistent, clean and fully documented API, inspired by the scikit-learn API.\nOur code is easily modifiable by users as we favour readability and simplicity over modularity, although we make use of object-oriented programming to reduce code duplication.\nFeatures Stable-Baselines3 provides many features, ranging from a simple API to a complete experimental framework that allows advanced usage like automatic hyperparameters tuning.\nSimple API Training agents in Stable-Baselines3 takes just a few lines of code, after which the agent can be queried for actions (see quick example below). This allows you to easily use the baseline algorithms and components in your experiments (eg. Imitating Animals, Slime Volleyball, Adversarial Policies), as well as apply RL to novel tasks and environments, like continual learning when attacking WiFi networks or dampening bridge vibrations.\nimport gym from stable_baselines3 import A2C from stable_baselines3.common.monitor import Monitor from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback # Save a checkpoint every 1000 steps checkpoint_callback = CheckpointCallback(save_freq=5000, save_path=\u0026quot;./logs/\u0026quot;, name_prefix=\u0026quot;rl_model\u0026quot;) # Evaluate the model periodically # and auto-save the best model and evaluations # Use a monitor wrapper to properly report episode stats eval_env = Monitor(gym.make(\u0026quot;LunarLander-v2\u0026quot;)) # Use deterministic actions for evaluation eval_callback = EvalCallback(eval_env, best_model_save_path=\u0026quot;./logs/\u0026quot;, log_path=\u0026quot;./logs/\u0026quot;, eval_freq=2000, deterministic=True, render=False) # Train an agent using A2C on LunarLander-v2 model = A2C(\u0026quot;MlpPolicy\u0026quot;, \u0026quot;LunarLander-v2\u0026quot;, verbose=1) model.learn(total_timesteps=20000, callback=[checkpoint_callback, eval_callback]) # Retrieve and reset the environment env = model.get_env() obs = env.reset() # Query the agent (stochastic action here) action, _ = model.predict(obs, deterministic=False)  Documentation SB3 comes with extensive documentation of the code API. We also include a user guide, covering both basic and more advanced usage with a collection of concrete examples. Moreover, we have developed a Colab notebook based RL tutorial, so you can demo the library directly in the browser. Additionally, we include common tips for running RL experiments and a developer guide.\nWe also pay close attention to questions and uncertainties from SB3 users, updating the documentation to address these.\nStable-Baselines3 Documentation\nHigh-Quality Implementations Algorithms are verified against published results by comparing the agent learning curves (cf issues #48 and #48).\nAs an example, to compare against TD3 and SAC original implementation, we integrated SB3 callbacks and made sure both SB3 and original implementations were using the same hyperparameters (the code diff for SAC and TD3 repos can be found here and there).\nDuring this period, that\u0026rsquo;s how we realized some tricky details that made a big difference. For example, PyTorch RMSProp is different from TensorFlow one (we include a custom version inside our codebase), and the epsilon value of the optimizer can make a big difference:\nA and B are actually the same RL algorithm (A2C), sharing the exact same code, same hardware, same hyperparameters\u0026hellip; except the epsilon value to avoid division by zero in the optimizer (one is eps=1e-5, the other eps=1e-7)\nDespite all those tricky details (and other nasty bugs), at the end, we managed to match SB2 results and original implementations closely:\nStable-Baselines (SB2) vs Stable-Baselines3 (SB3) A2C result on CartPole-v1\nStable-Baselines (SB2) vs Stable-Baselines3 (SB3) results on BreakoutNoFrameskip-v4\nStable-Baselines3 (SB3) vs original implementations results on HalfCheetahBulletEnv-v0\nComprehensive Stable-Baselines3 contains the following state-of-the-art on- and off-policy algorithms, commonly used as experimental baselines: A2C, DDPG, DQN, HER, PPO, SAC and TD3.\nMoreover, SB3 provides various algorithm-independent features. We support logging to CSV files and TensorBoard. Users can log custom metrics and modify training via user-provided callbacks.\nTo speed up training, we support parallel (or \u0026ldquo;vectorized\u0026rdquo;) environments. To simplify training, we implement common environment wrappers, like preprocessing Atari observations to match the original DQN experiments.\nYour browser does not support the video tag.  Tensorboard video integration\nExperimental Framework  RL Baselines Zoo provides scripts to train and evaluate agents, tune hyperparameters, record videos, store experiment setup and visualize results. We also include a collection of pre-trained reinforcement learning agents together with tuned hyperparameters for simple control tasks, PyBullet environments and Atari games, optimized using Optuna.\nWe follow best practices for training and evaluation, such as evaluating in a separate environment, using deterministic evaluation where required (SAC) and storing all hyperparameters necessary to replicate the experiment.\nBelow, you can see basic usage of the RL zoo (training, loading, tuning hyperparameters), which has a simple command line:\n# Train an A2C agent on Atari breakout using tuned hyperparameters, # evaluate the agent every 10k steps and save a checkpoint every 50k steps python train.py --algo a2c --env BreakoutNoFrameskip-v4 \\ --eval-freq 10000 --save-freq 50000 # Plot the learning curve python scripts/all_plots.py -a a2c -e BreakoutNoFrameskip-v4 -f logs/ # Load and evaluate a trained agent for 1000 steps # optionally, you can also load a checkpoint using --load-checkpoint python enjoy.py --algo sac --env Pendulum-v0 -n 1000 # Tune the hyperparameters of ppo on BipedalWalker-v3 with a budget of 50 trials # using 2 parallel jobs, a TPE sampler and median pruner python train.py --algo ppo --env BipedalWalker-v3 -optimize --n-trials 50 \\ --n-jobs 2 --sampler tpe --pruner median  Stable-Baselines3 Contrib We implement experimental features in a separate contrib repository. This allows SB3 to maintain a stable and compact core, while still providing the latest features, like Quantile Regression DQN (QR-DQN) or Truncated Quantile Critics (TQC).\nImplementations in contrib need not be tightly integrated with the main SB3 codebase, but we maintain the same stringent review requirements to ensure users can trust the contrib implementations. Implementations from contrib that have stood the test of time may be integrated into the main repository.\nfrom sb3_contrib import QRDQN, TQC # Train an agent using QR-DQN on Acrobot-v0 model = QRDQN(\u0026quot;MlpPolicy\u0026quot;, \u0026quot;Acrobot-v0\u0026quot;).learn(total_timesteps=20000) # Train an agent using Truncated Quantile Critics on Pendulum-v0 model = TQC(\u0026quot;MlpPolicy\u0026quot;, \u0026quot;Pendulum-v0\u0026quot;).learn(total_timesteps=20000)  Migration from Stable-Baselines (SB2) If you are Stable-Baselines (SB2) user and would like to switch to SB3, we have a migration guide waiting for you ;)\nMost of the time, it only requires to change the import from stable_baselines by from stable_baselines3 and rename some parameters.\nFor instance, if your code was like that for Stable-Baselines:\nfrom stable_baselines import PPO2 from stable_baselines.common.cmd_util import make_atari_env env = make_atari_env(\u0026quot;BreakoutNoFrameskip-v4\u0026quot;, num_env=8, seed=21) model = PPO2(\u0026quot;MlpPolicy\u0026quot;, env, n_steps=128, nminibatches=4, noptepochs=4, ent_coef=0.01, verbose=1) model.learn(int(1e5))  the corresponding SB3 code is:\nfrom stable_baselines3 import PPO # cmd_util was renamed env_util for clarity from stable_baselines3.common.env_util import make_atari_env # num_env was renamed n_envs env = make_atari_env(\u0026quot;BreakoutNoFrameskip-v4\u0026quot;, n_envs=8, seed=21) # we use batch_size instead of nminibatches which #Â was dependent on the number of environments # batch_size = (n_steps * n_envs) // nminibatches = 256 # noptepochs was renamed n_epochs model = PPO(\u0026quot;MlpPolicy\u0026quot;, env, n_steps=128, batch_size=256, n_epochs=4, ent_coef=0.01, verbose=1) model.learn(int(1e5))  For a complete migration example, you can also compare the RL Zoo of SB2 with the one from SB3.\n   Examples Let\u0026rsquo;s see now how we can now use the library in practice with some examples. We\u0026rsquo;re going to see how to easily customize the network architecture, train an agent to play Atari games and normalize observations when training on continuous control tasks like PyBullet environments.\nFor each of them, you can try it online using Google colab notebook.\nCustom Policy Network To customize a policy with SB3, all you need to do is choose a network architecture and pass a policy_kwargs (\u0026ldquo;policy keyword arguments\u0026rdquo;) to the algorithm constructor.\nThe following snippet shows how to customize the architecture and activation function for one on-policy (PPO) and one off-policy (SAC) algorithm:\nimport torch as th from stable_baselines3 import PPO, SAC # Custom actor (pi) and value function (vf) networks # of two layers of size 32 each with Relu activation function policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=dict(pi=[32, 32], vf=[32, 32])) # Create the agent model = PPO(\u0026quot;MlpPolicy\u0026quot;, \u0026quot;CartPole-v1\u0026quot;, policy_kwargs=policy_kwargs, verbose=1) # Custom actor architecture with two layers of 64 units each # Custom critic architecture with two layers of 400 and 300 units policy_kwargs = dict(net_arch=dict(pi=[64, 64], qf=[400, 300])) # Create the agent model = SAC(\u0026quot;MlpPolicy\u0026quot;, \u0026quot;Pendulum-v0\u0026quot;, policy_kwargs=policy_kwargs, verbose=1) model.learn(5000)  Atari Games Training a RL agent on Atari games is straightforward thanks to make_atari_env helper function and the VecFrameStack wrapper. It will do all the preprocessing and multiprocessing for you.\nColab link: Try it online\nfrom stable_baselines3.common.env_util import make_atari_env from stable_baselines3.common.vec_env import VecFrameStack from stable_baselines3 import A2C # There already exists an environment generator # that will make and wrap atari environments correctly. # Here we are also multi-worker training (n_envs=4 =\u0026gt; 4 environments) env = make_atari_env('PongNoFrameskip-v4', n_envs=4, seed=0) # Frame-stacking with 4 frames env = VecFrameStack(env, n_stack=4) model = A2C('CnnPolicy', env, verbose=1) model.learn(total_timesteps=25000) obs = env.reset() while True: # By default, deterministic=False, so we use the stochastic policy action, _states = model.predict(obs) obs, rewards, dones, info = env.step(action) env.render()  PyBullet: Normalizing Input Features Normalizing input features may be essential to successful training of an RL agent (by default, images are scaled but not other types of input), for instance when training on PyBullet environments. For that, a wrapper exists and will compute a running average and standard deviation of input features (it can do the same for rewards).\nColab link: Try it online\nimport os import gym import pybullet_envs from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize from stable_baselines3 import PPO env = DummyVecEnv([lambda: gym.make(\u0026quot;HalfCheetahBulletEnv-v0\u0026quot;)]) # Automatically normalize the input features and reward env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.) model = PPO('MlpPolicy', env) model.learn(total_timesteps=2000) # Don't forget to save the VecNormalize statistics when saving the agent log_dir = \u0026quot;/tmp/\u0026quot; model.save(log_dir + \u0026quot;ppo_halfcheetah\u0026quot;) stats_path = os.path.join(log_dir, \u0026quot;vec_normalize.pkl\u0026quot;) env.save(stats_path) # To demonstrate loading del model, env # Load the agent model = PPO.load(log_dir + \u0026quot;ppo_halfcheetah\u0026quot;) # Load the saved statistics env = DummyVecEnv([lambda: gym.make(\u0026quot;HalfCheetahBulletEnv-v0\u0026quot;)]) env = VecNormalize.load(stats_path, env) # do not update them at test time env.training = False # reward normalization is not needed at test time env.norm_reward = False  More Examples You can find more examples and associated colab notebooks in the documentation.\nTo the Infinity and Beyond! We presented Stable-Baselines3 v1.0, a set of reliable reinforcement learning implementations and the next major version of the Stable-Baselines.\nIf you want to follow the updates of the library, we encourage you to star the repo on GitHub and click on \u0026ldquo;Watch -\u0026gt; Custom -\u0026gt; Releases\u0026rdquo; to be notified each time a new version is released ;) (you can also follow Adam or Antonin on Twitter). Coming soon, one of our long-time requested feature: mixed observations (aka dict obs) support.\nIn case you want to contribute, make sure to read the contributing guide first.\nFinally, if you make a cool project using Stable-Baselines3, please tell us when you want it to appear in our project page.\nAbout the Authors This blog post was co-written by Stable-Baselines3 maintainers:\n  Antonin Raffin (@araffin)  Ashley Hill (@hill-a)  Maximilian Ernestus (@ernestum)  Adam Gleave (@AdamGleave)  Anssi Kanervisto (@Miffyli).  Citing the Project To cite Stable-Baselines3 in publications:\n@misc{stable-baselines3, author = {Raffin, Antonin and Hill, Ashley and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Dormann, Noah}, title = {Stable Baselines3}, year = {2019}, publisher = {GitHub}, journal = {GitHub repository}, howpublished = {\\url{https://github.com/DLR-RM/stable-baselines3}}, }  Did you find this post helpful? Consider sharing it ðŸ™Œ ","date":1614470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614470400,"objectID":"42b3adc02895c275560062c8febf4972","permalink":"/post/sb3/","publishdate":"2021-02-28T00:00:00Z","relpermalink":"/post/sb3/","section":"post","summary":"After several months of beta, we are happy to announce the release of Stable-Baselines3 (SB3) v1.0, a set of reliable implementations of reinforcement learning (RL) algorithms in PyTorch =D! It is the next major version of Stable Baselines.","tags":null,"title":"Stable-Baselines3: Reliable Reinforcement Learning Implementations","type":"post"},{"authors":null,"categories":null,"content":"Stable Baselines3 is a set of improved implementations of reinforcement learning algorithms in PyTorch. It is the next major version of Stable Baselines.\nGithub repository: https://github.com/DLR-RM/stable-baselines3\nDocumentation: https://stable-baselines3.readthedocs.io/\nRL Baselines3 Zoo (collection of pre-trained agents): https://github.com/DLR-RM/rl-baselines3-zoo\nRL Baselines3 Zoo also offers a simple interface to train, evaluate agents and do hyperparameter tuning.\n","date":1589155200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589155200,"objectID":"0031d0cb36479d7f729f11e3c0e1f3bf","permalink":"/project/stable-baselines3/","publishdate":"2020-05-11T00:00:00Z","relpermalink":"/project/stable-baselines3/","section":"project","summary":"A set of improved implementations of reinforcement learning algorithms in PyTorch.","tags":["Deep Learning","Machine Learning","Reinforcement Learning","Python"],"title":"Stable Baselines3","type":"project"},{"authors":["Antonin Raffin"],"categories":null,"content":"","date":1571357340,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571357340,"objectID":"191967224eab4fd5d222185d2af424c8","permalink":"/talk/rl-tuto-jnrr19/","publishdate":"2019-10-18T00:09:00Z","relpermalink":"/talk/rl-tuto-jnrr19/","section":"talk","summary":"Beginner tutorial on Stable Baselines library with colab notebooks","tags":["Reinforcement Learning"],"title":"RL Tutorial on Stable Baselines","type":"talk"},{"authors":["Antonin Raffin"],"categories":null,"content":"","date":1565049600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565049600,"objectID":"5b1fe9449e0ac6cc0e935c621adf569b","permalink":"/talk/sb-ias-19/","publishdate":"2019-08-06T00:00:00Z","relpermalink":"/talk/sb-ias-19/","section":"talk","summary":"A talk about SRL, lessons learned from building Stable Baselines and short tutorial on how to use it","tags":["Reinforcement Learning"],"title":"SRL - Stable Baselines Presentation","type":"talk"},{"authors":["Antonin Raffin","Ashley Hill","RenÃ© TraorÃ©","TimothÃ©e Lesort","Natalia DÃ­az-RodrÃ­guez","David Filliat"],"categories":null,"content":"","date":1548460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548460800,"objectID":"ddac45bde6a4955f3cb901f3e9c36262","permalink":"/publication/decoupling/","publishdate":"2019-01-26T00:00:00Z","relpermalink":"/publication/decoupling/","section":"publication","summary":"We evaluate the benefits of decoupling feature extraction from policy learning in robotics and propose a new way of combining state representation learning methods.","tags":["Reinforcement Learning,","State Representation Learning","Robotics"],"title":"Decoupling feature extraction from policy learning: assessing benefits of state representation learning in goal based robotics","type":"publication"},{"authors":["Antonin Raffin"],"categories":null,"content":"Read the full article on Medium\n","date":1548460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548460800,"objectID":"50154211818d2381ba531579ee213d5e","permalink":"/post/learning-to-drive/","publishdate":"2019-01-26T00:00:00Z","relpermalink":"/post/learning-to-drive/","section":"post","summary":"Learning to drive smoothly in minutes using reinforcement learning on a Donkey Car.","tags":["Deep Learning","Machine Learning","Reinforcement Learning","Python","Robotics"],"title":"Learning to Drive Smoothly in Minutes","type":"post"},{"authors":null,"categories":null,"content":"Learning to drive smoothly in minutes, using a reinforcement learning algorithm \u0026ndash; Soft Actor-Critic (SAC) \u0026ndash; and a Variational AutoEncoder (VAE) in the Donkey Car simulator.\nGithub repository: https://github.com/araffin/learning-to-drive-in-5-minutes\nBlog post on Medium\n","date":1548460800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548460800,"objectID":"72a50c53b91c902541ade0c884907d03","permalink":"/project/learning-to-drive/","publishdate":"2019-01-26T00:00:00Z","relpermalink":"/project/learning-to-drive/","section":"project","summary":"Learning to drive smoothly in minutes using reinforcement learning on a Donkey Car.","tags":["Deep Learning","Machine Learning","Reinforcement Learning","Python","Robotics"],"title":"Learning to Drive Smoothly in Minutes","type":"project"},{"authors":null,"categories":null,"content":"A collection of trained Reinforcement Learning (RL) agents, with tuned hyperparameters, using Stable Baselines.\nGithub repository: https://github.com/DLR-RM/rl-baselines3-zoo\nGoals of this repository:\n Provide a simple interface to train and enjoy RL agents Benchmark the different Reinforcement Learning algorithms Provide tuned hyperparameters for each environment and RL algorithm Have fun with the trained agents!  ","date":1542931200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1542931200,"objectID":"f3225517a7e404e918da7dd7028d960e","permalink":"/project/rl-baselines-zoo/","publishdate":"2018-11-23T00:00:00Z","relpermalink":"/project/rl-baselines-zoo/","section":"project","summary":"A collection of 70+ pre-trained RL agents using Stable Baselines","tags":["Deep Learning","Machine Learning","Reinforcement Learning","Python"],"title":"RL Baselines Zoo","type":"project"},{"authors":null,"categories":null,"content":"S-RL Toolbox: Reinforcement Learning (RL) and State Representation Learning (SRL) Toolbox for Robotics.\nGithub repository: https://github.com/araffin/robotics-rl-srl\nDocumentation: https://s-rl-toolbox.readthedocs.io\nPaper: https://arxiv.org/abs/1809.09369\nMain Features  10 RL algorithms ( Stable Baselines included) logging / plotting / visdom integration / replay trained agent hyperparameter search (hyperband, hyperopt) integration with State Representation Learning (SRL) methods (for feature extraction) visualisation tools (explore latent space, display action proba, live plot in the state space, \u0026hellip;) robotics environments to compare SRL methods easy install using anaconda env or Docker images (CPU/GPU)  ","date":1539129600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539129600,"objectID":"c3fb3b79be6c5eacc05315b01f07641e","permalink":"/project/srl-toolbox/","publishdate":"2018-10-10T00:00:00Z","relpermalink":"/project/srl-toolbox/","section":"project","summary":"S-RL Toolbox: Reinforcement Learning (RL) and State Representation Learning (SRL) for Robotics","tags":["Deep Learning","Machine Learning","Reinforcement Learning","Python","State Representation Learning","Robotics"],"title":"S-RL Toolbox","type":"project"},{"authors":null,"categories":null,"content":"Stable Baselines is a set of improved implementations of Reinforcement Learning (RL) algorithms based on OpenAI Baselines.\nGithub repository: https://github.com/hill-a/stable-baselines\nYou can read a detailed presentation of Stable Baselines in the Medium article\n","date":1538006400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538006400,"objectID":"004002cb9541f4bf06ff2b18c7b8d978","permalink":"/project/stable-baselines/","publishdate":"2018-09-27T00:00:00Z","relpermalink":"/project/stable-baselines/","section":"project","summary":"A fork of OpenAI Baselines, implementations of reinforcement learning algorithms ","tags":["Deep Learning","Machine Learning","Reinforcement Learning","Python"],"title":"Stable Baselines","type":"project"},{"authors":["Antonin Raffin","Ashley Hill","RenÃ© TraorÃ©","TimothÃ©e Lesort","Natalia DÃ­az-RodrÃ­guez","David Filliat"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"0edb8f334860f81d1bf3320939fd3327","permalink":"/publication/srl-toolbox/","publishdate":"2018-09-01T00:00:00Z","relpermalink":"/publication/srl-toolbox/","section":"publication","summary":"State representation learning aims at learning compact representations from raw observations in robotics and control applications. Approaches used for this objective are auto-encoders, learning forward models, inverse dynamics or learning using generic priors on the state characteristics. However, the diversity in applications and methods makes the field lack standard evaluation datasets, metrics and tasks. This paper provides a set of environments, data generators, robotic control tasks, metrics and tools to facilitate iterative state representation learning and evaluation in reinforcement learning settings.","tags":["Reinforcement Learning,","State Representation Learning","Robotics"],"title":"S-RL Toolbox: Environments, Datasets and Evaluation Metrics for State Representation Learning","type":"publication"},{"authors":["Antonin Raffin","Ashley Hill"],"categories":null,"content":"Read the full article on Medium\n","date":1534723200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"043512a89581bc30a7e0c74434b099ae","permalink":"/post/stable-baselines/","publishdate":"2018-08-20T00:00:00Z","relpermalink":"/post/stable-baselines/","section":"post","summary":"Unified structure (scikit-learn like interface) and single codestyle with documentation","tags":["Reinforcement Learning"],"title":"Stable Baselines: a Fork of OpenAI Baselinesâ€Šâ€”â€ŠReinforcement Learning Made Easy","type":"post"},{"authors":["Antonin Raffin"],"categories":null,"content":"Read the full article on Medium\n","date":1518220800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519862400,"objectID":"a58f8f7777263a21aec75d124ae5f992","permalink":"/post/arduino-serial/","publishdate":"2018-02-10T00:00:00Z","relpermalink":"/post/arduino-serial/","section":"post","summary":"Arduino built-in functions for sending/receiving data are not very handy and sturdy. We introduce a protocol to communicate (using serial port, bluetooth or sockets) with the Arduino (but not only) in a simple and robust way.","tags":["Robotics"],"title":"Simple and Robust {Computerâ€Šâ€”â€ŠArduino} Serial Communication","type":"post"},{"authors":["Antonin Raffin"],"categories":null,"content":"Read the full article on Medium\n","date":1510272000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"e79827b3c277ba827f7e2ffa72eaf3a0","permalink":"/post/racing-robot/","publishdate":"2017-11-10T00:00:00Z","relpermalink":"/post/racing-robot/","section":"post","summary":"Read the full article on Medium","tags":["Robotics","Machine Learning"],"title":"Autonomous Racing Robot With an Arduino, a Raspberry Pi and a Pi Camera","type":"post"},{"authors":null,"categories":null,"content":"Autonomous toy racing car. CAMaleon team at the Toulouse Robot Race 2017. Humbavision team at IronCar. Medium article: https://medium.com/@araffin/autonomous-racing-robot-with-an-arduino-a-raspberry-pi-and-a-pi-camera-3e72819e1e63\nVideo of the car: https://www.youtube.com/watch?v=xhI71ZdSh6k\n","date":1509062400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509062400,"objectID":"558b84bfdb1ba7f66e8b1e9f2cb096e6","permalink":"/project/racing-robot/","publishdate":"2017-10-27T00:00:00Z","relpermalink":"/project/racing-robot/","section":"project","summary":"Autonomous Racing Robot With an Arduino, a Raspberry Pi and a Pi Camera","tags":["Machine Learning","Python","Robotics","Arduino"],"title":"Racing Robot","type":"project"},{"authors":null,"categories":null,"content":"Robust Arduino Serial is a simple and robust serial communication protocol. It was designed to make two Arduinos communicate, but can also be useful when you want a computer (e.g. a Raspberry Pi) to communicate with an Arduino.\nPlease read the Medium Article to have an overview of this protocol.\nImplementations are available in various programming languages:\n Arduino (arduino-serial/ folder)  Python  C++  Rust  ","date":1508976000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1508976000,"objectID":"89f85cdeaf6edf9b18c5ba0f38ab3754","permalink":"/project/arduino-robust-serial/","publishdate":"2017-10-26T00:00:00Z","relpermalink":"/project/arduino-robust-serial/","section":"project","summary":"A simple and robust serial communication protocol. Implementation in C Arduino, C++, Python and Rust.","tags":["Python","State Representation Learning","Robotics","Arduino","Rust","C++"],"title":"Arduino Robust Serial","type":"project"},{"authors":["Antonin Raffin","Sebastian HÃ¶fer","Rico Jonschkowski","Oliver Brock","Freek Stulp"],"categories":null,"content":"","date":1472688000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1472688000,"objectID":"5382672b2659be28c3190eaa44bbda83","permalink":"/publication/multiple-tasks-srl/","publishdate":"2016-09-01T00:00:00Z","relpermalink":"/publication/multiple-tasks-srl/","section":"publication","summary":"We present an approach for learning state representations in multi-task reinforcement learning. Our method learns multiple low-dimensional state representations from raw observations in an unsupervised fashion, without any knowledge of which task is executed, nor of the number of tasks involved. The method is based on a gated neural network architecture, trained with an extension of the learning with robotic priors objective. In simulated experiments, we show that our method is able to learn better state representations for reinforcement learning, and we analyze why and when it manages to do so.","tags":["Reinforcement Learning,","State Representation Learning"],"title":"Unsupervised learning of state representations for multiple tasks","type":"publication"}]